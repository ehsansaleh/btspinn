{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import io\n",
    "import time\n",
    "import glob\n",
    "import fnmatch\n",
    "from itertools import chain, product\n",
    "from textwrap import dedent\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict as odict\n",
    "from os.path import exists\n",
    "from pandas.api.types import union_categoricals\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import EngFormatter\n",
    "\n",
    "from bokeh.io import push_notebook, show, output_notebook, output_file, save\n",
    "from bokeh.models import Band, ColumnDataSource, CheckboxButtonGroup\n",
    "from bokeh.models import TapTool, Select, Div, BoxZoomTool, ResetTool\n",
    "from bokeh.models import HoverTool, CustomJS, TabPanel, Tabs\n",
    "from bokeh.models import GroupFilter, CDSView, Slider, Range1d\n",
    "from bokeh.plotting import figure \n",
    "from bokeh.layouts import gridplot, layout, row, column\n",
    "\n",
    "import bspinn\n",
    "from bspinn.io_utils import get_ovatgrps, drop_unqcols\n",
    "from bspinn.io_cfg import configs_dir, results_dir\n",
    "from bspinn.io_cfg import keyspecs, nullstr\n",
    "from bspinn.io_utils import deep2hie, hie2deep\n",
    "from bspinn.io_utils import save_h5data, load_h5data\n",
    "from bspinn.io_utils import get_h5du, resio, get_dfidxs\n",
    "from bspinn.summary import summarize\n",
    "\n",
    "import yaml\n",
    "from ruamel import yaml as ruyaml\n",
    "from IPython import display as ICD\n",
    "\n",
    "from bspinn.tch_utils import BatchRNG, bffnn\n",
    "from bspinn.poisson import DeltaProblem\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This notebook is a continuation of `13_hdpviz.ipynb`\n",
    "\n",
    "Here, we wanted to adjust the code for including the second set of bootstrap hyper-parameters in `01_poisson/08_hidim.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_unifball(k, dim, np_random):\n",
    "    c = np_random.randn(k, dim)\n",
    "    c = c / np.linalg.norm(c, axis=1, keepdims=True)\n",
    "    c = c * (np_random.rand(k, 1) ** (1./dim))\n",
    "    return c\n",
    "\n",
    "def get_rsamps(n_r, dim, hpdf, k=100000):\n",
    "    # Checking that we're not handling irrelevant specs\n",
    "    ad = {'eval/ur/dstr': 'uniform',\n",
    "          'eval/ur/low': '[-1.0]',\n",
    "          'eval/ur/high': '[1.0]',\n",
    "          'eval/ub1/dstr': 'ball',\n",
    "          'eval/ub1/c': '[0.0]',\n",
    "          'eval/ub1/r': 1.0,\n",
    "          'eval/ub2/dstr': 'ball',\n",
    "          'eval/ub2/c': '[0.0]',\n",
    "          'eval/ub2/r': 'sqrt(dim)',\n",
    "          'eval/tv/dstr': 'trnvol',\n",
    "          'vol/dstr': 'ball',\n",
    "          'vol/c/dstr': 'ball',\n",
    "          'vol/c/c': '[0.0]',\n",
    "          'vol/c/r': 1.0,\n",
    "          'vol/r/dstr': 'unifdpow',\n",
    "          'vol/r/low': 0.0,\n",
    "          'vol/r/high': 1.0}\n",
    "    \n",
    "    if hpdf is not None:\n",
    "        for kk, vv in ad.items():\n",
    "            msg_ = f'{kk} != {vv} not supported!'\n",
    "            assert np.array(hpdf[kk] == vv).all(), msg_\n",
    "\n",
    "    # half-bin size\n",
    "    hb = 1./(2.*n_r)\n",
    "    # uniform representatives (i.e., quantiles)\n",
    "    q = np.linspace(hb, 1.-hb, n_r)\n",
    "    # The uniform radius (ur) representatives\n",
    "    ur = q\n",
    "    # The uniform ball with r=1 (ub1) representatives\n",
    "    ub1 = q ** (1./dim)\n",
    "    # The uniform ball with r=sqrt(dim) (ub2) representatives\n",
    "    ub2 = ub1 * np.sqrt(dim)\n",
    "\n",
    "    # Simulating the train volumnes (tv) representatives\n",
    "    np_random = np.random.RandomState(12345)\n",
    "    c = sample_unifball(k, dim, np_random)\n",
    "    assert c.shape == (k, dim)\n",
    "    r = np_random.rand(k, 1) ** (1./dim)\n",
    "    assert r.shape == (k, 1)\n",
    "    x = c + r * sample_unifball(k, dim, np_random)\n",
    "    assert x.shape == (k, dim)\n",
    "    xr = np.linalg.norm(x, axis=1)\n",
    "    assert xr.shape == (k,)\n",
    "    tv = np.percentile(xr, q*100)\n",
    "\n",
    "    outdict = dict(q=q, ur=ur, ub1=ub1, ub2=ub2, tv=tv)\n",
    "    return outdict\n",
    "\n",
    "def eval_rsol(fpidx, n_r, n_eval, n_q, eid_list, n_seeds, device_name):\n",
    "    rio = resio(fpidx, full=False, driver=None)\n",
    "    hpdf = rio('hp')\n",
    "\n",
    "    hpdict = dict(hpdf.iloc[0])\n",
    "    tch_device = torch.device(device_name)\n",
    "    tch_dtype = torch.float32\n",
    "    dim = hpdict['dim']\n",
    "    rng = BatchRNG(shape=(n_seeds,), lib='torch', \n",
    "        device=tch_device, dtype=tch_dtype)\n",
    "\n",
    "    # Creating a model instance for evaluation\n",
    "    mlpspec = hie2deep(hpdict)['nn']\n",
    "    assert mlpspec['dstr'] == 'mlp'\n",
    "    model = bffnn(dim, mlpspec['width'], mlpspec['hidden'], \n",
    "                mlpspec['act'], (n_seeds,), rng)\n",
    "\n",
    "    # Creating the problem instance\n",
    "    cdict = {'chrg/dstr': 'dmm', \n",
    "            'chrg/n': 1, \n",
    "            'chrg/w': '[1.0]', \n",
    "            'chrg/mu': '[[0.0]]'}\n",
    "\n",
    "    for kk, vv in cdict.items():\n",
    "        assert hpdict[kk] == vv, f'{kk} should be {vv}'\n",
    "\n",
    "    chrg_n = 1\n",
    "    chrg_w = np.ones((n_seeds, chrg_n))\n",
    "    chrg_mu = np.zeros((n_seeds, chrg_n, dim))\n",
    "\n",
    "    problem = DeltaProblem(weights=chrg_w, locations=chrg_mu,\n",
    "        tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "\n",
    "    # Collecting the model state dictionaries from the hdf file\n",
    "    mdlkeys = [kk for kk in rio.keys() if kk.startswith('mdl/')]\n",
    "    sdict, info = dict(), None\n",
    "    for key in mdlkeys:\n",
    "        sdkey = key.replace('mdl/', '')\n",
    "        val_np, info = rio(key, ret_info=True)\n",
    "        val = torch.from_numpy(val_np)\n",
    "        val = val.to(device=tch_device, dtype=tch_dtype)\n",
    "        sdict[sdkey] = val\n",
    "\n",
    "    # Obtaining the ioidx array\n",
    "    assert info is not None\n",
    "    ioidx = info['ioidx'].values\n",
    "    n_mdl = ioidx.size\n",
    "\n",
    "    # Collecting the radius samples\n",
    "    rdict = get_rsamps(n_r=n_r, dim=dim, hpdf=hpdict)\n",
    "    rq = rdict['q']\n",
    "    assert rq.shape == (n_r,)\n",
    "    qout = torch.linspace(0, 1, n_q, device=tch_device, dtype=tch_dtype)\n",
    "    for eid in eid_list:\n",
    "        r_np = rdict[eid]\n",
    "        assert r_np.shape == (n_r,)\n",
    "        r_tch = torch.from_numpy(r_np)\n",
    "        r_tch = r_tch.to(device=tch_device, dtype=tch_dtype)\n",
    "        assert r_tch.shape == (n_r,)\n",
    "        rdict[f'{eid}/tch'] = r_tch\n",
    "\n",
    "    # Creating a number of unit points\n",
    "    unitpnts = rng.normal((n_seeds, 1, n_eval, dim))\n",
    "    assert unitpnts.shape == (n_seeds, 1, n_eval, dim)\n",
    "    unitpnts = unitpnts / unitpnts.norm(p=2, dim=-1, keepdim=True)\n",
    "    assert unitpnts.shape == (n_seeds, 1, n_eval, dim)\n",
    "\n",
    "    # Evaluating the model on the sampled radius points\n",
    "    n_for = int(np.ceil(ioidx.size/n_seeds))\n",
    "    eid2uqlist = defaultdict(list)\n",
    "    for ii in list(range(n_for)):\n",
    "        sd = {kk: vv[ii*n_seeds:(ii+1)*n_seeds]\n",
    "            for kk, vv in sdict.items()}\n",
    "        model.load_state_dict(sd)\n",
    "        \n",
    "        for eid in eid_list:\n",
    "            r_np = rdict[eid]\n",
    "            assert r_np.shape == (n_r,)\n",
    "            r_np_bc = np.broadcast_to(r_np[None, :], (n_seeds, n_r))\n",
    "            assert r_np_bc.shape == (n_seeds, n_r)\n",
    "            r_tch = rdict[f'{eid}/tch']\n",
    "            assert r_tch.shape == (n_r,)    \n",
    "            pnts = unitpnts * r_tch.reshape(1, n_r, 1, 1)\n",
    "            assert pnts.shape == (n_seeds, n_r, n_eval, dim)\n",
    "            \n",
    "            assert (problem.n_chrg == 1)\n",
    "            assert (problem.weights == 1).all()\n",
    "            assert (problem.locations == 0).all()\n",
    "            u_gt = problem.potential(pnts[:, :, 0, :])\n",
    "            assert u_gt.shape == (n_seeds, n_r)\n",
    "            uq_gt_np = u_gt.detach().cpu().numpy()\n",
    "            assert uq_gt_np.shape == (n_seeds, n_r)\n",
    "            \n",
    "            u_mdl = model(pnts)\n",
    "            assert u_mdl.shape == (n_seeds, n_r, n_eval, 1)\n",
    "            u_mdl_ = u_mdl.squeeze(dim=-1)\n",
    "            assert u_mdl_.shape == (n_seeds, n_r, n_eval)\n",
    "            uq_mdl_ = u_mdl_.quantile(q=qout, dim=-1)\n",
    "            assert uq_mdl_.shape == (n_q, n_seeds, n_r)\n",
    "            uq_mdl = uq_mdl_.permute(1, 2, 0)\n",
    "            assert uq_mdl.shape == (n_seeds, n_r, n_q)\n",
    "            uq_mdl_np = uq_mdl.detach().cpu().numpy()\n",
    "            assert uq_mdl_np.shape == (n_seeds, n_r, n_q)\n",
    "            \n",
    "            eid2uqlist[f'{eid}/rsol/r'].append(r_np_bc)\n",
    "            eid2uqlist[f'{eid}/rsol/gt'].append(uq_gt_np)\n",
    "            eid2uqlist[f'{eid}/rsol/mdl'].append(uq_mdl_np)\n",
    "\n",
    "    eid2uq = {eid: np.concatenate(uql, axis=0) for eid, uql in eid2uqlist.items()}\n",
    "    # Example:\n",
    "    #   eid2uq = {'ur/rsol/mdl':  np.array(...), # shape: (n_mdl, n_r, n_q)\n",
    "    #             'ub1/rsol/mdl': np.array(...), # shape: (n_mdl, n_r, n_q)\n",
    "    #             'ub2/rsol/mdl': np.array(...), # shape: (n_mdl, n_r, n_q)\n",
    "    #             'tv/rsol/mdl':  np.array(...), # shape: (n_mdl, n_r, n_q)\n",
    "    #             'ur/rsol/gt':   np.array(...), # shape: (n_mdl, n_r)\n",
    "    #             'ub1/rsol/gt':  np.array(...), # shape: (n_mdl, n_r)\n",
    "    #             'ub2/rsol/gt':  np.array(...), # shape: (n_mdl, n_r)\n",
    "    #             'tv/rsol/gt':   np.array(...), # shape: (n_mdl, n_r)\n",
    "    #             'ur/rsol/r':    np.array(...), # shape: (n_mdl, n_r)\n",
    "    #             'ub1/rsol/r':   np.array(...), # shape: (n_mdl, n_r)\n",
    "    #             'ub2/rsol/r':   np.array(...), # shape: (n_mdl, n_r)\n",
    "    #             'tv/rsol/r':    np.array(...)} # shape: (n_mdl, n_r)\n",
    "\n",
    "    savedata = {f'P{pidx:08d}/var/eval/{eid}/rsol/ioidx': ioidx for eid in eid_list}\n",
    "    savedata[f'P{pidx:08d}/var/eval'] = eid2uq\n",
    "    savedata = deep2hie(savedata)\n",
    "    # Example:\n",
    "    #   savedata is a key to np.array mapping with the following shapes:\n",
    "    #     {'P00000000/var/eval/ur/rsol/ioidx':  (n_mdl,),\n",
    "    #      'P00000000/var/eval/ub1/rsol/ioidx': (n_mdl,),\n",
    "    #      'P00000000/var/eval/ub2/rsol/ioidx': (n_mdl,),\n",
    "    #      'P00000000/var/eval/tv/rsol/ioidx':  (n_mdl,),\n",
    "    #      'P00000000/var/eval/ur/rsol/r':      (n_mdl, n_r),\n",
    "    #      'P00000000/var/eval/ur/rsol/gt':     (n_mdl, n_r),\n",
    "    #      'P00000000/var/eval/ur/rsol/mdl':    (n_mdl, n_r, n_q),\n",
    "    #      'P00000000/var/eval/ub1/rsol/r':     (n_mdl, n_r),\n",
    "    #      'P00000000/var/eval/ub1/rsol/gt':    (n_mdl, n_r),\n",
    "    #      'P00000000/var/eval/ub1/rsol/mdl':   (n_mdl, n_r, n_q),\n",
    "    #      'P00000000/var/eval/ub2/rsol/r':     (n_mdl, n_r),\n",
    "    #      'P00000000/var/eval/ub2/rsol/gt':    (n_mdl, n_r),\n",
    "    #      'P00000000/var/eval/ub2/rsol/mdl':   (n_mdl, n_r, n_q),\n",
    "    #      'P00000000/var/eval/tv/rsol/r':      (n_mdl, n_r),\n",
    "    #      'P00000000/var/eval/tv/rsol/gt':     (n_mdl, n_r),\n",
    "    #      'P00000000/var/eval/tv/rsol/mdl':    (n_mdl, n_r, n_q)}\n",
    "\n",
    "    return savedata\n",
    "\n",
    "def get_method(df):\n",
    "    method_col = []\n",
    "    for i, row in df.iterrows():\n",
    "        method = 'Expensive' if row['srfpts/n/trg'] > 2 else 'Cheap'\n",
    "        if row['trg/btstrp']:\n",
    "            method += ' Bootstrapping'\n",
    "            if (row['trg/tau'] == 0.984) and (row['trg/reg/w'] == 2.0):\n",
    "                method += '+'\n",
    "        elif row['srfpts/dblsmpl']:\n",
    "            method += ' Double-Sampling'\n",
    "        else:\n",
    "            method += ' Standard'\n",
    "        method_col.append(method)\n",
    "    return method_col\n",
    "\n",
    "def get_solperf(rsol_r1, rsol_gt1, rsol_mdl1, \n",
    "    device_name=None, n_eval=10000):\n",
    "    if device_name is None:\n",
    "        device_name = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    tch_device = torch.device(device_name)\n",
    "    \n",
    "    n_r, n_q = rsol_mdl1.shape[-2:]\n",
    "    odims = rsol_mdl1.shape[:-2]\n",
    "    n_cfgseeds = np.prod(odims)\n",
    "    \n",
    "    rsol_r2_all = rsol_r1.reshape(n_cfgseeds, n_r)\n",
    "    assert rsol_r2_all.shape == (n_cfgseeds, n_r)\n",
    "    rsol_gt2_all = rsol_gt1.reshape(n_cfgseeds, n_r, 1)\n",
    "    assert rsol_gt2_all.shape == (n_cfgseeds, n_r, 1)\n",
    "    rsol_mdl2_all = rsol_mdl1.reshape(n_cfgseeds, n_r, n_q)\n",
    "    assert rsol_mdl2_all.shape == (n_cfgseeds, n_r, n_q)\n",
    "\n",
    "    n_batches = int(np.ceil(n_cfgseeds/n_eval))\n",
    "    \n",
    "    catlist = []\n",
    "    for i_mb in range(n_batches):\n",
    "        i1 = i_mb * n_eval\n",
    "        i2 = (i_mb + 1) * n_eval\n",
    "        \n",
    "        rsol_r2 = rsol_r2_all[i1:i2, ...]\n",
    "        n_mb = rsol_r2.shape[0]\n",
    "        assert rsol_r2.shape  == (n_mb, n_r)\n",
    "        \n",
    "        rsol_gt2 = rsol_gt2_all[i1:i2, ...]\n",
    "        assert rsol_gt2.shape == (n_mb, n_r, 1)\n",
    "        \n",
    "        rsol_mdl2 = rsol_mdl2_all[i1:i2, ...]\n",
    "        assert rsol_mdl2.shape == (n_mb, n_r, n_q)\n",
    "        \n",
    "        rsol_r2 = torch.from_numpy(rsol_r2).to(tch_device)\n",
    "        rsol_gt2 = torch.from_numpy(rsol_gt2).to(tch_device)\n",
    "        rsol_mdl2 = torch.from_numpy(rsol_mdl2).to(tch_device)\n",
    "        \n",
    "        # Plain\n",
    "        rsol_err1 = rsol_mdl2 - rsol_gt2\n",
    "        assert rsol_err1.shape == (n_mb, n_r, n_q)\n",
    "\n",
    "        # Unbiased\n",
    "        rsol_mdl3 = rsol_mdl2 - rsol_mdl2.mean(dim=(-1, -2), keepdims=True)\n",
    "        assert rsol_mdl3.shape == (n_mb, n_r, n_q)\n",
    "\n",
    "        rsol_gt3 = rsol_gt2 - rsol_gt2.mean(dim=(-1, -2), keepdims=True)\n",
    "        assert rsol_gt3.shape == (n_mb, n_r, 1)\n",
    "\n",
    "        rsol_err2 = rsol_mdl3 - rsol_gt3\n",
    "        assert rsol_err2.shape == (n_mb, n_r, n_q)\n",
    "\n",
    "        # Unbiased\n",
    "        rsol_mdl4 = rsol_mdl3 / rsol_mdl3.std(dim=(-1, -2), keepdims=True)\n",
    "        assert rsol_mdl4.shape == (n_mb, n_r, n_q)\n",
    "\n",
    "        rsol_gt4 = rsol_gt3 / rsol_gt3.std(dim=(-1, -2), keepdims=True)\n",
    "        assert rsol_gt4.shape == (n_mb, n_r, 1)\n",
    "\n",
    "        rsol_err3 = rsol_mdl4 - rsol_gt4\n",
    "        assert rsol_err3.shape == (n_mb, n_r, n_q)\n",
    "\n",
    "\n",
    "        rsol_mse1 = rsol_err1.square().mean(dim=(-1, -2))\n",
    "        assert rsol_mse1.shape == (n_mb,)\n",
    "\n",
    "        rsol_mae1 = rsol_err1.abs().mean(dim=(-1, -2))\n",
    "        assert rsol_mae1.shape == (n_mb,)\n",
    "\n",
    "        rsol_mse2 = rsol_err2.square().mean(dim=(-1, -2))\n",
    "        assert rsol_mse2.shape == (n_mb,)\n",
    "\n",
    "        rsol_mae2 = rsol_err2.abs().mean(dim=(-1, -2))\n",
    "        assert rsol_mae2.shape == (n_mb,)\n",
    "\n",
    "        rsol_mse3 = rsol_err3.square().mean(dim=(-1, -2))\n",
    "        assert rsol_mse2.shape == (n_mb,)\n",
    "\n",
    "        rsol_mae3 = rsol_err3.abs().mean(dim=(-1, -2))\n",
    "        assert rsol_mae2.shape == (n_mb,)\n",
    "        \n",
    "        meanerrs = (rsol_mse1, rsol_mae1, rsol_mse2, rsol_mae2, rsol_mse3, rsol_mae3)\n",
    "        catlist.append(tuple(v.detach().cpu().numpy() for v in meanerrs))\n",
    "    \n",
    "    names = ('pln/mse', 'pln/mae', 'bc/mse', 'bc/mae', 'bcn/mse', 'bcn/mae')\n",
    "    aa = [[x[i] for x in catlist] for i in range(len(names))]\n",
    "    outdict = dict()\n",
    "    for name, valslist in zip(names, aa):\n",
    "        val = np.concatenate(valslist, axis=0)\n",
    "        val = val.reshape(*odims, *val.shape[1:])\n",
    "        outdict[name] = val\n",
    "        \n",
    "    return outdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = './20_hdpviz'\n",
    "! mkdir -p {workdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Evaluation Point Radius Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpidx = '01_poisson/06_hidim.0.0'\n",
    "rio = resio(fpidx, full=False, driver=None)\n",
    "hpdf = rio('hp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = 2, 3\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*4,n_rows*3), \n",
    "    dpi=100, sharex=False, sharey=False)\n",
    "axes = np.array(axes).reshape(n_rows, n_cols).reshape(-1)\n",
    "rid_list = ['ur', 'ub1', 'ub2', 'tv']\n",
    "n_r = 50\n",
    "for ax_idx, (yaxis,  dim) in enumerate(product(['cdf', 'pdf'], [2, 4, 10])):\n",
    "    ax = axes[ax_idx]\n",
    "    rdict = get_rsamps(n_r=n_r, dim=dim, hpdf=hpdf)\n",
    "    q = rdict['q']\n",
    "    dq = np.diff(q)\n",
    "    r_min = min(rdict[rid][0] for rid in rid_list)\n",
    "    r_max = max(rdict[rid][-1] for rid in rid_list)\n",
    "    for rid in rid_list:\n",
    "        rp = rdict[rid]\n",
    "        if yaxis == 'cdf':\n",
    "            x = [r_min] + rp.tolist() + [r_max] \n",
    "            y = [0.0] + q.tolist() + [1.0]\n",
    "        elif yaxis == 'pdf':\n",
    "            r = rp[:-1] + np.diff(rp)/2\n",
    "            rpdf = dq / np.diff(rp)\n",
    "            x = [r_min, r[0]] + r.tolist() + [r[-1], r_max]\n",
    "            y = [0.0, 0.0] + rpdf.tolist() + [0.0, 0.0]\n",
    "        else:\n",
    "            raise ValueError(f'yaxis={yaxis} undef')\n",
    "        ax.plot(x, y, label=rid, lw=2)\n",
    "\n",
    "    ax.grid()\n",
    "    ax.set_ylabel(yaxis.upper())\n",
    "    ax.set_xlabel('Radius')\n",
    "    ax.set_title(f'{dim}-Dimensional Problem')\n",
    "\n",
    "ax.legend()\n",
    "fig.set_tight_layout(True)\n",
    "fig.savefig(f'{workdir}/00_rdistr.pdf', dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the Dashboard Data\n",
    "\n",
    "**Context**: \n",
    "\n",
    "We're dealing with a single Dirac-delta charge located at zero in a poisson problem with 2-10 dimensions.\n",
    "\n",
    "**Objective**: \n",
    "\n",
    "* We realized that sampling the radius of the evaluation points in a deterministic manner can significantly affect the results. \n",
    "\n",
    "* The training code only performed IID radius sampling, so we needed to supplement the results with a deterministic radius sampling evaluation. \n",
    "\n",
    "* IID radius sampling induced a lot of bias into the process and the normalized MSE statistics(i.e., correlations).\n",
    "    \n",
    "    * The range of radi changed from one evaluation to another, rendering the MSE statistics in higher-dimensions \n",
    "    \n",
    "      so noisy that no training trend could be observed without normalizing the solutions before calculating the MSEs.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "* The next cell will load the neural network checkpoints, and evaluate them on a bunch of points with deterministically sampled radi.\n",
    "\n",
    "* The radius resolution is determined by the `n_r` hyper-parameter; the numbe of radi we will use in point evaluations.\n",
    "\n",
    "* Due to storage limitations, we use `n_q=5` and only store five percentiles \n",
    "  \n",
    "  (min, lower-quartile, median, higher-quartile, and max) for the 1000 sampled point solutions at each radius.\n",
    "\n",
    "    * This was done for the \n",
    "      \n",
    "      `['ur', 'ub1', 'ub2', 'tv']` \n",
    "      \n",
    "      evaluation profiles.\n",
    "\n",
    "* We only stored the `tv` evaluation profile with `n_q=101` for a more accurate downstream analysis. \n",
    "    \n",
    "    * This accurate version with `n_q = 101` was labeled as `tv2` in the hdf files.\n",
    "\n",
    "        * Example: In the `results/06_hidim_00.h5` file, there is a key named \n",
    "            \n",
    "            `'P00000000/var/eval/tv2/rsol/mdl'`\n",
    "            \n",
    "            which contains an array with the shape `(n_mdl, n_r, 101)`.\n",
    "    \n",
    "    * The original version with `n_q = 5` was labeled as `tv` in the hdf files.\n",
    "\n",
    "        * Example: In the `results/06_hidim_00.h5` file, there is a key named \n",
    "            \n",
    "            `'P00000000/var/eval/tv/rsol/mdl'`\n",
    "            \n",
    "            which contains an array with the shape `(n_mdl, n_r, 5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_r = 50\n",
    "n_eval = 1000\n",
    "# Note: only for the 'tv' evaluation profile, we used \n",
    "#       'n_q' equal to both 5 and 101, and named them\n",
    "#       'tv' and 'tv2' in the hdf files, respectively.\n",
    "n_q = 5\n",
    "eid_list = ['ur', 'ub1', 'ub2', 'tv']\n",
    "n_seeds = 100\n",
    "device_name = 'cuda:0'\n",
    "\n",
    "cfg_tree = ['01_poisson/06_hidim', '01_poisson/07_hidim', '01_poisson/08_hidim']\n",
    "rio = resio(cfg_tree, full=True, driver=None)\n",
    "dtdict = rio.dtypes()\n",
    "\n",
    "hdfpath2data = defaultdict(dict)\n",
    "for fpidx, key2dtype in hie2deep(dtdict, sep=':').items():\n",
    "    cfg_tree, fidx, pidx = fpidx.split('.')\n",
    "    fidx, pidx = int(fidx), int(pidx)\n",
    "    pats = [f'var/eval/{eid}/rsol/*' for eid in eid_list]\n",
    "    exists_rsol = all(any(fnmatch.fnmatch(key, pat) \n",
    "                          for key in key2dtype)\n",
    "                      for pat in pats)\n",
    "    if (not exists_rsol):\n",
    "        print(f'Working on {fpidx}...')\n",
    "        fpidx = f'{cfg_tree}.{fidx}.{pidx}'\n",
    "        fpisavedata = eval_rsol(fpidx, n_r, n_eval, n_q, \n",
    "            eid_list, n_seeds, device_name)\n",
    "        hdfpath = f'{results_dir}/{cfg_tree}_{fidx:02d}.h5'\n",
    "        hdfpath2data[hdfpath].update(fpisavedata)\n",
    "        print(f'Finished working on {fpidx}.')\n",
    "\n",
    "if len(hdfpath2data) > 0:\n",
    "    # Closing the resio instance since we have to write down the results.\n",
    "    rio.close()\n",
    "    \n",
    "    for hdfpath, savedata in hdfpath2data.items():\n",
    "        with h5py.File(hdfpath, mode=\"a\") as h5hdf:\n",
    "            for key, np_arr in savedata.items():\n",
    "                if n_q == 101:\n",
    "                    key = key.replace('/tv/', '/tv2/')\n",
    "                if key in h5hdf:\n",
    "                    h5hdf[key][...] = np_arr\n",
    "                else:\n",
    "                    h5hdf.create_dataset(\n",
    "                        key,\n",
    "                        shape=np_arr.shape,\n",
    "                        dtype=np_arr.dtype,\n",
    "                        data=np_arr,\n",
    "                        compression=\"gzip\",\n",
    "                        compression_opts=0,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the MSE and MAE Values for the Above Evaluations\n",
    "\n",
    "The following will compute the MSE and MAE values for the above-generated solutions (i.e., the ones evaluation at deterministically-sampled radi). \n",
    "\n",
    "The results will be cached into the `13_hdpviz/02_hdpviz.h5` HDF file for easier access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = f'{workdir}/04_hdpviz.h5'\n",
    "if exists(cache_path):\n",
    "    cache_data = load_h5data(cache_path, driver=None)\n",
    "    hpdf = cache_data['hp']\n",
    "    stdf = cache_data['stat']\n",
    "else:\n",
    "    hpdf1, hpinfo = rio('hp', ret_info=True)\n",
    "    hpdf1['fpidx'] = hpinfo['fpidx']\n",
    "    hpdf2 = hpdf1.groupby('fpidx').agg('last').reset_index()\n",
    "\n",
    "    espdf_lst =[]\n",
    "    for eid in ['ur', 'ub1', 'ub2', 'tv', 'tv2']:\n",
    "        print('.', end='')\n",
    "        rsol_r1,   info1 = rio(f'var/eval/{eid}/rsol/r', ret_info=True)\n",
    "        rsol_gt1,  info2 = rio(f'var/eval/{eid}/rsol/gt', ret_info=True)\n",
    "        rsol_mdl1, info3 = rio(f'var/eval/{eid}/rsol/mdl', ret_info=True)\n",
    "\n",
    "        assert info1.equals(info2)\n",
    "        assert info1.equals(info3)\n",
    "        info4 = info1\n",
    "\n",
    "        n_r, n_q = rsol_mdl1.shape[1:]\n",
    "        assert rsol_r1.shape[1:]   == (n_r,)\n",
    "        assert rsol_gt1.shape[1:]  == (n_r,)\n",
    "        assert rsol_mdl1.shape[1:] == (n_r, n_q)\n",
    "\n",
    "        info4 = info4.reset_index(drop=True)\n",
    "        info5 = info4.sort_values(by=['fpidx', 'epoch', 'rng_seed'])\n",
    "        a = [df.shape[0] for _, df in info5.groupby(['fpidx', 'epoch'])]\n",
    "        assert len(set(a)) == 1\n",
    "        n_seeds = a[0]\n",
    "\n",
    "        if not (info5.index.values == np.arange(info5.shape[0])).all():\n",
    "            rsol_mdl1 = rsol_mdl1[info5.index.values]\n",
    "            rsol_gt1 = rsol_gt1[info5.index.values]\n",
    "            rsol_r1 = rsol_r1[info5.index.values]\n",
    "        info6 = info5.reset_index(drop=True)\n",
    "\n",
    "        n_cfg = rsol_r1.shape[0] // n_seeds\n",
    "\n",
    "        spdict = get_solperf(rsol_r1, rsol_gt1, rsol_mdl1, n_eval=10000)\n",
    "        # Example:\n",
    "        #   spdict = \n",
    "        #     {pln/mse: np.array.shape(364500,)\n",
    "        #      pln/mae: np.array.shape(364500,)\n",
    "        #      bc/mse: np.array.shape(364500,)\n",
    "        #      bc/mae: np.array.shape(364500,)\n",
    "        #      bcn/mse: np.array.shape(364500,)\n",
    "        #      bcn/mae: np.array.shape(364500,)}\n",
    "\n",
    "        spdict2 = {f'perf/det/{eid}/mdl/{key}':val \n",
    "                for key, val in spdict.items()}\n",
    "        espdf = pd.DataFrame(spdict2)\n",
    "        espdf = pd.concat([info6, espdf], axis=1)\n",
    "        espdf_lst.append(espdf)\n",
    "        \n",
    "    info_cols = ['fpidx', 'epoch', 'rng_seed']\n",
    "    espdfs = [espdf.set_index(info_cols) for espdf in espdf_lst]\n",
    "    espdf = pd.concat(espdfs, axis=1).reset_index()\n",
    "\n",
    "    fpdf = espdf[['fpidx', 'epoch', 'rng_seed']]\n",
    "    ostdf1 = rio('stat', fpidx=fpdf)\n",
    "\n",
    "    assert (fpdf['epoch'] == ostdf1['epoch']).all()\n",
    "    assert (fpdf['rng_seed'] == ostdf1['rng_seed']).all()\n",
    "    ostdf2 = pd.concat([fpdf, ostdf1.drop(columns=['epoch', 'rng_seed'])], axis=1)\n",
    "    rnmdict = {col: col.replace('perf/', 'perf/iid/') for col in ostdf2.columns}\n",
    "    ostdf = ostdf2.rename(columns=rnmdict)\n",
    "\n",
    "    aa = [ostdf.set_index(info_cols), espdf.set_index(info_cols)]\n",
    "    newstdf = pd.concat(aa, axis=1).reset_index().drop(columns=['ioidx'])\n",
    "    hpdf = hpdf2.set_index('fpidx').loc[newstdf['fpidx'], :].reset_index()\n",
    "    hpdf['fpidxgrp'] = hpdf['fpidx']\n",
    "    stdf = newstdf.drop(columns='fpidx')\n",
    "    \n",
    "    cache_data = {'hp': hpdf, 'stat': stdf}\n",
    "    save_h5data(cache_data, cache_path, driver=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting the Data for the Poisson Solution Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = f'{workdir}/03_hdpviz.h5'\n",
    "if exists(cache_path):\n",
    "    cache_data = load_h5data(cache_path, driver=None)\n",
    "    hpdf = cache_data['hp']\n",
    "    stdf = cache_data['stat']\n",
    "else:\n",
    "    eid_list = ['ur', 'ub1', 'ub2', 'tv']\n",
    "    \n",
    "    hpdf1, hpinfo = rio('hp', ret_info=True)\n",
    "    hpdf1['fpidx'] = hpinfo['fpidx']\n",
    "    hpdf2 = hpdf1.groupby('fpidx').agg('last').reset_index()\n",
    "    hpdf3 = drop_unqcols(hpdf2)\n",
    "    hpdf3['method'] = get_method(hpdf3)\n",
    "\n",
    "    estdf_list, ehpdf_list = [], []\n",
    "    for eid in eid_list:\n",
    "        print('.', end='', flush=True)\n",
    "        rsol_r1,   info1 = rio(f'var/eval/{eid}/rsol/r', ret_info=True)\n",
    "        rsol_gt1,  info2 = rio(f'var/eval/{eid}/rsol/gt', ret_info=True)\n",
    "        rsol_mdl1, info3 = rio(f'var/eval/{eid}/rsol/mdl', ret_info=True)\n",
    "\n",
    "        assert info1.equals(info2)\n",
    "        assert info1.equals(info3)\n",
    "        info4 = info1\n",
    "\n",
    "        n_r, n_q = rsol_mdl1.shape[1:]\n",
    "        assert rsol_r1.shape[1:]   == (n_r,)\n",
    "        assert rsol_gt1.shape[1:]  == (n_r,)\n",
    "        assert rsol_mdl1.shape[1:] == (n_r, n_q)\n",
    "\n",
    "        info4 = info4.reset_index(drop=True)\n",
    "        info5 = info4.sort_values(by=['fpidx', 'epoch', 'rng_seed'])\n",
    "        a = [df.shape[0] for _, df in info5.groupby(['fpidx', 'epoch'])]\n",
    "        assert len(set(a)) == 1\n",
    "        n_seeds = a[0]\n",
    "\n",
    "        if not (info5.index.values == np.arange(info5.shape[0])).all():\n",
    "            rsol_mdl1 = rsol_mdl1[info5.index.values]\n",
    "            rsol_gt1 = rsol_gt1[info5.index.values]\n",
    "            rsol_r1 = rsol_r1[info5.index.values]\n",
    "        info6 = info5.reset_index(drop=True)\n",
    "\n",
    "        # average-aggregating the rng_seeds\n",
    "        info7 = info6.iloc[::n_seeds].drop(columns='rng_seed')\n",
    "        info = info7.reset_index(drop=True)\n",
    "        rsol_r = rsol_r1.reshape(-1, n_seeds, n_r).mean(axis=1)\n",
    "        rsol_gt = rsol_gt1.reshape(-1, n_seeds, n_r).mean(axis=1)\n",
    "        rsol_mdl = rsol_mdl1.reshape(-1, n_seeds, n_r, n_q).mean(axis=1)\n",
    "        n_cfg = rsol_r.shape[0]\n",
    "        assert rsol_r.shape   == (n_cfg, n_r)\n",
    "        assert rsol_gt.shape  == (n_cfg, n_r)\n",
    "        assert rsol_mdl.shape == (n_cfg, n_r, n_q)\n",
    "\n",
    "        x = rsol_r.reshape(n_cfg*n_r)\n",
    "        y1 = rsol_mdl.reshape(n_cfg*n_r, n_q)\n",
    "        y2 = rsol_gt.reshape(n_cfg*n_r)\n",
    "        estdict = {'x': x,\n",
    "                'y1/mean': y1[:, n_q//2],\n",
    "                'y1/low': y1[:, 0],\n",
    "                'y1/high': y1[:, -1],\n",
    "                'y2': y2,\n",
    "                'eid': eid}\n",
    "        estdf = pd.DataFrame(estdict)\n",
    "        \n",
    "        hpdf4 = hpdf3.set_index('fpidx').loc[info['fpidx'], :].reset_index()\n",
    "        hpdf4['epoch'] = info['epoch']\n",
    "\n",
    "        hpdf5 = hpdf4.iloc[np.arange(n_cfg).repeat(n_r), :]\n",
    "        hpdf5 = hpdf5.reset_index(drop=True)\n",
    "        ehpdf = hpdf5.drop(columns=['fpidx'])\n",
    "        \n",
    "        estdf_list.append(estdf)\n",
    "        ehpdf_list.append(ehpdf)\n",
    "        \n",
    "    stdf = pd.concat(estdf_list, axis=0, ignore_index=True)\n",
    "    hpdf = pd.concat(ehpdf_list, axis=0, ignore_index=True)\n",
    "    \n",
    "    cache_data = {'hp': hpdf, 'stat': stdf}\n",
    "    save_h5data(cache_data, cache_path, driver=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking the hyper-parameter and statistics data-frames\n",
    "fdf = pd.concat([hpdf, stdf], axis=1)\n",
    "\n",
    "# Lowering the storage requirement by droping some epochs \n",
    "i1 = (fdf['epoch'] % 20000) == 0\n",
    "i2 = (fdf['epoch'] < 40000)\n",
    "# Making sure only the following versions of bootstrapping parameters make it to the dash\n",
    "i3 = ((fdf['trg/tau'] == 0.999) & (fdf['trg/reg/w'] == 5.0) & (fdf['trg/w'] == 0.99))\n",
    "i4 = ((fdf['trg/tau'] == 0.984) & (fdf['trg/reg/w'] == 2.0) & (fdf['trg/w'] == 0.99))\n",
    "i5 = (fdf['trg/btstrp'] != True) | i3 | i4\n",
    "fdf = fdf[(i1 | i2) & i5]\n",
    "\n",
    "# Dropping the unneeded columns\n",
    "fdf = fdf.drop(columns=['vol/n', 'srfpts/n/mdl', 'srfpts/n/trg', \n",
    "    'srfpts/dblsmpl', 'trg/btstrp', 'trg/tau', 'trg/reg/w', 'trg/w'])\n",
    "fdf = fdf.reset_index(drop=True)\n",
    "\n",
    "# Renaming the columns and the menu contents\n",
    "a = ['ur', 'ub1', 'ub2', 'tv']\n",
    "b = ['Uniform Radius', 'Uniform Ball (R=1)', \n",
    "     'Uniform Ball (R=Sqrt(Dim))', 'Training Volumes']\n",
    "fdf['eid'] = fdf['eid'].replace(a, b)\n",
    "col_renames = {'eid': 'Evaluation Profile', 'dim': 'Dimension', 'epoch': 'Epoch'}\n",
    "fdf = fdf.rename(columns=col_renames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds = {'Evaluation Profile': 'Training Volumes', 'Epoch':  200000}\n",
    "i1 = get_dfidxs(fdf, conds)\n",
    "pltdf = fdf.copy(deep=True).loc[i1].reset_index(drop=True)\n",
    "\n",
    "nrows, ncols = 2, 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*2.1, nrows*1.8), \n",
    "    dpi=100, sharex=False, sharey='row', constrained_layout = True)\n",
    "axes = np.array(axes).reshape(nrows, ncols)\n",
    "\n",
    "snspald = {'blue': '#001c7f', 'orange': '#b1400d', 'green': '#12711c', \n",
    "    'red': '#8c0800', 'purple': '#591e71', 'brown': '#592f0d', 'pink': '#a23582',\n",
    "    'gray': '#3c3c3c', 'yellow': '#b8850a', 'cyan': '#006374', 'black': '#000000'}\n",
    "\n",
    "pltcfgs = [(2,  'Cheap Bootstrapping', '2-D Bootstrapping\\n          (N=1)'), \n",
    "           (2,  'Expensive Standard', '      2-D Standard\\n          (N=100)'), \n",
    "           (2,  'Expensive Double-Sampling', ' 2-D Dbl Sampling\\n         (N=100)'),\n",
    "           (10, 'Cheap Bootstrapping', '10-D Bootstrapping\\n          (N=1)'), \n",
    "           (10, 'Expensive Standard', '    10-D Standard\\n          (N=100)'), \n",
    "           (10, 'Expensive Double-Sampling', '10-D Dbl Sampling\\n         (N=100)')]\n",
    "for i_ax, (dim, method, ttl) in enumerate(pltcfgs):\n",
    "    ax = axes.ravel()[i_ax]\n",
    "    i2 = get_dfidxs(pltdf, {'Dimension': dim, 'method': method})\n",
    "    pltdf2 = pltdf.loc[i2]\n",
    "    y2 = pltdf2['y2'].values\n",
    "    y1, y1l, y1h = pltdf2['y1/mean'].values, pltdf2['y1/low'].values, pltdf2['y1/high'].values\n",
    "    y2p = y2 - y2.mean() + y2.mean()\n",
    "    y1p = y1 - y1.mean() + y2.mean()\n",
    "    y1lp = y1l - y1.mean() + y2.mean()\n",
    "    y1hp = y1h - y1.mean() + y2.mean()\n",
    "    ax.plot(pltdf2['x'], y2p, color=snspald['red'], lw=2)\n",
    "    ax.plot(pltdf2['x'], y1p, color=snspald['blue'])\n",
    "    ax.fill_between(pltdf2['x'], y1lp, y1hp, color=snspald['blue'], alpha=0.3)\n",
    "\n",
    "    ttl1, ttl2 = ttl.split('\\n')\n",
    "    ax.set_title(ttl1.strip())\n",
    "    ax.annotate(xy=(0.35, 0.87), text=ttl2.strip(), xytext=(0.35, 0.87), \n",
    "        xycoords='axes fraction', textcoords='axes fraction')\n",
    "\n",
    "    if dim == 10:\n",
    "        ax.set_xticks([1.0, 1.5])\n",
    "        ax.set_ylim(-0.12, 0.05)\n",
    "        ax.set_xlabel('Radius')\n",
    "        ax.annotate(xytext=(1.15, -0.055), xy=(1.65, -0.001), text='Ground\\n  Truth', \n",
    "             arrowprops=dict(arrowstyle=\"-|>\", relpos=(1.0, 0.5), connectionstyle=\"arc3,rad=0.2\"))\n",
    "\n",
    "        y_arrow = [-0.035, -0.028, -0.02][i_ax % ncols]\n",
    "        ax.annotate(xytext=(1.0, -0.1), xy=(0.9, y_arrow), text='Trained Model', \n",
    "             arrowprops=dict(arrowstyle=\"-|>\", relpos=(0.0, 0.1), connectionstyle=\"arc3,rad=-0.1\"))\n",
    "    \n",
    "    if i_ax % ncols == 0:\n",
    "        ax.set_ylabel('Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f'{workdir}/05_pred_vs_radius_hdp.pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "slider_cols = [col_renames[c] for c in ['eid', 'dim', 'epoch']]\n",
    "colors = ['#001c7f', '#b1400d', '#12711c', '#8c0800', '#591e71', \n",
    "          '#592f0d', '#a23582', '#3c3c3c', '#b8850a', '#006374']\n",
    "sharex, sharey = True, True\n",
    "drop_slidercols = True\n",
    "frame_width = 430\n",
    "frame_height = 255\n",
    "slider_width = 280\n",
    "slider_height = 50\n",
    "header = \"Poisson Solution Visualization\"\n",
    "m_top, m_right, m_bottom, m_left = (5, 15, 5, 15)\n",
    "\n",
    "# Preparing the sliders\n",
    "sliders = []\n",
    "for icol, col in enumerate(slider_cols):\n",
    "    all_vals = fdf[col]\n",
    "    vals = all_vals.unique() \n",
    "    if np.issubdtype(fdf[col].dtype, np.number):\n",
    "        vals = np.sort(vals)\n",
    "        step = np.gcd.reduce(vals)\n",
    "        isval = vals[0]\n",
    "        margin = (m_top, m_right, m_bottom, m_left)\n",
    "        slider = Slider(start=vals[0], end=vals[-1], value=isval, \n",
    "            step=step, title=col, width=slider_width, \n",
    "            height=slider_height, min_height=slider_height, \n",
    "            margin=margin, sizing_mode='fixed')\n",
    "    else:\n",
    "        margin = (m_top, m_right, m_bottom, m_left)\n",
    "        slider = Select(options=vals.tolist(), value=np.sort(vals)[0], \n",
    "            title=col, margin=margin,\n",
    "            width=200, height=slider_height, sizing_mode='fixed')\n",
    "        slider.align = 'end'\n",
    "    sliders.append(slider)\n",
    "    \n",
    "# The centralization / normalization control button\n",
    "margin = (m_top, m_right, m_bottom, m_left)\n",
    "norm_chckboxgrp = CheckboxButtonGroup(labels=[\"Centralize\", \"Normalize\"], \n",
    "    active=[0], width=200, margin=margin)\n",
    "\n",
    "tab_figs = []\n",
    "all_cbargs = []\n",
    "\n",
    "n_tabfig = len(fdf['method'].unique().tolist())\n",
    "n_rows = int(np.ceil(n_tabfig / ncols))\n",
    "screen_width = int(frame_width*(ncols+0.1))\n",
    "screen_hight = int(frame_height*(n_rows+0.1))\n",
    "for fig_idx, (method, df) in enumerate(fdf.groupby('method', sort=False, as_index=False)):\n",
    "    ax_row, ax_col = fig_idx // ncols, fig_idx % ncols\n",
    "\n",
    "    show_xaxis = not(sharex) or (ax_row == (n_rows-1)) \n",
    "    aaa = ((ax_row == (n_rows-2)) and ((fig_idx+n_cols+1)>n_tabfig))\n",
    "    show_xaxis = show_xaxis or aaa\n",
    "    show_yaxis = not(sharey) or (ax_col == 0)\n",
    "    \n",
    "    df = df.drop(columns='method')\n",
    "    sv2sdf = [(sv, sdf.reset_index(drop=True)) \n",
    "        for sv, sdf in df.groupby(slider_cols)]\n",
    "    if drop_slidercols:\n",
    "        sv2sdf = [(sv, sdf.drop(columns=slider_cols)) \n",
    "                  for sv, sdf in sv2sdf]\n",
    "    sv2sdf = sorted(sv2sdf, key=lambda p: p[0])\n",
    "\n",
    "    svlist = [list(sv) for sv, sdf in sv2sdf]\n",
    "    sdflist = [sdf for sv, sdf in sv2sdf]\n",
    "    ssrclist = [ColumnDataSource(sdf) for sdf in sdflist]\n",
    "    \n",
    "    # The Centralization / Normalization Data\n",
    "    dd = {'y1_mean': [sdf[['y1/mean']].values.mean().item() for sdf in sdflist], \n",
    "          'y1_std': [sdf[['y1/mean']].values.std().item() for sdf in sdflist], \n",
    "          'y2_mean': [sdf[['y2']].values.mean().item() for sdf in sdflist], \n",
    "          'y2_std': [sdf[['y2']].values.std().item() for sdf in sdflist]}\n",
    "    norm_info_d = dd\n",
    "    norm_info = ColumnDataSource(norm_info_d)\n",
    "\n",
    "    # Figure and Tools Creation\n",
    "    zoomtool = BoxZoomTool()\n",
    "    figtools = [zoomtool, 'reset,pan,wheel_zoom']\n",
    "    fig = figure(title=method, tools=figtools, \n",
    "        frame_width=frame_width, frame_height=frame_height)\n",
    "    fig.toolbar.active_drag = zoomtool\n",
    "    \n",
    "    # Defining the source\n",
    "    initsdf = sdflist[0]\n",
    "    for col in ('y1/mean', 'y1/low', 'y1/high'):\n",
    "        initsdf[col] -= norm_info_d['y1_mean'][0]\n",
    "    initsdf['y2'] -= norm_info_d['y2_mean'][0]\n",
    "    source = ColumnDataSource(initsdf)\n",
    "\n",
    "    # Plotting the lines\n",
    "    clr1, clr2 = colors[0], colors[3]\n",
    "    line1 = fig.line('x', 'y1/mean', source=source, \n",
    "        color=clr1, line_width=3)\n",
    "    band = Band(base='x', lower=f'y1/low', \n",
    "        upper=f'y1/high', source=source,\n",
    "        fill_alpha=0.2, fill_color=clr1)\n",
    "    fig.add_layout(band)\n",
    "    line2 = fig.line('x', 'y2', source=source, \n",
    "        color=clr2, line_width=1)\n",
    "    fig.star_dot('x', 'y2', source=source, color=clr2, size=7)\n",
    "\n",
    "    # Adding the tooltips\n",
    "    hover_opts = dict(show_arrow=False,\n",
    "        line_policy='next', mode='mouse', toggleable=False)\n",
    "    # tooltips = [(col, '@{'+col+'}') for col in slider_cols]\n",
    "    fig.add_tools(HoverTool(renderers=[line1], \n",
    "        tooltips=[('Solution', 'Model')], **hover_opts))\n",
    "    fig.add_tools(HoverTool(renderers=[line2], \n",
    "        tooltips=[('Solution', 'Ground Truth')], **hover_opts))\n",
    "    \n",
    "    # Figure Customization\n",
    "    fig.outline_line_color = 'black'\n",
    "    fig.min_border = 10\n",
    "    \n",
    "    fontsize = \"10pt\"\n",
    "    if show_xaxis:\n",
    "        fig.xaxis.axis_label = 'Radius'\n",
    "        fig.xaxis.axis_label_text_font_size = fontsize\n",
    "        fig.xaxis.major_label_text_font_size = fontsize\n",
    "        fig.xaxis.axis_label_text_color = \"black\"\n",
    "    else:\n",
    "        fig.xaxis.visible = False\n",
    "\n",
    "    if show_yaxis:\n",
    "        fig.yaxis.axis_label = 'Solution'\n",
    "        fig.yaxis.axis_label_text_font_size = fontsize\n",
    "        fig.yaxis.major_label_text_font_size = fontsize\n",
    "        fig.yaxis.axis_label_text_color = \"black\"\n",
    "    else:\n",
    "        fig.yaxis.visible = False\n",
    "\n",
    "    # Javascript callback arguments\n",
    "    cp_cols = ['x', 'y1/mean', 'y1/low', 'y1/high', 'y2']\n",
    "    if not drop_slidercols:\n",
    "        cp_cols += slider_cols\n",
    "    figcb_args = dict(source=source, ssrclist=ssrclist, svlist=svlist, \n",
    "        sliders=sliders, cp_cols=cp_cols, \n",
    "        norm_info=norm_info, \n",
    "        norm_chckboxgrp=norm_chckboxgrp\n",
    "        )\n",
    "    all_cbargs.append(figcb_args)\n",
    "    tab_figs.append(fig)\n",
    "\n",
    "# Sharing the range of all variables\n",
    "all_figs = tab_figs\n",
    "if sharey:\n",
    "    for fig in all_figs:\n",
    "        fig.y_range = all_figs[0].y_range\n",
    "if sharex:\n",
    "    for fig in all_figs:\n",
    "        fig.x_range = all_figs[0].x_range    \n",
    "\n",
    "jscode=\"\"\"\n",
    "    var source, ssrclist, svlist; \n",
    "    var sliders, cp_cols, figcb_args;\n",
    "    var aa, bb, svi;\n",
    "    var newsrc, col, newcol, srccol;\n",
    "    var colbias, colscale;\n",
    "    var norm_info, norm_chckboxgrp;\n",
    "    var do_cntralize, do_normalize;\n",
    "    \n",
    "    for (let kk = 0; kk < all_cbargs.length; kk++) {\n",
    "        figcb_args = all_cbargs[kk];\n",
    "        source = figcb_args[\"source\"];\n",
    "        ssrclist = figcb_args[\"ssrclist\"];\n",
    "        svlist = figcb_args[\"svlist\"];\n",
    "        sliders = figcb_args[\"sliders\"];\n",
    "        cp_cols = figcb_args[\"cp_cols\"];\n",
    "        \n",
    "        norm_info = figcb_args[\"norm_info\"];\n",
    "        norm_chckboxgrp = figcb_args[\"norm_chckboxgrp\"];\n",
    "        do_cntralize = (norm_chckboxgrp.active.indexOf(0) > -1);\n",
    "        do_normalize = (norm_chckboxgrp.active.indexOf(1) > -1);\n",
    "        \n",
    "        svi = 0;        \n",
    "        for (let i = 0; i < svlist.length; i++) {\n",
    "            aa = svlist[i];\n",
    "            bb = true;\n",
    "            for (let j = 0; j < aa.length; j++) {\n",
    "                bb = bb && (aa[j] <= sliders[j].value);\n",
    "            }\n",
    "            if (bb === true) { svi = i; }\n",
    "        }\n",
    "        \n",
    "        newsrc = ssrclist[svi];\n",
    "        for (let i = 0; i < cp_cols.length; i++) {\n",
    "            col = cp_cols[i];\n",
    "            \n",
    "            colbias = 0;\n",
    "            colscale = 1;\n",
    "            if ([\"y1/mean\", \"y1/low\", \"y1/high\"].indexOf(col) > -1) {\n",
    "                if (do_cntralize) { colbias = norm_info.data[\"y1_mean\"][svi]; }\n",
    "                if (do_normalize) { colscale = norm_info.data[\"y1_std\"][svi]; }\n",
    "            }\n",
    "            if ([\"y2\"].indexOf(col) > -1) {\n",
    "                if (do_cntralize) { colbias = norm_info.data[\"y2_mean\"][svi]; }\n",
    "                if (do_normalize) { colscale = norm_info.data[\"y2_std\"][svi]; }\n",
    "            }\n",
    "            \n",
    "            newcol = newsrc.data[col];\n",
    "            source.data[col] = [];\n",
    "            srccol = source.data[col];\n",
    "            for (let j = 0; j < newcol.length; j++) {\n",
    "                srccol.push((newcol[j] - colbias)/colscale);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for (let kk = 0; kk < all_cbargs.length; kk++) {\n",
    "        source = all_cbargs[kk][\"source\"];\n",
    "        source.change.emit();\n",
    "    }\n",
    "\"\"\"\n",
    "callback = CustomJS(args=dict(all_cbargs=all_cbargs), code=jscode)\n",
    "for slider in sliders:\n",
    "    slider.js_on_change('value', callback)\n",
    "norm_chckboxgrp.js_on_change('active', callback)\n",
    "\n",
    "gridfigs = gridplot(tab_figs, ncols=ncols, sizing_mode='inherit')\n",
    "\n",
    "margin = (m_top, m_right, 0, m_left)\n",
    "chckbox_title = Div(text=\"Y-axis Transformation\", margin=margin)\n",
    "\n",
    "# heading fills available width\n",
    "margin = (m_top, m_right, m_bottom, m_left)\n",
    "heading = Div(text=f'<h1 style=\"text-align: center\">{header}</h1>', \n",
    "    height=slider_height, sizing_mode='stretch_width', \n",
    "    margin=margin)\n",
    "\n",
    "margin = (0, 0, 10, 0)\n",
    "empty_text = Div(text='', height=10, \n",
    "    sizing_mode='stretch_width', margin=margin)\n",
    "    \n",
    "fulllayout = column(\n",
    "    row(heading,\n",
    "        column(chckbox_title, norm_chckboxgrp),\n",
    "        *sliders, sizing_mode='stretch_width'),\n",
    "    gridfigs,\n",
    "    sizing_mode='stretch_both'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file(f'{workdir}/03_hdpviz.html')\n",
    "save(fulllayout, title=header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 | packaged by conda-forge | (default, Nov 22 2019, 19:11:38) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38de3f176bf2b657438bbdc11cbc18610e8dfd1fbbc6e7c8cdd2eabbe631bf14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
