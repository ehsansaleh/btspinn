{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e827df9",
   "metadata": {},
   "source": [
    "## The Poisson Problem Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d52d6d",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "%matplotlib inline\n",
    "if importlib.util.find_spec(\"matplotlib_inline\") is not None:\n",
    "    import matplotlib_inline\n",
    "    matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
    "else:\n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    set_matplotlib_formats('retina')\n",
    "\n",
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import socket\n",
    "import random\n",
    "import pathlib\n",
    "import fnmatch\n",
    "import datetime\n",
    "import resource\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboardX\n",
    "import psutil\n",
    "from pyinstrument import Profiler\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "from scipy.special import gamma\n",
    "from os.path import exists, isdir\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict as odict\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a370bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bspinn.io_utils import DataWriter\n",
    "from bspinn.io_utils import get_git_commit\n",
    "from bspinn.io_utils import preproc_cfgdict\n",
    "from bspinn.io_utils import hie2deep, deep2hie\n",
    "\n",
    "from bspinn.tch_utils import isscalar\n",
    "from bspinn.tch_utils import EMA\n",
    "from bspinn.tch_utils import BatchRNG\n",
    "from bspinn.tch_utils import bffnn\n",
    "from bspinn.tch_utils import profmem\n",
    "\n",
    "from bspinn.io_cfg import configs_dir\n",
    "from bspinn.io_cfg import results_dir\n",
    "from bspinn.io_cfg import storage_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429fa7a",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Consider the $d$-dimensional space $\\mathbb{R}^{d}$, and the following charge:\n",
    "\n",
    "$$\\rho(x) = \\delta^d(x).$$\n",
    "\n",
    "For $d \\neq 2$, the analytical solution to the system\n",
    "\n",
    "$$\\nabla \\cdot \\vec{E} = \\rho$$\n",
    "\n",
    "$$\\nabla V = \\vec{E}$$\n",
    "\n",
    "can be defined as \n",
    "\n",
    "$$V_{\\vec{x}} = \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\|\\vec{x}\\|^{2-d}, $$\n",
    "\n",
    "$$\\vec{E}_{\\vec{x}} = \\frac{\\Gamma(d/2)}{2\\cdot \\pi^{d/2}\\cdot \\|\\vec{x}\\|^{d}} \\vec{x}.$$\n",
    "\n",
    "For $d=2$, $\\vec{E}_{\\vec{x}}$ is the same, but for $V_{\\vec{x}}$ we have\n",
    "\n",
    "$$V_{\\vec{x}} = \\frac{1}{2\\pi} \\ln(\\|\\vec{x}\\|).$$\n",
    "\n",
    "We want to solve this system using the divergence theorem:\n",
    "\n",
    "$$\\iint_{S_{d-1}(V)} \\vec{E}\\cdot \\hat{n}\\text{ d}S = \\iiint_{V_d} \\nabla.\\vec{E}\\text{ d}V.$$\n",
    "\n",
    "Keep in mind that the $d-1$-dimensional surface of a $d$-dimensional shpere with radius $r$ is \n",
    "$$\\iint_{S_{d-1}(V^{\\text{d-Ball}}_{r})} 1\\text{ d}S = \\frac{2\\cdot \\pi^{d/2}}{\\Gamma(d/2)}\\cdot r^{d-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697fb22",
   "metadata": {},
   "source": [
    "### Dimensionality Scaling\n",
    "\n",
    "We will assume that our domain of solution is a d-Ball centerred at zero with a radius of $r_b$.\n",
    "$$C_1 := \\int_{V_{r_b}^{d\\text{-Ball}}} 1 d\\vec{x} = \\frac{2\\pi^{d/2}}{d\\cdot\\Gamma(d/2)} r_b^d$$\n",
    "\n",
    "#### The Expectation of the Anlytical Solution\n",
    "\n",
    "$$E_v := \\int_{V_r^{d\\text{-Ball}}} V_{\\vec{x}} d\\vec{x} = \\int \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\|\\vec{x}\\|^{2-d} d\\vec{x}$$\n",
    "\n",
    "$$ = C_1 \\cdot \\int \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\|\\vec{x}\\|^{2-d} \\cdot \\frac{1}{C_1} d\\vec{x} $$\n",
    "\n",
    "$$ = C_1 \\cdot \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\int \\|\\vec{x}\\|^{2-d} \\cdot \\frac{1}{C_1} d\\vec{x} $$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\int \\|\\vec{x}\\|^{2-d} \\cdot \\frac{1}{C_1} d\\vec{x} $$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\mathbb{E}_{\\vec{x}} [\\|\\vec{x}\\|^{2-d}] $$\n",
    "\n",
    "By defining the radius of $\\vec{x}$ as $r=\\|\\vec{x}\\|$, the distribution of $r$ is\n",
    "\n",
    "$$Pr(\\|\\vec{x}\\|<r) = (\\frac{r}{r_b})^d$$\n",
    "\n",
    "$$P(\\|\\vec{x}\\|=r) = \\frac{(d-1) \\cdot r^d}{r_b^d}$$\n",
    "\n",
    "Therefore, we have\n",
    "\n",
    "$$E_v = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\mathbb{E}_{\\vec{x}} [r^{2-d}] $$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\int_{r=0}^{r_b} r^{2-d} \\frac{(d-1) \\cdot r^d}{r_b^d} dr$$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\frac{d}{r_b^d} \\int_{r=0}^{r_b} r dr$$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\frac{d}{r_b^d} \\int_{r=0}^{r_b} r dr$$\n",
    "\n",
    "$$ = \\frac{r_b^2}{2\\cdot(2-d)}$$\n",
    "\n",
    "#### The Expectation of the Volume Ratio\n",
    "\n",
    "$$\\mathbb{E}_{r\\sim U[r_l, r_h]}[(\\frac{r}{r_b})^d] = \\frac{1}{r_h - r_l} \\int_{r_l}^{r_h} (\\frac{r}{r_b})^d dr$$\n",
    "\n",
    "$$=\\frac{1}{d+1} \\cdot \\frac{1}{r_b^d} \\frac{r_h^{d+1} - r_l^{d+1}}{r_h - r_l}.$$\n",
    "\n",
    "By setting $r_h=r_b$ and $r_l < r_h$, the above value closes in on $$\\frac{1}{d+1}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e4bd0",
   "metadata": {},
   "source": [
    "### Defining the Problem and the Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cfae0c",
   "metadata": {
    "code_folding": [
     0,
     15,
     66
    ]
   },
   "outputs": [],
   "source": [
    "class DeltaProblem:\n",
    "    def __init__(self, weights, locations, tch_device, tch_dtype):\n",
    "        # weights          -> np.array -> shape=(n_bch, n_chrg)\n",
    "        # locations.shape  -> np.array -> shape=(n_bch, n_chrg, d)\n",
    "        self.weights = weights\n",
    "        self.locations = locations\n",
    "        self.n_bch, self.n_chrg = self.weights.shape\n",
    "        self.d = self.locations.shape[-1]\n",
    "        assert self.weights.shape == (self.n_bch, self.n_chrg,)\n",
    "        assert self.locations.shape == (self.n_bch, self.n_chrg, self.d)\n",
    "        self.weights_tch = torch.from_numpy(\n",
    "            self.weights).to(tch_device, tch_dtype)\n",
    "        self.locations_tch = torch.from_numpy(\n",
    "            self.locations).to(tch_device, tch_dtype)\n",
    "        self.shape = (self.n_bch,)\n",
    "        self.ndim = 1\n",
    "        self.tch_pi = torch.tensor(np.pi, device=tch_device, dtype=tch_dtype)\n",
    "\n",
    "    def integrate_volumes(self, volumes):\n",
    "        # volumes -> dictionary\n",
    "        assert volumes['type'] == 'ball'\n",
    "        centers = volumes['centers']\n",
    "        radii = volumes['radii']\n",
    "        n_v = radii.shape[-1]\n",
    "        n_bch, n_chrg, d = self.n_bch, self.n_chrg, self.d\n",
    "        assert radii.shape == (n_bch, n_v,)\n",
    "        assert centers.shape == (n_bch, n_v, d)\n",
    "        lib = torch if torch.is_tensor(centers) else np\n",
    "        mu = self.locations_tch if torch.is_tensor(centers) else self.locations\n",
    "        w = self.weights_tch if torch.is_tensor(centers) else self.weights\n",
    "\n",
    "        c_diff_mu = centers.reshape(\n",
    "            n_bch, n_v, 1, d) - mu.reshape(n_bch, 1, n_chrg, d)\n",
    "        assert c_diff_mu.shape == (n_bch, n_v, n_chrg, d)\n",
    "        distl2 = lib.sqrt(lib.square(c_diff_mu).sum(-1))\n",
    "        assert distl2.shape == (n_bch, n_v, n_chrg)\n",
    "        integ = ((distl2 < radii.reshape(n_bch, n_v, 1))\n",
    "                 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "        assert integ.shape == (n_bch, n_v)\n",
    "        return integ\n",
    "\n",
    "    def potential(self, x):\n",
    "        lib = torch if torch.is_tensor(x) else np\n",
    "        lib_pi = self.tch_pi if torch.is_tensor(x) else np.pi\n",
    "        w = self.weights_tch if torch.is_tensor(x) else self.weights\n",
    "        mu = self.locations_tch if torch.is_tensor(x) else self.locations\n",
    "        n_bch, n_chrg, d = self.n_bch, self.n_chrg, self.d\n",
    "        n_x = x.shape[-2]\n",
    "        assert x.shape == (\n",
    "            n_bch, n_x, d), f'x.shape={x.shape}, (n_bch, n_x, d)={(n_bch, n_x, d)}'\n",
    "        x_diff_mu = x.reshape(n_bch, n_x, 1, d) - \\\n",
    "            mu.reshape(self.n_bch, 1, n_chrg, d)\n",
    "        assert x_diff_mu.shape == (n_bch, n_x, n_chrg, d)\n",
    "        x_dists = lib.sqrt(lib.square(x_diff_mu).sum(-1))\n",
    "        assert x_dists.shape == (n_bch, n_x, n_chrg)\n",
    "        if d != 2:\n",
    "            poten1 = (x_dists**(2-d))\n",
    "            assert poten1.shape == (n_bch, n_x, n_chrg)\n",
    "            poten2 = (poten1 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "            assert poten2.shape == (n_bch, n_x)\n",
    "            cst = gamma(d/2) / (2*(lib_pi**(d/2)))\n",
    "            cst = cst / (2-d)\n",
    "            assert isscalar(cst)\n",
    "            poten = cst * poten2\n",
    "            assert poten.shape == (n_bch, n_x)\n",
    "        else:\n",
    "            poten1 = lib.log(x_dists)\n",
    "            assert poten1.shape == (n_bch, n_x, n_chrg)\n",
    "            poten2 = (poten1 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "            assert poten2.shape == (n_bch, n_x)\n",
    "            poten = poten2 / (2*lib_pi)\n",
    "            assert poten.shape == (n_bch, n_x)\n",
    "        return poten\n",
    "\n",
    "    def field(self, x):\n",
    "        lib = torch if torch.is_tensor(x) else np\n",
    "        lib_pi = self.tch_pi if torch.is_tensor(x) else np.pi\n",
    "        w = self.weights_tch if torch.is_tensor(x) else self.weights\n",
    "        mu = self.locations_tch if torch.is_tensor(x) else self.locations\n",
    "        n_bch, n_chrg, d = self.n_bch, self.n_chrg, self.d\n",
    "        n_x = x.shape[-2]\n",
    "        assert x.shape == (n_bch, n_x, d)\n",
    "        x_diff_mu = x.reshape(n_bch, n_x, 1, d) - \\\n",
    "            mu.reshape(n_bch, 1, n_chrg, d)\n",
    "        assert x_diff_mu.shape == (n_bch, n_x, n_chrg, d)\n",
    "        x_dists = lib.sqrt(lib.square(x_diff_mu).sum(-1))\n",
    "        assert x_dists.shape == (n_bch, n_x, n_chrg)\n",
    "        poten1 = (x_dists**(-d))\n",
    "        assert poten1.shape == (n_bch, n_x, n_chrg)\n",
    "        poten2 = (poten1 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "        assert poten2.shape == (n_bch, n_x)\n",
    "        cst = gamma(d/2) / (2*(lib_pi**(d/2)))\n",
    "        assert isscalar(cst)\n",
    "        poten = cst * poten2\n",
    "        assert poten.shape == (n_bch, n_x)\n",
    "        field = poten.reshape(n_bch, n_x, 1) * x\n",
    "        assert field.shape == (n_bch, n_x, d)\n",
    "        return field\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3319be1",
   "metadata": {},
   "source": [
    "### Defining the Volume Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5270d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BallSampler:\n",
    "    def __init__(self, c_dstr, c_params, r_dstr, r_params, batch_rng):\n",
    "        assert isinstance(c_params, dict)\n",
    "        for name, param in c_params.items():\n",
    "            msg_ = f'center param {name} is not np.array'\n",
    "            assert isinstance(param, np.ndarray), msg_\n",
    "        \n",
    "        assert isinstance(r_params, dict)\n",
    "        for name, param in r_params.items():\n",
    "            msg_ = f'radius param {name} is not np.array'\n",
    "            assert isinstance(param, np.ndarray), msg_\n",
    "\n",
    "        self.batch_rng = batch_rng\n",
    "        self.lib = batch_rng.lib\n",
    "        \n",
    "        ##############################################################\n",
    "        ################# Center Sampling Parameters #################\n",
    "        ##############################################################\n",
    "        c_params_ = c_params.copy()\n",
    "        self.c_dstr = c_dstr\n",
    "        if c_dstr == 'uniform':\n",
    "            c_low = c_params_.pop('low')\n",
    "            c_high = c_params_.pop('high')\n",
    "            \n",
    "            n_bch, dim = c_low.shape\n",
    "            \n",
    "            self.c_low_np = c_low.reshape(n_bch, 1, dim)\n",
    "            self.c_high_np = c_high.reshape(n_bch, 1, dim)\n",
    "            self.c_size_np = (self.c_high_np - self.c_low_np)\n",
    "\n",
    "            if self.lib == 'torch':\n",
    "                self.c_low_tch = torch.from_numpy(self.c_low_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_high_tch = torch.from_numpy(self.c_high_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_size_tch = torch.from_numpy(self.c_size_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            \n",
    "            self.c_low = self.c_low_np if self.lib == 'numpy' else self.c_low_tch\n",
    "            self.c_size = self.c_size_np if self.lib == 'numpy' else self.c_size_tch\n",
    "        elif c_dstr == 'normal':\n",
    "            c_loc = c_params_.pop('loc')\n",
    "            c_scale = c_params_.pop('scale')\n",
    "            \n",
    "            n_bch, dim = c_loc.shape\n",
    "            self.c_loc_np = c_loc.reshape(n_bch, 1, dim)\n",
    "            self.c_scale_np = c_scale.reshape(n_bch, 1, 1)\n",
    "            \n",
    "            if self.lib == 'torch':\n",
    "                self.c_loc_tch = torch.from_numpy(self.c_loc_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_scale_tch = torch.from_numpy(self.c_scale_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                \n",
    "            self.c_loc = self.c_loc_np if self.lib == 'numpy' else self.c_loc_tch\n",
    "            self.c_scale = self.c_scale_np if self.lib == 'numpy' else self.c_scale_tch\n",
    "        elif c_dstr == 'ball':\n",
    "            c_cntr = c_params_.pop('c')\n",
    "            c_radi = c_params_.pop('r')\n",
    "            \n",
    "            n_bch, dim = c_cntr.shape\n",
    "            self.c_cntr_np = c_cntr.reshape(n_bch, 1, dim)\n",
    "            self.c_radi_np = c_radi.reshape(n_bch, 1, 1)\n",
    "            \n",
    "            if self.lib == 'torch':\n",
    "                self.c_cntr_tch = torch.from_numpy(self.c_cntr_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_radi_tch = torch.from_numpy(self.c_radi_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                \n",
    "            self.c_cntr = self.c_cntr_np if self.lib == 'numpy' else self.c_cntr_tch\n",
    "            self.c_radi = self.c_radi_np if self.lib == 'numpy' else self.c_radi_tch\n",
    "        else:\n",
    "            raise ValueError(f'c_dstr=\"{c_dstr}\" not implemented')\n",
    "        \n",
    "        msg_ = f'Some center parameters were left unused: {list(c_params_.keys())}'\n",
    "        assert len(c_params_) == 0, msg_\n",
    "            \n",
    "        self.n_bch, self.d = n_bch, dim\n",
    "        \n",
    "        ##############################################################\n",
    "        ################# Radius Sampling Parameters #################\n",
    "        ##############################################################\n",
    "        r_params_ = r_params.copy()\n",
    "        r_low = r_params_.pop('low')\n",
    "        r_high = r_params_.pop('high')\n",
    "        \n",
    "        if r_dstr == 'uniform':\n",
    "            self.r_upow = 1.0\n",
    "        elif r_dstr == 'unifdpow':\n",
    "            self.r_upow = 1.0 / self.d\n",
    "        else:\n",
    "            raise ValueError(f'r_dstr={r_dstr} not implemented')\n",
    "\n",
    "        r_low_rshp = r_low.reshape(self.n_bch, 1)\n",
    "        r_high_rshp = r_high.reshape(self.n_bch, 1)\n",
    "        assert (r_low >= 0.0).all()\n",
    "        assert (r_high >= r_low).all()\n",
    "        \n",
    "        self.r_dstr = r_dstr\n",
    "        self.r_low_np = np.power(r_low_rshp, 1.0/self.r_upow)\n",
    "        self.r_high_np = np.power(r_high_rshp, 1.0/self.r_upow)\n",
    "        self.r_size_np = (self.r_high_np - self.r_low_np)\n",
    "        \n",
    "        if self.lib == 'torch':\n",
    "            self.r_low_tch = torch.from_numpy(self.r_low_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            self.r_high_tch = torch.from_numpy(self.r_high_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            self.r_size_tch = torch.from_numpy(self.r_size_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            \n",
    "        self.r_low = self.r_low_np if self.lib == 'numpy' else self.r_low_tch\n",
    "        self.r_size = self.r_size_np if self.lib == 'numpy' else self.r_size_tch\n",
    "        \n",
    "        msg_ = f'Some center parameters were left unused: {list(r_params_.keys())}'\n",
    "        assert len(r_params_) == 0, msg_\n",
    "\n",
    "    def __call__(self, n=1):\n",
    "        radii = self.r_low + self.r_size * \\\n",
    "            self.batch_rng.uniform((self.n_bch, n))\n",
    "        radii = radii ** self.r_upow\n",
    "        \n",
    "        if self.c_dstr == 'uniform':\n",
    "            centers = self.batch_rng.uniform((self.n_bch, n, self.d))\n",
    "            centers = centers * self.c_size + self.c_low\n",
    "        elif self.c_dstr == 'normal':\n",
    "            centers = self.batch_rng.normal((self.n_bch, n, self.d))\n",
    "            centers = centers * self.c_scale + self.c_loc\n",
    "        elif self.c_dstr == 'ball':\n",
    "            rnd1 = self.batch_rng.normal((self.n_bch, n, self.d))\n",
    "            rnd1 = rnd1 / ((rnd1**2).sum(-1, keepdims=True)**0.5)\n",
    "            \n",
    "            rnd2 = self.batch_rng.uniform((self.n_bch, n, 1))\n",
    "            rnd2 = rnd2 ** (1./self.d)\n",
    "            \n",
    "            centers = self.c_radi * rnd2 * rnd1 + self.c_cntr\n",
    "        else:\n",
    "            raise ValueError(f'c_dstr=\"{self.c_dstr}\" not implemented')\n",
    "        \n",
    "        d = dict()\n",
    "        d['type'] = 'ball'\n",
    "        d['centers'] = centers\n",
    "        d['radii'] = radii\n",
    "        return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b6294",
   "metadata": {},
   "source": [
    "### Sruface Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d890c38",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SphereSampler:\n",
    "    def __init__(self, batch_rng):\n",
    "        self.tch_dtype = batch_rng.dtype\n",
    "        self.tch_device = batch_rng.device\n",
    "        self.batch_rng = batch_rng\n",
    "\n",
    "    def np_exlinspace(self, start, end, n):\n",
    "        assert n >= 1\n",
    "        a = np.linspace(start, end, n, endpoint=False)\n",
    "        b = a + 0.5 * (end - a[-1])\n",
    "        return b\n",
    "\n",
    "    def tch_exlinspace(self, start, end, n):\n",
    "        assert n >= 1\n",
    "        a = torch.linspace(start, end, n+1,\n",
    "                           device=self.tch_device,\n",
    "                           dtype=self.tch_dtype)[:-1]\n",
    "        b = a + 0.5 * (end - a[-1])\n",
    "        return b\n",
    "\n",
    "    def __call__(self, volumes, n, do_detspacing=True):\n",
    "        # volumes -> dictionary\n",
    "        assert volumes['type'] == 'ball'\n",
    "        centers = volumes['centers']\n",
    "        radii = volumes['radii']\n",
    "        n_bch, n_v, d = centers.shape\n",
    "        use_np = not torch.is_tensor(centers)\n",
    "        assert centers.shape == (n_bch, n_v, d)\n",
    "        assert radii.shape == (n_bch, n_v)\n",
    "        assert not (use_np) or (self.batch_rng.lib == 'numpy')\n",
    "        assert use_np or (self.batch_rng.device == centers.device)\n",
    "        assert use_np or (self.batch_rng.dtype == centers.dtype)\n",
    "        assert self.batch_rng.shape == (n_bch,)\n",
    "        exlinspace = self.np_exlinspace if use_np else self.tch_exlinspace\n",
    "        meshgrid = np.meshgrid if use_np else torch.meshgrid\n",
    "        sin = np.sin if use_np else torch.sin\n",
    "        cos = np.cos if use_np else torch.cos\n",
    "        matmul = np.matmul if use_np else torch.matmul\n",
    "\n",
    "        if do_detspacing and (d == 2):\n",
    "            theta = exlinspace(0.0, 2*np.pi, n)\n",
    "            assert theta.shape == (n,)\n",
    "            theta_2d = theta.reshape(n, 1)\n",
    "            x_tilde_2d_list = [cos(theta_2d), sin(theta_2d)]\n",
    "            if use_np:\n",
    "                x_tilde_2d = np.concatenate(x_tilde_2d_list, axis=1)\n",
    "            else:\n",
    "                x_tilde_2d = torch.cat(x_tilde_2d_list, dim=1)\n",
    "            assert x_tilde_2d.shape == (n, d)\n",
    "            x_tilde_4d = x_tilde_2d.reshape(1, 1, n, d)\n",
    "            assert x_tilde_4d.shape == (1, 1, n, d)\n",
    "            x_tilde = x_tilde_4d.expand(n_bch, 1, n, d)\n",
    "            assert x_tilde.shape == (n_bch, 1, n, d)\n",
    "        elif do_detspacing and (d == 3):\n",
    "            n_sqrt = int(np.sqrt(n))\n",
    "            assert n == n_sqrt * n_sqrt, 'Need n to be int-square for now!'\n",
    "            theta_1d = exlinspace(0.0, 2*np.pi, n_sqrt)\n",
    "            unit_unif = exlinspace(0.0, 1.0, n_sqrt)\n",
    "            if use_np:\n",
    "                phi_1d = np.arccos(1-2*unit_unif)\n",
    "            else:\n",
    "                phi_1d = torch.arccos(1-2*unit_unif)\n",
    "            theta_msh, phi_msh = meshgrid(theta_1d, phi_1d)\n",
    "            assert theta_msh.shape == (n_sqrt, n_sqrt)\n",
    "            assert phi_msh.shape == (n_sqrt, n_sqrt)\n",
    "            theta_2d, phi_2d = theta_msh.reshape(n, 1), phi_msh.reshape(n, 1)\n",
    "            assert theta_2d.shape == (n, 1)\n",
    "            assert phi_2d.shape == (n, 1)\n",
    "            x_tilde_lst = [sin(phi_2d) * cos(theta),\n",
    "                           sin(phi_2d) * sin(theta), cos(phi_2d)]\n",
    "            if use_np:\n",
    "                x_tilde_2d = np.concatenate(x_tilde_lst, axis=1)\n",
    "            else:\n",
    "                x_tilde_2d = torch.cat(x_tilde_lst, dim=1)\n",
    "            assert x_tilde_2d.shape == (n, d)\n",
    "            x_tilde_4d = x_tilde_2d.reshape(1, 1, n, d)\n",
    "            assert x_tilde_4d.shape == (1, 1, n, d)\n",
    "            x_tilde = x_tilde_4d.expand(n_bch, 1, n, d)\n",
    "            assert x_tilde.shape == (n_bch, 1, n, d)\n",
    "        elif (not do_detspacing) and (not use_np):\n",
    "            x_tilde_unnorm = self.batch_rng.normal((n_bch, n_v, n, d))\n",
    "            x_tilde_l2 = torch.sqrt(torch.square(x_tilde_unnorm).sum(dim=-1))\n",
    "            x_tilde = x_tilde_unnorm / x_tilde_l2.reshape(n_bch, n_v, n, 1)\n",
    "            assert x_tilde.shape == (n_bch, n_v, n, d)\n",
    "        else:\n",
    "            raise RuntimeError('Not implemented yet!')\n",
    "\n",
    "        if do_detspacing:\n",
    "            rot_mats = self.batch_rng.so_n((n_bch, n_v, d, d))\n",
    "            assert rot_mats.shape == (n_bch, n_v, d, d)\n",
    "\n",
    "        if do_detspacing:\n",
    "            x_tilde_rot = matmul(x_tilde, rot_mats)\n",
    "        else:\n",
    "            x_tilde_rot = x_tilde\n",
    "        assert x_tilde_rot.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        points = x_tilde_rot * \\\n",
    "            radii.reshape(n_bch, n_v, 1, 1) + centers.reshape(n_bch, n_v, 1, d)\n",
    "        assert points.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        if use_np:\n",
    "            x_tilde_bc = np.broadcast_to(x_tilde, (n_bch, n_v, n, d))\n",
    "        else:\n",
    "            x_tilde_bc = x_tilde.expand(n_bch, n_v, n, d)\n",
    "\n",
    "        if do_detspacing:\n",
    "            rot_x_tilde = matmul(x_tilde_bc, rot_mats)\n",
    "        else:\n",
    "            rot_x_tilde = x_tilde_bc\n",
    "        assert rot_x_tilde.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        cst = (2*(np.pi**(d/2))) / gamma(d/2)\n",
    "        csts = cst * (radii**(d-1))\n",
    "        assert csts.shape == (n_bch, n_v)\n",
    "\n",
    "        ret_dict = dict(points=points, normals=rot_x_tilde, areas=csts)\n",
    "        return ret_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae26e3",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c393f4d9",
   "metadata": {
    "code_folding": [
     0,
     57,
     66,
     142,
     225
    ]
   },
   "outputs": [],
   "source": [
    "def get_nn_sol(model, x, n_eval=None, get_field=True, \n",
    "    out_lib='numpy'):\n",
    "    \"\"\"\n",
    "    Gets a model and evaluates it minibatch-wise on the tensor x. \n",
    "    The minibatch size is capped at n_eval. The output will have the \n",
    "    predicted potentials and the vector fields at them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: (nn.module) the batched neural network.\n",
    "\n",
    "    x: (torch.tensor) the evaluation points. This array should be \n",
    "        >2-dimensional and have a shape of `(..., x_rows, x_cols)`.\n",
    "\n",
    "    n_eval: (int or None) the maximum mini-batch size. If None is \n",
    "        given, `x_rows` will be used as `n_eval`.\n",
    "        \n",
    "    out_lib: (str) determines the output tensor type. Should be either \n",
    "        'numpy' or 'torch'.\n",
    "    \n",
    "    Output Dictionary\n",
    "    ----------\n",
    "    v: (np.array or torch.tensor) the evaluated potentials \n",
    "        with a shape of `(*model.shape, x_rows)` where\n",
    "        model.shape is the batch dimensions of the model. \n",
    "\n",
    "    e: (np.array or torch.tensor) the evaluated vector fields \n",
    "        with a shape of `(*model.shape, x_rows, x_cols)` where\n",
    "        model.shape is the batch dimensions of the model.\n",
    "    \"\"\"\n",
    "    x_rows, x_cols = tuple(x.shape)[-2:]\n",
    "    x_bd_ = tuple(x.shape)[:-2]\n",
    "    x_bd = (1,) if len(x_bd_) == 0 else x_bd_\n",
    "    msg_ = f'Cannot have {x.shape} fed to {model.shape}'\n",
    "    assert len(x_bd) <= model.ndim, msg_\n",
    "    if len(x_bd) < model.ndim:\n",
    "        x_bd = tuple([1] * (model.ndim-len(x_b)) + list(x_bd))\n",
    "    assert all((a == b) or (a == 1) or (b == 1) \n",
    "               for a, b in zip(x_bd, model.shape)), msg_\n",
    "    n_eval = x_rows if n_eval is None else n_eval\n",
    "    if out_lib == 'numpy':\n",
    "        to_lib = lambda a: a.detach().cpu().numpy()\n",
    "        lib_cat = lambda al: np.concatenate(al, axis=1)\n",
    "        lpf = '_np'\n",
    "    elif out_lib == 'torch':\n",
    "        to_lib = lambda a: a\n",
    "        lib_cat = lambda al: torch.cat(al, dim=1)\n",
    "        lpf = ''\n",
    "    else:\n",
    "        raise ValueError(f'outlib={outlib} not defined.')\n",
    "\n",
    "    n_batches = int(np.ceil(x_rows / n_eval))\n",
    "    v_pred_list = []\n",
    "    e_pred_list = []\n",
    "    for i in range(n_batches):\n",
    "        x_i = x[..., (i*n_eval):((i+1)*n_eval), :]\n",
    "        xi_rows = x_i.shape[-2]\n",
    "        x_ii = x_i.reshape(*x_bd, xi_rows, x_cols)\n",
    "        x_iii = x_ii.expand(*model.shape, xi_rows, x_cols)\n",
    "        x_iiii = nn.Parameter(x_iii)\n",
    "        v_pred_i = model(x_iiii).squeeze(-1)\n",
    "        v_pred_ii = to_lib(v_pred_i.detach())\n",
    "        v_pred_list.append(v_pred_ii)\n",
    "        if get_field:\n",
    "            e_pred_i, = torch.autograd.grad(v_pred_i.sum(), [x_iiii],\n",
    "                grad_outputs=None, retain_graph=False, create_graph=False,\n",
    "                only_inputs=True, allow_unused=False)\n",
    "            e_pred_ii = to_lib(e_pred_i.squeeze(-1).detach())\n",
    "            e_pred_list.append(e_pred_ii)\n",
    "\n",
    "    v_pred = lib_cat(v_pred_list)\n",
    "    if get_field:\n",
    "        e_pred = lib_cat(e_pred_list)\n",
    "    else:\n",
    "        e_pred = None\n",
    "\n",
    "    outdict = {f'v{lpf}': v_pred, f'e{lpf}': e_pred}\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def get_prob_sol(problem, x, n_eval=None, get_field=True, \n",
    "    out_lib='numpy'):\n",
    "    \"\"\"\n",
    "    Gets a problem and evaluates the analytical solution to its \n",
    "    potentials and vector fields minibatch-wise on the tensor x. \n",
    "    The minibatch size is capped at n_eval. The output will have the \n",
    "    predicted potentials and the vector fields at them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problem: (object) the problem with both the `potential` and \n",
    "        `field` methods for analytical solution evaluation.\n",
    "\n",
    "    x: (torch.tensor) the evaluation points. This array should be \n",
    "        >2-dimensional and have a shape of `(..., x_rows, x_cols)`.\n",
    "\n",
    "    n_eval: (int or None) the maximum mini-batch size. If None is \n",
    "        given, `x_rows` will be used as `n_eval`.\n",
    "\n",
    "    Output Dictionary\n",
    "    ----------\n",
    "    v_np: (np.array) the evaluated potentials with a shape of\n",
    "        `(..., x_rows)`. \n",
    "\n",
    "    e_np: (np.array) the evaluated vector fields with a shape of\n",
    "        `(..., x_rows, x_cols)`.\n",
    "    \"\"\"\n",
    "\n",
    "    assert hasattr(problem, 'potential')\n",
    "    assert callable(problem.potential)\n",
    "    assert hasattr(problem, 'field')\n",
    "    assert callable(problem.field)\n",
    "\n",
    "    x_rows, x_cols = tuple(x.shape)[-2:]\n",
    "    x_bd_ = tuple(x.shape)[:-2]\n",
    "    x_bd = (1,) if len(x_bd_) == 0 else x_bd_\n",
    "    msg_ = f'Cannot have {x.shape} fed to {problem.shape}'\n",
    "    assert len(x_bd) <= problem.ndim, msg_\n",
    "    if len(x_bd) < problem.ndim:\n",
    "        x_bd = tuple([1] * (problem.ndim-len(x_b)) + list(x_bd))\n",
    "    assert all((a == b) or (a == 1) or (b == 1) \n",
    "               for a, b in zip(x_bd, problem.shape)), msg_\n",
    "    n_eval = x_rows if n_eval is None else n_eval\n",
    "    if out_lib == 'numpy':\n",
    "        to_lib = lambda a: a.detach().cpu().numpy()\n",
    "        lib_cat = lambda al: np.concatenate(al, axis=1)\n",
    "        lpf = '_np'\n",
    "    elif out_lib == 'torch':\n",
    "        to_lib = lambda a: a\n",
    "        lib_cat = lambda al: torch.cat(al, dim=1)\n",
    "        lpf = ''\n",
    "    else:\n",
    "        raise ValueError(f'outlib={outlib} not defined.')\n",
    "\n",
    "    n_batches = int(np.ceil(x_rows / n_eval))\n",
    "    v_list = []\n",
    "    e_list = []\n",
    "    for i in range(n_batches):\n",
    "        x_i = x[..., (i*n_eval):((i+1)*n_eval), :]\n",
    "        xi_rows = x_i.shape[-2]\n",
    "        x_ii = x_i.reshape(*x_bd, xi_rows, x_cols)\n",
    "        x_iii = x_ii.expand(*problem.shape, xi_rows, x_cols)\n",
    "        v_i = problem.potential(x_iii)\n",
    "        v_list.append(to_lib(v_i))\n",
    "        if get_field:\n",
    "            e_i = problem.field(x_iii)\n",
    "            e_list.append(to_lib(e_i))\n",
    "\n",
    "    v = lib_cat(v_list)\n",
    "    if get_field:\n",
    "        e = lib_cat(e_list)\n",
    "    else:\n",
    "        e = None\n",
    "    outdict = {f'v{lpf}': v, f'e{lpf}': e}\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def make_grid(x_low, x_high, dim, n_gpd, lib):\n",
    "    \"\"\"\n",
    "    Creates a grid of points using the mesgrid functions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_low: (list) a list of length `dim` with floats \n",
    "        representing the lower limits of the grid.\n",
    "    \n",
    "    x_high: (list) a list of length `dim` with floats \n",
    "        representing the higher limits of the grid.\n",
    "    \n",
    "    dim: (int) the dimension of the grid space.\n",
    "    \n",
    "    n_gpd: (int) the number of points in each \n",
    "        grid dimension. This yields a total of \n",
    "        `n_gpd**dim` points in the total grid.\n",
    "        \n",
    "    lib: (str) either 'torch' or 'numpy'. This determines \n",
    "        the type of `x` output.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    x: (torch.tensor or np.array) a 2-d tensor or array \n",
    "        with the shape of `(n_gpd**dim, dim)`. \n",
    "    \n",
    "    xi_msh_np: (list of np.array) a list of length `dim` \n",
    "        with meshgrid tensors each with a shape of \n",
    "        `[n_gpd] * dim`.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert dim == 2, 'not implemented yet'\n",
    "    assert len(x_low) == dim\n",
    "    assert len(x_high) == dim\n",
    "    assert lib in ('torch', 'numpy')\n",
    "    library = torch if lib == 'torch' else np\n",
    "    tnper = lambda a: a.cpu().detach().numpy()\n",
    "    nper = tnper if lib == 'torch' else lambda a: a\n",
    "    \n",
    "    x1_low, x2_low = x_low\n",
    "    x1_high, x2_high = x_high\n",
    "    n_g_plt = n_gpd ** dim\n",
    "\n",
    "    x1_1d = library.linspace(x1_low, x1_high, n_gpd)\n",
    "    assert x1_1d.shape == (n_gpd,)\n",
    "\n",
    "    x2_1d = library.linspace(x2_low, x2_high, n_gpd)\n",
    "    assert x2_1d.shape == (n_gpd,)\n",
    "\n",
    "    x1_msh, x2_msh = library.meshgrid(x1_1d, x2_1d)\n",
    "    assert x1_msh.shape == (n_gpd, n_gpd)\n",
    "    assert x2_msh.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x1 = x1_msh.reshape(n_g_plt, 1)\n",
    "    assert x1.shape == (n_g_plt, 1)\n",
    "\n",
    "    x2 = x2_msh.reshape(n_g_plt, 1)\n",
    "    assert x2.shape == (n_g_plt, 1)\n",
    "\n",
    "    x1_1d_c = x1_1d.reshape(n_gpd, 1)\n",
    "    assert x1_1d_c.shape == (n_gpd, 1)\n",
    "\n",
    "    x2_1d_c = x2_1d.reshape(n_gpd, 1)\n",
    "    assert x2_1d_c.shape == (n_gpd, 1)\n",
    "\n",
    "    x1_msh_np = nper(x1_msh)\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x2_msh_np = nper(x2_msh)\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x = torch.cat([x1, x2], dim=1)\n",
    "    assert x.shape == (n_g_plt, dim)\n",
    "\n",
    "    x_np = nper(x)\n",
    "    assert x_np.shape == (n_g_plt, dim)\n",
    "    \n",
    "    xi_msh_np = [x1_msh_np, x2_msh_np]\n",
    "    outdict = dict(x=x, xi_msh_np=xi_msh_np)\n",
    "\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=None, ax=None, cax=None):\n",
    "    n_gpd, dim = x1_msh_np.shape[0], x1_msh_np.ndim\n",
    "    assert dim == 2, f'dim={dim}, x1_msh_np.shape={x1_msh_np.shape}'\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "    assert x2_msh_np.shape == (n_gpd, n_gpd)\n",
    "    n_g = (n_gpd ** dim)\n",
    "   \n",
    "    if fig is None:\n",
    "        assert ax is None\n",
    "        assert cax is None\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(3.0, 2.5), dpi=72)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    else:\n",
    "        assert ax is not None\n",
    "   \n",
    "    e_percentile_cap = 90\n",
    "    if 'v_np' in sol_dict:\n",
    "        v_np = sol_dict['v_np']\n",
    "    else:\n",
    "        v_np = sol_dict['v'].detach().cpu().numpy()\n",
    "    assert v_np.shape[-1] == n_g\n",
    "    \n",
    "    v_msh_np = v_np.reshape(-1, n_gpd, n_gpd).mean(axis=0)\n",
    "    im = ax.pcolormesh(x1_msh_np, x2_msh_np, v_msh_np,\n",
    "                        shading='auto', cmap='RdBu')\n",
    "    if cax is not None:\n",
    "        fig.colorbar(im, cax=cax)\n",
    "\n",
    "    if 'e_np' in sol_dict:\n",
    "        e_msh_np = sol_dict['e_np']\n",
    "    else:\n",
    "        e_msh_np = sol_dict['e']\n",
    "        if e_msh_np is not None:\n",
    "            e_msh_np = e_msh_np.detach().cpu().numpy()\n",
    "    \n",
    "    if e_msh_np is not None:\n",
    "        assert e_msh_np.shape[-2:] == (n_g, dim)\n",
    "        e_msh_np = e_msh_np.reshape(-1, n_gpd,\n",
    "            n_gpd, dim).mean(axis=0)\n",
    "        if e_percentile_cap is not None:\n",
    "            e_size = np.sqrt((e_msh_np**2).sum(axis=-1))\n",
    "            e_size_cap = np.percentile(a=e_size, \n",
    "                q=e_percentile_cap, axis=None)\n",
    "            cap_coef = np.ones_like(e_size)\n",
    "            cap_coef[e_size > e_size_cap] = e_size_cap / \\\n",
    "                e_size[e_size > e_size_cap]\n",
    "            e_msh_capped = e_msh_np * \\\n",
    "                cap_coef.reshape(*e_msh_np.shape[:-1], 1)\n",
    "        else:\n",
    "            e_msh_capped = e_msh_np\n",
    "\n",
    "        ax.quiver(x1_msh_np, x2_msh_np,\n",
    "            e_msh_capped[:, :, 0], e_msh_capped[:, :, 1])\n",
    "    return fig, ax, cax\n",
    "\n",
    "\n",
    "def get_perfdict(e_pnts, e_mdlsol, e_prbsol):\n",
    "    \"\"\"\n",
    "    Computes the biased, bias-corrected, and slope-corrected error \n",
    "    metrics for the solutions of a Poisson problem.\n",
    "    \n",
    "    This function computes three types of MSE and MAE statistics:\n",
    "        \n",
    "        1. Plain: just take the model and ground truth solution\n",
    "            and subtract them to get the errors. No bias- or slope-correction \n",
    "            is applied to offset those degrees of freedom.\n",
    "            \n",
    "            shorthand: 'pln'\n",
    "            \n",
    "        2. Bias-corrected: subtracts the average value from both the model \n",
    "            and ground truth solutions, and then computes the errors.\n",
    "            \n",
    "            shorthand: 'bc'\n",
    "            \n",
    "        3. Slope-corrected: Since any linear function can be added to the\n",
    "            Poisson solutions without violating the poisson equation, this\n",
    "            function fits an ordinary least squares to both the model and\n",
    "            ground truth solutions, and then subtracts it from them. This\n",
    "            way, even the arbitrary-slope issue can be addressed.\n",
    "            \n",
    "            shorthand: 'slc'\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    e_pnts: (torch.tensor) The input points to the model and the ground truth.\n",
    "        This should have a shape of (n_seeds, n_evlpnts, dim).\n",
    "        \n",
    "    e_mdlsol: (torch.tensor) The model solution with a\n",
    "        (n_seeds, n_evlpnts) shape.\n",
    "    \n",
    "    e_prbsol: (torch.tensor) The ground truth solution with a\n",
    "        (n_seeds, n_evlpnts) shape.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    outdict: (dict) A mapping between the error keys and their numpy arrays.\n",
    "        The error keys are the cartesian product of ('pln', 'bc', 'slc') \n",
    "        and ('mse', 'mae').\n",
    "    \"\"\"\n",
    "    n_seeds, n_evlpnts, dim = e_pnts.shape\n",
    "    assert e_mdlsol.shape == (n_seeds, n_evlpnts)\n",
    "    assert e_prbsol.shape == (n_seeds, n_evlpnts)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # The plain non-processed error matrix\n",
    "        err_pln = e_mdlsol - e_prbsol\n",
    "        assert err_pln.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # The bias-corrected error matrix\n",
    "        e_mdlsol2 = e_mdlsol - e_mdlsol.mean(dim=1, keepdims=True)\n",
    "        assert e_mdlsol2.shape == (n_seeds, n_evlpnts)\n",
    "        e_prbsol2 = e_prbsol - e_prbsol.mean(dim=1, keepdims=True)\n",
    "        assert e_prbsol2.shape == (n_seeds, n_evlpnts)\n",
    "        err_bc = e_mdlsol2 - e_prbsol2\n",
    "        assert err_bc.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # The slope-corrected error matrix\n",
    "        e_pntstrans = e_pnts.transpose(-1, -2)\n",
    "        assert e_pntstrans.shape == (n_seeds, dim, n_evlpnts)\n",
    "        e_pntsig = e_pntstrans.matmul(e_pnts)\n",
    "        assert e_pntsig.shape == (n_seeds, dim, dim)\n",
    "        e_pntsiginv = torch.pinverse(e_pntsig)\n",
    "        assert e_pntsiginv.shape == (n_seeds, dim, dim)\n",
    "        e_pntpinv = e_pntsiginv.matmul(e_pntstrans)\n",
    "        assert e_pntpinv.shape == (n_seeds, dim, n_evlpnts)\n",
    "        \n",
    "        # e_pntpinv = torch.pinverse(e_pnts)\n",
    "        # assert e_pntpinv.shape == (n_seeds, dim, n_evlpnts)\n",
    "        \n",
    "        e_mdlbeta = e_pntpinv.matmul(e_mdlsol2.unsqueeze(-1))\n",
    "        assert e_mdlbeta.shape == (n_seeds, dim, 1)\n",
    "        e_mdlslpcrc = e_pnts.matmul(e_mdlbeta)\n",
    "        assert e_mdlslpcrc.shape == (n_seeds, n_evlpnts, 1)\n",
    "        e_mdlsol3 = e_mdlsol2 - e_mdlslpcrc.squeeze(-1)\n",
    "        assert e_mdlsol3.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        e_prbbeta = e_pntpinv.matmul(e_prbsol2.unsqueeze(-1))\n",
    "        assert e_prbbeta.shape == (n_seeds, dim, 1)\n",
    "        e_prbslpcrc = e_pnts.matmul(e_prbbeta)\n",
    "        assert e_prbslpcrc.shape == (n_seeds, n_evlpnts, 1)\n",
    "        e_prbsol3 = e_prbsol2 - e_prbslpcrc.squeeze(-1)\n",
    "        assert e_prbsol3.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        err_slc = e_mdlsol3 - e_prbsol3\n",
    "        assert err_slc.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # The normalized slope-corrected error matrix\n",
    "        e_mdlsol4 = e_mdlsol3 / e_mdlsol3.std(dim=1, keepdim=True)\n",
    "        assert e_mdlsol4.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        e_prbsol4 = e_prbsol3 / e_prbsol3.std(dim=1, keepdim=True)\n",
    "        assert e_prbsol4.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        err_scn = e_mdlsol4 - e_prbsol4\n",
    "        assert err_scn.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # Computing the mse and mae values\n",
    "        e_plnmse = err_pln.square().mean(dim=-1)\n",
    "        assert e_plnmse.shape == (n_seeds,)\n",
    "        e_plnmae = err_pln.abs().mean(dim=-1)\n",
    "        assert e_plnmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_bcmse = err_bc.square().mean(dim=-1)\n",
    "        assert e_bcmse.shape == (n_seeds,)\n",
    "        e_bcmae = err_bc.abs().mean(dim=-1)\n",
    "        assert e_bcmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_slcmse = err_slc.square().mean(dim=-1)\n",
    "        assert e_slcmse.shape == (n_seeds,)\n",
    "        e_slcmae = err_slc.abs().mean(dim=-1)\n",
    "        assert e_slcmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_scnmse = err_scn.square().mean(dim=-1)\n",
    "        assert e_scnmse.shape == (n_seeds,)\n",
    "        e_scnmae = err_scn.abs().mean(dim=-1)\n",
    "        assert e_scnmse.shape == (n_seeds,)\n",
    "    \n",
    "        outdict = {'pln/mse': e_plnmse.detach().cpu().numpy(),\n",
    "                   'pln/mae': e_plnmae.detach().cpu().numpy(),\n",
    "                   'bc/mse': e_bcmse.detach().cpu().numpy(),\n",
    "                   'bc/mae': e_bcmae.detach().cpu().numpy(),\n",
    "                   'slc/mse': e_slcmse.detach().cpu().numpy(),\n",
    "                   'slc/mae': e_slcmae.detach().cpu().numpy(),\n",
    "                   'scn/mse': e_scnmse.detach().cpu().numpy(),\n",
    "                   'scn/mae': e_scnmae.detach().cpu().numpy()}\n",
    "    \n",
    "    return outdict\n",
    "\n",
    "\n",
    "# MCMC Utility Functions\n",
    "def normal_logprob(x, loc, scale):\n",
    "    n_seeds, n_pnts, dim = loc.shape\n",
    "    assert scale.shape == (n_seeds, n_pnts)\n",
    "    \n",
    "    scale2 = scale.reshape(n_seeds, n_pnts, 1)\n",
    "    assert scale2.shape == (n_seeds, n_pnts, 1)\n",
    "    \n",
    "    e = (x - loc) / scale2\n",
    "    assert e.shape == (n_seeds, n_pnts, dim)\n",
    "    \n",
    "    hltp = np.log(2*np.pi).item() / 2\n",
    "    ll = -0.5 * e.square() - hltp - scale2.log()\n",
    "    assert ll.shape == (n_seeds, n_pnts, dim)\n",
    "    \n",
    "    llsum = ll.sum(dim=-1)\n",
    "    assert llsum.shape == (n_seeds, n_pnts)\n",
    "    \n",
    "    return llsum\n",
    "\n",
    "\n",
    "def replicate_top(param, top_idx):\n",
    "    n_seeds = param.shape[0]\n",
    "    n_grps, srch_reset_k = top_idx.shape\n",
    "    \n",
    "    n_cpg = n_seeds // n_grps\n",
    "    assert n_seeds == (n_grps * n_cpg)\n",
    "    \n",
    "    srch_reset_reps = n_cpg // srch_reset_k\n",
    "    assert n_cpg == (srch_reset_reps * srch_reset_k)\n",
    "    \n",
    "    pshape = param.shape[1:]\n",
    "    opshape = tuple([1 for _ in pshape])\n",
    "    \n",
    "    param2 = param.reshape(n_grps, n_cpg, *pshape)\n",
    "    assert param2.shape == (n_grps, n_cpg, *pshape)\n",
    "    \n",
    "    top_idx2 = top_idx.reshape(n_grps, srch_reset_k, *opshape)\n",
    "    assert top_idx2.shape == (n_grps, srch_reset_k, *opshape)\n",
    "    \n",
    "    param3 = torch.take_along_dim(param2, top_idx2, dim=1)\n",
    "    assert param3.shape == (n_grps, srch_reset_k, *pshape)\n",
    "    \n",
    "    param4 = param3.unsqueeze(1)\n",
    "    assert param4.shape == (n_grps, 1, srch_reset_k, *pshape)\n",
    "    \n",
    "    param5 = param4.expand(n_grps, srch_reset_reps, srch_reset_k, *pshape).clone()\n",
    "    assert param5.shape == (n_grps, srch_reset_reps, srch_reset_k, *pshape)\n",
    "    \n",
    "    param6 = param5.reshape(n_seeds, *pshape)\n",
    "    assert param6.shape == (n_seeds, *pshape)\n",
    "    \n",
    "    return param6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ea5e7",
   "metadata": {},
   "source": [
    "## Optional Visualization Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3e68d",
   "metadata": {},
   "source": [
    "### Visualizing the True Potential and Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e74ea5",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "ex_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "ex_tchdevice = torch.device(ex_device)\n",
    "ex_tchdtype = torch.double\n",
    "prob2d_ex1 = DeltaProblem(weights=np.array([[1.0, 1.0, 1.0]]),\n",
    "                          locations=np.array([[[0.0,  0.0],\n",
    "                                               [-0.5, -0.5],\n",
    "                                               [0.5,  0.5]]]),\n",
    "                          tch_device=ex_tchdevice,\n",
    "                          tch_dtype=ex_tchdtype)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3.0, 2.5), dpi=72)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "ex_gdict = make_grid(x_low=[-1, -1], x_high=[1., 1.], \n",
    "    dim=2 , n_gpd=50, lib='torch')\n",
    "x = ex_gdict['x'].to(ex_tchdevice, ex_tchdtype)\n",
    "x1_msh_np, x2_msh_np = ex_gdict['xi_msh_np']\n",
    "\n",
    "ex_sol = get_prob_sol(prob2d_ex1, x, n_eval=200, get_field=True)\n",
    "fig, ax, cax = plot_sol(x1_msh_np, x2_msh_np, ex_sol, \n",
    "    fig=fig, ax=ax, cax=cax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e1605",
   "metadata": {},
   "source": [
    "### Visualizing the Sampler and Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1d8e8",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "n_bch = 5\n",
    "rng = BatchRNG(shape=(n_bch,), lib='torch', device=ex_tchdevice, dtype=ex_tchdtype,\n",
    "               unif_cache_cols=100, norm_cache_cols=500)\n",
    "rng.seed(np.broadcast_to(12345+np.arange(n_bch), rng.shape))\n",
    "\n",
    "prob2d_ex2 = DeltaProblem(weights=np.broadcast_to([1.0, 1.0, 1.0], (n_bch, 3)).copy(),\n",
    "                          locations=np.broadcast_to([[0.0,  0.0],\n",
    "                                                     [-0.5, -0.5],\n",
    "                                                     [0.5,  0.5]], (n_bch, 3, 2)).copy(),\n",
    "                          tch_device=ex_tchdevice, tch_dtype=ex_tchdtype)\n",
    "\n",
    "volsampler_2d = BallSampler(c_dstr='uniform', c_params=dict(\n",
    "                            low=np.broadcast_to([-1.0, -1.0], (n_bch, 2)).copy(),\n",
    "                            high=np.broadcast_to([1.0,  1.0], (n_bch, 2)).copy()),\n",
    "                            r_dstr='uniform', r_params=dict(\n",
    "                            low=np.broadcast_to([0.1], (n_bch,)).copy(),\n",
    "                            high=np.broadcast_to([1.5], (n_bch,)).copy()),\n",
    "                            batch_rng=rng)\n",
    "\n",
    "vols = volsampler_2d(n=10)\n",
    "integs = prob2d_ex2.integrate_volumes(vols)\n",
    "for key, val in vols.items():\n",
    "    if torch.is_tensor(val):\n",
    "        vols[key] = val.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd410b",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=36)\n",
    "ax = plt.gca()\n",
    "\n",
    "i = 0\n",
    "max_integ = prob2d_ex2.weights[i][prob2d_ex2.weights[i] > 0].sum()\n",
    "min_integ = prob2d_ex2.weights[i][prob2d_ex2.weights[i] < 0].sum()\n",
    "cmap = mpl.cm.get_cmap('RdBu')\n",
    "cnorm = mpl.colors.Normalize(vmin=min_integ, vmax=max_integ)\n",
    "\n",
    "ax.scatter(prob2d_ex2.locations[i, :, 0],\n",
    "           prob2d_ex2.locations[i, :, 1], marker='*', color='black', s=150)\n",
    "for center, radius, integ in zip(vols['centers'][i], vols['radii'][i], integs[i]):\n",
    "    circle = plt.Circle(center, radius, fill=False,\n",
    "                        color=cmap(1.0-cnorm(integ.item())))\n",
    "    ax.add_patch(circle)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01766402",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "n_bch = 5\n",
    "rng = BatchRNG(shape=(n_bch,), lib='torch', device=ex_tchdevice, dtype=ex_tchdtype,\n",
    "               unif_cache_cols=100, norm_cache_cols=500)\n",
    "rng.seed(np.broadcast_to(12345+np.arange(n_bch), rng.shape))\n",
    "\n",
    "prob2d_ex3 = DeltaProblem(weights=np.broadcast_to([1.0, 1.0, 1.0], (n_bch, 3)).copy(),\n",
    "                          locations=np.broadcast_to([[0.0,  0.0],\n",
    "                                                     [-0.5, -0.5],\n",
    "                                                     [0.5,  0.5]], (n_bch, 3, 2)).copy(),\n",
    "                          tch_device=ex_tchdevice,\n",
    "                          tch_dtype=ex_tchdtype)\n",
    "\n",
    "\n",
    "volsampler_2d = BallSampler(c_dstr='uniform', c_params=dict(\n",
    "                            low=np.broadcast_to([-1.0, -1.0], (n_bch, 2)).copy(),\n",
    "                            high=np.broadcast_to([1.0,  1.0], (n_bch, 2)).copy()),\n",
    "                            r_dstr='uniform', r_params=dict(\n",
    "                            low=np.broadcast_to([0.1], (n_bch,)).copy(),\n",
    "                            high=np.broadcast_to([1.5], (n_bch,)).copy()),\n",
    "                            batch_rng=rng)\n",
    "\n",
    "sphsampler_2d = SphereSampler(batch_rng=rng)\n",
    "\n",
    "vols = volsampler_2d(n=10)\n",
    "sphsamps2d = sphsampler_2d(vols, 100, do_detspacing=True)\n",
    "points = sphsamps2d['points']\n",
    "surfacenorms = sphsamps2d['normals']\n",
    "if torch.is_tensor(points):\n",
    "    points = points.detach().cpu().numpy()\n",
    "if torch.is_tensor(surfacenorms):\n",
    "    surfacenorms = surfacenorms.detach().cpu().numpy()\n",
    "points.shape, surfacenorms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5aa16a",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=36)\n",
    "ax = plt.gca()\n",
    "\n",
    "i = 0\n",
    "\n",
    "max_integ = prob2d_ex3.weights[i][prob2d_ex3.weights[i] > 0].sum()\n",
    "min_integ = prob2d_ex3.weights[i][prob2d_ex3.weights[i] < 0].sum()\n",
    "cmap = mpl.cm.get_cmap('RdBu')\n",
    "cnorm = mpl.colors.Normalize(vmin=min_integ, vmax=max_integ)\n",
    "\n",
    "ax.scatter(prob2d_ex3.locations[i, :, 0], prob2d_ex3.locations[i, :, 1],\n",
    "           marker='*', color='black', s=150)\n",
    "for pnts, srfnrms, center, radius, integ in zip(points[i],\n",
    "                                                surfacenorms[i], vols['centers'][i], vols['radii'][i], integs[i]):\n",
    "    ax.scatter(pnts[:, 0], pnts[:, 1], marker='o',\n",
    "               color=cmap(1.0-cnorm(integ.item())), s=1)\n",
    "    ax.quiver(pnts[:, 0], pnts[:, 1], srfnrms[:, 0],\n",
    "              srfnrms[:, 1], width=0.002)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa95d5",
   "metadata": {},
   "source": [
    "## Utility Functions for Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f525f",
   "metadata": {
    "code_folding": [
     8,
     60,
     85
    ]
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "########### Sanity Checking Utility Functions ###########\n",
    "#########################################################\n",
    "\n",
    "msg_bcast = '{} should be np broadcastable to {}={}. '\n",
    "msg_bcast += 'However, it has an inferred shape of {}.'\n",
    "\n",
    "\n",
    "def get_arr(name, trgshp_str, trns_opts):\n",
    "    \"\"\"\n",
    "    Gets a list of values, and checks if it is broadcastable to a \n",
    "    target shape. If the shape does not match, it will raise a proper\n",
    "    assertion error with a meaninful message. The output is a numpy \n",
    "    array that is guaranteed to be broadcastable to the target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: (str) name of the option / hyper-parameter.\n",
    "\n",
    "    trgshp_str: (str) the target shape elements representation. Must be a \n",
    "        valid python expression where the needed elements .\n",
    "\n",
    "    trns_opts: (dict) a dictionary containing the variables needed \n",
    "        for the string to list translation of val.\n",
    "\n",
    "    Key Variables\n",
    "    -------------\n",
    "    `val = trns_opts[name]`: (list or str) list of values read \n",
    "        from the config file. If a string is provided, python's \n",
    "        `eval` function will be used to translate it into a list.\n",
    "        \n",
    "    `trg_shape = eval_formula(trgshp_str, trns_opts)`: (tuple) \n",
    "        the target shape.\n",
    "    \n",
    "    Output\n",
    "    ----------\n",
    "    val_np: (np.array) the numpy array of val. \n",
    "    \"\"\"\n",
    "    msg_ =  f'\"{name}\" must be in trns_opts but it isnt: {trns_opts}'\n",
    "    assert name in trns_opts, msg_\n",
    "    val = trns_opts[name]\n",
    "    \n",
    "    if isinstance(val, str):\n",
    "        val_list = eval_formula(val, trns_opts)\n",
    "    else:\n",
    "        val_list = val\n",
    "    val_np = np.array(val_list)\n",
    "    src_shape = val_np.shape\n",
    "    trg_shape = eval_formula(trgshp_str, trns_opts)\n",
    "    msg_ = msg_bcast.format(name, trgshp_str, trg_shape, src_shape)\n",
    "\n",
    "    assert len(val_np.shape) == len(trg_shape), msg_\n",
    "\n",
    "    is_bcastble = all((x == y or x == 1 or y == 1) for x, y in\n",
    "                      zip(src_shape, trg_shape))\n",
    "    assert is_bcastble, msg_\n",
    "\n",
    "    return val_np\n",
    "\n",
    "\n",
    "def eval_formula(formula, variables):\n",
    "    \"\"\"\n",
    "    Gets a string formula and uses the `eval` function of python to  \n",
    "    translate it into a python variable. The necessary variables for \n",
    "    translation are provided through the `variables` argument.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    formula (str): a string that can be passed to `eval`.\n",
    "        Example: \"[np.sqrt(dim), 'a', None]\"\n",
    "\n",
    "    variables (dict): a dictionary of variables used in the formula.\n",
    "        Example: {\"dim\": 4}\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    pyobj (object): the translated formula into a python object\n",
    "        Example: [2.0, 'a', None]\n",
    "\n",
    "    \"\"\"\n",
    "    locals().update(variables)\n",
    "    pyobj = eval(formula)\n",
    "    return pyobj\n",
    "\n",
    "\n",
    "def chck_dstrargs(opt, cfgdict, dstr2args, opt2req, parnt_optdstr=None):\n",
    "    \"\"\"\n",
    "    Checks if the distribution arguments are provided correctly. Works \n",
    "    with hirarchical models through recursive applications. Proper error \n",
    "    messages are displayed if one of the checks fails.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    opt: (str) the option name.\n",
    "\n",
    "    cfgdict: (dict) the config dictionary.\n",
    "\n",
    "    dstr2args: (dict) a mapping between distribution and their \n",
    "        required arguments.\n",
    "        \n",
    "    opt2req: (dict) required arguments for an option itself, not \n",
    "        necessarily required by the option's distribution.\n",
    "    \"\"\"\n",
    "    opt_dstr = cfgdict.get(f'{opt}/dstr', 'fixed')\n",
    "\n",
    "    msg_ = f'Unknown {opt}_dstr: it should be one of {list(dstr2args.keys())}'\n",
    "    assert opt_dstr in dstr2args, msg_\n",
    "\n",
    "    opt2req = dict() if opt2req is None else opt2req\n",
    "    optreqs = opt2req.get(opt, tuple())\n",
    "    must_spec = list(dstr2args[opt_dstr]) + list(optreqs)\n",
    "    avid_spec = list(chain.from_iterable(\n",
    "        v for k, v in dstr2args.items() if k != opt_dstr))\n",
    "    avid_spec = [k for k in avid_spec if k not in must_spec]\n",
    "\n",
    "    if opt_dstr == 'fixed':\n",
    "        # To avoid infinite recursive calls, we should end this here.\n",
    "        msg_ = f'\"{opt}\" must be specified.'\n",
    "        if parnt_optdstr is not None:\n",
    "            parnt_opt, parnt_dstr = parnt_optdstr\n",
    "            msg_ += f'\"{parnt_opt}\" was specified as \"{parnt_dstr}\", and'\n",
    "        msg_ += f' \"{opt}\" was specified as \"{opt_dstr}\".'\n",
    "        if len(optreqs) > 0:\n",
    "            msg_ += f' Also, \"{opt}\" requires \"{optreqs}\" to be specified.'\n",
    "        opt_val = cfgdict.get(opt, None)\n",
    "        assert opt_val is not None, msg_\n",
    "    else:\n",
    "        for arg in must_spec:\n",
    "            opt_arg = f'{opt}{arg}'\n",
    "            chck_dstrargs(opt_arg, cfgdict, dstr2args, opt2req, (opt, opt_dstr))\n",
    "\n",
    "    for arg in avid_spec:\n",
    "        opt_arg = f'{opt}{arg}'\n",
    "        opt_arg_val = cfgdict.get(opt_arg, None)\n",
    "        msg_ = f'\"{opt_arg}\" should not be specified, since \"{opt}\" '\n",
    "        msg_ += f'appears to follow the \"{opt_dstr}\" distribution.'\n",
    "        assert opt_arg_val is None, msg_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184b2f0",
   "metadata": {},
   "source": [
    "## JSON Config Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976278b",
   "metadata": {
    "code_folding": [
     1
    ],
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "json_cfgpath = f'../configs/00_scratch/06_mcmc.json'\n",
    "# ! rm -rf \"./16_poisson/results/06_mcmc.h5\"\n",
    "# ! rm -rf \"./16_poisson/storage/06_mcmc\"\n",
    "with open(json_cfgpath, 'r') as fp:\n",
    "    json_cfgdict = json.load(fp, object_pairs_hook=odict)\n",
    "json_cfgdict['io/config_id'] = '06_mcmc'\n",
    "json_cfgdict['io/results_dir'] = './16_poisson/results'\n",
    "json_cfgdict['io/storage_dir'] = './16_poisson/storage'\n",
    "json_cfgdict['io/tch/device'] = 'cuda:0'\n",
    "\n",
    "all_cfgdicts = preproc_cfgdict(json_cfgdict)\n",
    "cfg_dict_input = all_cfgdicts[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a080649e",
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "source": [
    "def main(cfg_dict_input):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ed4c3",
   "metadata": {},
   "source": [
    "## Retrieving Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a05c3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    cfg_dict = cfg_dict_input.copy()\n",
    "\n",
    "    #########################################################\n",
    "    #################### Ignored Options ####################\n",
    "    #########################################################\n",
    "    cfgdesc = cfg_dict.pop('desc', None)\n",
    "    cfgdate = cfg_dict.pop('date', None)\n",
    "\n",
    "    #########################################################\n",
    "    ################### Mandatory Options ###################\n",
    "    #########################################################\n",
    "    prob_type = cfg_dict.pop('problem')\n",
    "    rng_seed_list = cfg_dict.pop('rng_seed/list')\n",
    "    dim = cfg_dict.pop('dim')\n",
    "\n",
    "    n_srf = cfg_dict.pop('vol/n')\n",
    "    n_srfpts_mdl = cfg_dict.pop('srfpts/n/mdl')\n",
    "    n_srfpts_trg = cfg_dict.pop('srfpts/n/trg')\n",
    "    do_detspacing = cfg_dict.pop('srfpts/detspc')\n",
    "    do_dblsampling = cfg_dict.pop('srfpts/dblsmpl')\n",
    "\n",
    "    do_bootstrap = cfg_dict.pop('trg/btstrp')\n",
    "    if do_bootstrap:\n",
    "        tau = cfg_dict.pop('trg/tau')\n",
    "        w_trgreg = cfg_dict.pop('trg/reg/w')\n",
    "    else:\n",
    "        w_trgreg = 0.0\n",
    "    w_trg = cfg_dict.pop('trg/w', None)\n",
    "\n",
    "    opt_type = cfg_dict.pop('opt/dstr')\n",
    "    n_epochs = cfg_dict.pop('opt/epoch')\n",
    "    lr = cfg_dict.pop('opt/lr')\n",
    "\n",
    "    #########################################################\n",
    "    ################## Neural Spec Options ##################\n",
    "    #########################################################\n",
    "    nn_dstr = cfg_dict.pop('nn/dstr')\n",
    "    if nn_dstr == 'mlp':\n",
    "        nn_width = cfg_dict.pop('nn/width')\n",
    "        nn_hidden = cfg_dict.pop('nn/hidden')\n",
    "        nn_act = cfg_dict.pop('nn/act')\n",
    "    else:\n",
    "        msg_ = f'nn/dstr=\"{nn_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    #########################################################\n",
    "    ############## Charge Distribution Options ##############\n",
    "    #########################################################\n",
    "    chrg_dstr = cfg_dict.pop('chrg/dstr')\n",
    "    chrg_n = cfg_dict.pop('chrg/n')\n",
    "\n",
    "    chrg_w_dstr = cfg_dict.pop('chrg/w/dstr', 'fixed')\n",
    "    if chrg_w_dstr == 'fixed':\n",
    "        chrg_w_ = cfg_dict.pop('chrg/w')\n",
    "    elif chrg_w_dstr == 'uniform':\n",
    "        chrg_w_low_ = cfg_dict.pop('chrg/w/low', None)\n",
    "        chrg_w_high_ = cfg_dict.pop('chrg/w/high', None)\n",
    "    elif chrg_w_dstr == 'normal':\n",
    "        chrg_w_loc_ = cfg_dict.pop('chrg/w/loc', None)\n",
    "        chrg_w_scale_ = cfg_dict.pop('chrg/w/scale', None)\n",
    "    else:\n",
    "        msg_ = f'chrg/w/dstr=\"{chrg_w_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    chrg_mu_dstr = cfg_dict.pop('chrg/mu/dstr', 'fixed')\n",
    "    if chrg_mu_dstr == 'fixed':\n",
    "        chrg_mu_ = cfg_dict.pop('chrg/mu')\n",
    "    elif chrg_mu_dstr == 'uniform':\n",
    "        chrg_mu_low_ = cfg_dict.pop('chrg/mu/low')\n",
    "        chrg_mu_high_ = cfg_dict.pop('chrg/mu/high')\n",
    "    elif chrg_mu_dstr == 'normal':\n",
    "        chrg_mu_loc_ = cfg_dict.pop('chrg/mu/loc')\n",
    "        chrg_mu_scale_ = cfg_dict.pop('chrg/mu/scale')\n",
    "    elif chrg_mu_dstr == 'ball':\n",
    "        chrg_mu_c_ = cfg_dict.pop('chrg/mu/c')\n",
    "        chrg_mu_r_ = cfg_dict.pop('chrg/mu/r')\n",
    "    else:\n",
    "        msg_ = f'chrg/mu/dstr=\"{chrg_mu_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    #########################################################\n",
    "    ############ Problem Parameter Search Options ###########\n",
    "    #########################################################\n",
    "    srch_dstr = cfg_dict.pop('srch/dstr', None)\n",
    "    if srch_dstr in ('mcmc',):\n",
    "        srch_prior_mu_dstr = cfg_dict.pop('srch/prior/mu/dstr')\n",
    "        assert srch_prior_mu_dstr == 'normal'\n",
    "        srch_prior_mu_loc_ = cfg_dict.pop('srch/prior/mu/loc')\n",
    "        srch_prior_mu_scale_ = cfg_dict.pop('srch/prior/mu/scale')\n",
    "    elif srch_dstr is None:\n",
    "        pass\n",
    "    else:\n",
    "        msg_ = f'srch/dstr=\"{srch_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    # Shared parameters for all search methods\n",
    "    if srch_dstr is not None:\n",
    "        obs_x_dstr = cfg_dict.pop('srch/obs/x/dstr')\n",
    "        assert obs_x_dstr == 'uniform'\n",
    "        obs_x_low_ = cfg_dict.pop('srch/obs/x/low')\n",
    "        obs_x_high_ = cfg_dict.pop('srch/obs/x/high')\n",
    "        \n",
    "        obs_y_dstr = cfg_dict.pop('srch/obs/y/dstr')\n",
    "        obs_n = cfg_dict.pop('srch/obs/n')\n",
    "        \n",
    "        srch_metric_dstr = cfg_dict.pop('srch/metric/dstr')\n",
    "        srch_metric_coeff = cfg_dict.pop('srch/metric/coeff')\n",
    "        srch_metric_min = cfg_dict.pop('srch/metric/min', None)\n",
    "        srch_metric_max = cfg_dict.pop('srch/metric/max', None)\n",
    "        \n",
    "        srch_frq = cfg_dict.pop('srch/frq')\n",
    "        srch_inittrn = cfg_dict.pop('srch/inittrn')\n",
    "        srch_tmprtr = cfg_dict.pop('srch/tmprtr')\n",
    "        srch_sigma = cfg_dict.pop('srch/sigma')\n",
    "        n_grps = cfg_dict.pop('srch/grps')\n",
    "        srch_reset_n = cfg_dict.pop('srch/reset/n')\n",
    "        srch_reset_k = cfg_dict.pop('srch/reset/k')\n",
    "    \n",
    "    #########################################################\n",
    "    ############### Surface Sampling Options ################\n",
    "    #########################################################\n",
    "    vol_dstr = cfg_dict.pop('vol/dstr')\n",
    "\n",
    "    vol_c_dstr = cfg_dict.pop('vol/c/dstr', 'fixed')\n",
    "    if vol_c_dstr == 'fixed':\n",
    "        vol_c_ = cfg_dict.pop('vol/c')\n",
    "    elif vol_c_dstr == 'uniform':\n",
    "        vol_c_low_ = cfg_dict.pop('vol/c/low')\n",
    "        vol_c_high_ = cfg_dict.pop('vol/c/high')\n",
    "    elif vol_c_dstr == 'normal':\n",
    "        vol_c_loc_ = cfg_dict.pop('vol/c/loc')\n",
    "        vol_c_scale_ = cfg_dict.pop('vol/c/scale')\n",
    "    elif vol_c_dstr == 'ball':\n",
    "        vol_c_c_ = cfg_dict.pop('vol/c/c')\n",
    "        vol_c_r_ = cfg_dict.pop('vol/c/r')\n",
    "    else:\n",
    "        msg_ = f'vol/c/dstr=\"{vol_c_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    vol_r_dstr = cfg_dict.pop('vol/r/dstr', 'fixed')\n",
    "    if vol_r_dstr == 'fixed':\n",
    "        vol_r_ = cfg_dict.pop('vol/r')\n",
    "    elif vol_r_dstr in ('uniform', 'unifdpow'):\n",
    "        vol_r_low_ = cfg_dict.pop('vol/r/low')\n",
    "        vol_r_high_ = cfg_dict.pop('vol/r/high')\n",
    "    else:\n",
    "        msg_ = f'vol/r/dstr=\"{vol_r_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    #########################################################\n",
    "    ############## Initial Condition Options ###############\n",
    "    #########################################################\n",
    "    ic_dstr = cfg_dict.pop('ic/dstr', None)\n",
    "    if ic_dstr in ('sphere', 'trnvol'):\n",
    "        w_ic = cfg_dict.pop('ic/w')\n",
    "        ic_bpp = cfg_dict.pop('ic/bpp')\n",
    "        ic_n = cfg_dict.pop('ic/n')\n",
    "        ic_frq = cfg_dict.pop('ic/frq')\n",
    "        ic_bs = cfg_dict.pop('ic/bs')\n",
    "        ic_needsampling = True\n",
    "    elif ic_dstr in ('trnsrf',):\n",
    "        w_ic = cfg_dict.pop('ic/w')\n",
    "        ic_bpp = cfg_dict.pop('ic/bpp')\n",
    "        ic_n = n_srf * n_srfpts_mdl\n",
    "        ic_frq = cfg_dict.pop('ic/frq')\n",
    "        ic_bs = ic_n\n",
    "        ic_needsampling = False\n",
    "    elif ic_dstr is None:\n",
    "        ic_needsampling = False\n",
    "        ic_frq = 1\n",
    "        w_ic, ic_bpp = 0, 'all'\n",
    "    else:\n",
    "        msg_ = f'ic/dstr={ic_dstr} not defined'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    if ic_dstr == 'sphere':\n",
    "        ic_c_dstr = cfg_dict.pop('ic/c/dstr', 'fixed')\n",
    "        if ic_c_dstr == 'fixed':\n",
    "            ic_c_ = cfg_dict.pop('ic/c')\n",
    "        elif ic_c_dstr == 'uniform':\n",
    "            ic_c_low_ = cfg_dict.pop('ic/c/low')\n",
    "            ic_c_high_ = cfg_dict.pop('ic/c/high')\n",
    "        elif ic_c_dstr == 'normal':\n",
    "            ic_c_loc_ = cfg_dict.pop('ic/c/loc')\n",
    "            ic_c_scale_ = cfg_dict.pop('ic/c/scale')\n",
    "        else:\n",
    "            msg_ = f'ic/c/dstr=\"{ic_c_dstr}\" not defined!'\n",
    "            raise ValueError(msg_)\n",
    "\n",
    "        ic_r_dstr = cfg_dict.pop('ic/r/dstr', 'fixed')\n",
    "        if ic_r_dstr == 'fixed':\n",
    "            ic_r_ = cfg_dict.pop('ic/r')\n",
    "        elif ic_r_dstr in ('uniform', 'unifdpow'):\n",
    "            ic_r_low_ = cfg_dict.pop('ic/r/low')\n",
    "            ic_r_high_ = cfg_dict.pop('ic/r/high')\n",
    "        else:\n",
    "            msg_ = f'ic/r/dstr=\"{ic_r_dstr}\" not defined!'\n",
    "            raise ValueError(msg_)\n",
    "    elif ic_dstr in ('trnsrf', 'trnvol', None):\n",
    "        pass\n",
    "    else:\n",
    "        msg_ = f'ic/dstr=\"{ic_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    #########################################################\n",
    "    ########### Evaluation Point Sampling Options ###########\n",
    "    #########################################################\n",
    "    eid_list_dup = [opt.split('/')[1] for opt in cfg_dict\n",
    "                    if opt.startswith('eval/')]\n",
    "    eid_list = list(odict.fromkeys(eid_list_dup))\n",
    "    evalcfgs = odict()\n",
    "    for eid in eid_list:\n",
    "        evalcfgs[eid] = odict()\n",
    "        cfgopts = list(cfg_dict.keys())\n",
    "        for opt in cfgopts:\n",
    "            prfx = f'eval/{eid}/'\n",
    "            if opt.startswith(prfx):\n",
    "                optn = opt[len(prfx):]\n",
    "                optv = cfg_dict.pop(opt)\n",
    "                evalcfgs[eid][optn] = optv\n",
    "\n",
    "    #########################################################\n",
    "    ################# I/O Logistics Options #################\n",
    "    #########################################################\n",
    "    config_id = cfg_dict.pop('io/config_id')\n",
    "    results_dir = cfg_dict.pop('io/results_dir')\n",
    "    storage_dir = cfg_dict.pop('io/storage_dir', None)\n",
    "    io_avgfrq = cfg_dict.pop('io/avg/frq')\n",
    "    ioflsh_period = cfg_dict.pop('io/flush/frq')\n",
    "    chkpnt_period = cfg_dict.pop('io/ckpt/frq')\n",
    "    device_name = cfg_dict.pop('io/tch/device')\n",
    "    dtype_name = cfg_dict.pop('io/tch/dtype')\n",
    "    iomon_period = cfg_dict.pop('io/mon/frq')\n",
    "    io_cmprssnlvl = cfg_dict.pop('io/cmprssn_lvl')\n",
    "    eval_bs = cfg_dict.pop('io/eval/bs', None)\n",
    "\n",
    "    dtnow = datetime.datetime.now().isoformat(timespec='seconds')\n",
    "    hostname = socket.gethostname()\n",
    "    commit_hash = get_git_commit()\n",
    "    cfg_tree = '/'.join(config_id.split('/')[:-1])\n",
    "    cfg_name = config_id.split('/')[-1]\n",
    "    #########################################################\n",
    "    ##################### Sanity Checks #####################\n",
    "    #########################################################\n",
    "\n",
    "    # Making sure the specified option distributions are implemented.\n",
    "    fixed_opts = ['desc', 'date', 'rng_seed/list', 'problem',\n",
    "                  'dim', 'vol/n',  'srfpts/n/mdl', 'srfpts/n/trg', \n",
    "                  'srfpts/detspc', 'srfpts/dblsmpl','trg/btstrp', \n",
    "                  'trg/w', 'trg/tau', 'opt/lr', 'opt/epoch']\n",
    "\n",
    "    opt2availdstrs = {**{opt: ('fixed',) for opt in fixed_opts},\n",
    "        'chrg': ('dmm',), 'chrg/n': ('fixed',), 'chrg/w': ('fixed',), \n",
    "        'chrg/mu': ('fixed', 'uniform', 'normal', 'ball'),\n",
    "        'vol': ('ball',), 'vol/c': ('uniform', 'ball', 'normal'), \n",
    "        'vol/r': ('uniform', 'unifdpow'), \n",
    "        'ic': ('sphere', 'trnsrf', 'trnvol', 'fixed'), \n",
    "        'ic/c': ('fixed',), 'nn': ('mlp',), 'ic/r': ('fixed',), \n",
    "        'vol/c/low': ('fixed',), 'vol/c/high': ('fixed',),\n",
    "        'vol/c/loc': ('fixed',), 'vol/c/scale': ('fixed',),\n",
    "        'vol/c/c': ('fixed',),   'vol/c/r': ('fixed',),\n",
    "        'vol/r/low': ('fixed',), 'vol/r/high': ('fixed',),\n",
    "        **{f'eval/{eid}': ('uniform', 'grid', 'ball', 'trnvol')\n",
    "            for eid in eid_list}}\n",
    "\n",
    "    for opt, avail_dstrs in opt2availdstrs.items():\n",
    "        opt_dstr = cfg_dict_input.get(f'{opt}/dstr', 'fixed')\n",
    "        msg_  = f'\"{opt}\" cannot follow the \"{opt_dstr}\" distribution or type '\n",
    "        msg_ += f' since it is not implemented or available at least yet. The '\n",
    "        msg_ += f'only available options for \"{opt}\" are {avail_dstrs}.'\n",
    "        assert opt_dstr in avail_dstrs, msg_\n",
    "\n",
    "    # Making sure no other options are left unused.\n",
    "    if len(cfg_dict) > 0:\n",
    "        msg_ = f'The following settings were left unused:\\n'\n",
    "        for key, val in cfg_dict.items():\n",
    "            msg_ += f'  {key}: {val}'\n",
    "        raise RuntimeError(msg_)\n",
    "\n",
    "    # Making sure that all \"*_dstr\" options are valid\n",
    "    dstr2args = {'uniform':  ('/low', '/high'),\n",
    "                 'unifdpow': ('/low', '/high'),\n",
    "                 'normal':   ('/loc', '/scale'),\n",
    "                 'dmm':      ('/n', '/w', '/mu'),\n",
    "                 'ball':     ('/c', '/r'),\n",
    "                 'sphere':   ('/c', '/r'),\n",
    "                 'fixed':    ('',),\n",
    "                 'trnvol':   (),\n",
    "                 'trnsrf':   ()}\n",
    "\n",
    "    key2req = {'vol': ('/n',)}\n",
    "    if ic_dstr is not None:\n",
    "        key2req['ic'] = (*key2req['ic'], '/w')\n",
    "    if ic_needsampling: \n",
    "        key2req['ic'] = (*key2req['ic'], '/n', '/frq')\n",
    "\n",
    "    for key in ['chrg', 'vol']:\n",
    "        if key in key2req:\n",
    "            chck_dstrargs(key, cfg_dict_input, dstr2args, key2req)\n",
    "\n",
    "    edstr2args = {'uniform': ('/low', '/high', '/n', '/frq'),\n",
    "                  'grid':    ('/low', '/high', '/n', '/frq'),\n",
    "                  'ball':    ('/c', '/r', '/n', '/frq'),\n",
    "                  'fixed':   ('', '/n', '/frq'),\n",
    "                  'trnvol':  ('/n', '/frq')}\n",
    "    for eid in eid_list:\n",
    "        chck_dstrargs(f'eval/{eid}', cfg_dict_input, \n",
    "            edstr2args, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa88d1",
   "metadata": {},
   "source": [
    "## Problem Objects Construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c153ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Derived options and assertions\n",
    "    n_points = n_srfpts_mdl + n_srfpts_trg\n",
    "\n",
    "    assert not (do_dblsampling) or (n_srfpts_trg > 1)\n",
    "    if w_trg is None:\n",
    "        w_trg = n_srfpts_trg / n_points\n",
    "    assert not (n_srfpts_mdl == 0) or (w_trg == 1.0)\n",
    "    n_rsdls = 2 if do_dblsampling else 1\n",
    "\n",
    "    if eval_bs is None:\n",
    "        eval_bs = max(n_srfpts_mdl, n_srfpts_trg) * n_srf\n",
    "        \n",
    "    #########################################################\n",
    "    ########### I/O-Related Options and Operations ##########\n",
    "    #########################################################\n",
    "\n",
    "    name2dtype = dict(float64=torch.double,\n",
    "                      float32=torch.float32,\n",
    "                      float16=torch.float16)\n",
    "    tch_device = torch.device(device_name)\n",
    "    tch_dtype = name2dtype[dtype_name]\n",
    "\n",
    "    tch_dvcmdl = device_name\n",
    "    if device_name.startswith('cuda'):\n",
    "        tch_dvcmdl = torch.cuda.get_device_name(tch_device)\n",
    "\n",
    "    # Reserving 15.596 GB of memory for later usage\n",
    "    # t_gpumem = torch.cuda.get_device_properties(tch_device).total_memory\n",
    "    # tdt_elsize = torch.tensor(1).to(tch_device, tch_dtype).element_size()\n",
    "    # nuslss = int((0.90 * t_gpumem) / tdt_elsize)\n",
    "    # useless_tensor = torch.empty((nuslss,), device=tch_device, dtype=tch_dtype)\n",
    "    # del useless_tensor\n",
    "    \n",
    "    msg_ = f'\"io/mon/frq\" % \"io/avg/frq\" != 0'\n",
    "    assert iomon_period % io_avgfrq == 0, msg_\n",
    "    msg_ = f'\"io/ckpt/frq\" % \"io/avg/frq\" != 0'\n",
    "    assert chkpnt_period % io_avgfrq == 0, msg_\n",
    "    \n",
    "    do_logtb = storage_dir is not None\n",
    "    do_profile = storage_dir is not None\n",
    "    do_tchsave = storage_dir is not None\n",
    "    \n",
    "    assert not(do_logtb) or (storage_dir is not None)\n",
    "    assert not(do_profile) or (storage_dir is not None)\n",
    "    assert not(do_tchsave) or (storage_dir is not None)\n",
    "\n",
    "    #########################################################\n",
    "    ########### Constructing the Batch RNG Object ###########\n",
    "    #########################################################\n",
    "    n_seeds = len(rng_seed_list)\n",
    "    rng_seeds = np.array(rng_seed_list)\n",
    "    rng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "                   device=tch_device, dtype=tch_dtype,\n",
    "                   unif_cache_cols=1_000_000,\n",
    "                   norm_cache_cols=5_000_000)\n",
    "    rng.seed(np.broadcast_to(rng_seeds, rng.shape))\n",
    "\n",
    "    #########################################################\n",
    "    ########## Defining the Poisson Problem Object ##########\n",
    "    #########################################################\n",
    "    assert prob_type == 'poisson'\n",
    "\n",
    "    msg_ = f'chrg_dstr = {chrg_dstr} is not available/implemented.'\n",
    "    assert chrg_dstr in ('dmm',), msg_\n",
    "\n",
    "    trns_opts = dict(dim=dim, chrg_n=chrg_n, sqrt=np.sqrt)\n",
    "\n",
    "    # The poisson delta charge weights\n",
    "    if chrg_w_dstr == 'fixed':\n",
    "        chrg_w_0 = get_arr('chrg_w', '(chrg_n,)', \n",
    "            {**trns_opts, 'chrg_w': chrg_w_})\n",
    "        chrg_w = np.broadcast_to(chrg_w_0[None, ...],\n",
    "                                 (n_seeds, chrg_n)).copy()\n",
    "        assert chrg_w.shape == (n_seeds, chrg_n)\n",
    "    else:\n",
    "        raise ValueError(f'chrg_w_dstr={chrg_w_dstr} '\n",
    "                         'not implemented.')\n",
    "\n",
    "    # The poisson delta charge locations\n",
    "    if chrg_mu_dstr == 'fixed':\n",
    "        chrg_mu_0 = get_arr('chrg_mu', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'chrg_mu': chrg_mu_})\n",
    "        chrg_mu = np.broadcast_to(chrg_mu_0[None, ...],\n",
    "                                  (n_seeds, chrg_n, dim)).copy()\n",
    "        assert chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "    elif chrg_mu_dstr == 'uniform':\n",
    "        chrg_mu_low_0 =  get_arr('chrg_mu_low', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'chrg_mu_low': chrg_mu_low_})\n",
    "        chrg_mu_low = np.broadcast_to(chrg_mu_low_0[None, ...],\n",
    "            (n_seeds, chrg_n, dim)).copy()\n",
    "        assert chrg_mu_low.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        chrg_mu_high_0 =  get_arr('chrg_mu_high', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'chrg_mu_high': chrg_mu_high_})\n",
    "        chrg_mu_high = np.broadcast_to(chrg_mu_high_0[None, ...],\n",
    "            (n_seeds, chrg_n, dim)).copy()\n",
    "        assert chrg_mu_high.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        rnds = rng.uniform((n_seeds, chrg_n, dim)).detach().cpu().numpy()\n",
    "        chrg_mu = chrg_mu_low + rnds * (chrg_mu_high - chrg_mu_low)\n",
    "        assert chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "    elif chrg_mu_dstr == 'normal':\n",
    "        chrg_mu_loc_0 =  get_arr('chrg_mu_loc', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'chrg_mu_loc': chrg_mu_loc_})\n",
    "        chrg_mu_loc = np.broadcast_to(chrg_mu_loc_0[None, ...],\n",
    "            (n_seeds, chrg_n, dim)).copy()\n",
    "        assert chrg_mu_loc.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        chrg_mu_scale_0 =  get_arr('chrg_mu_scale', '(chrg_n,)', \n",
    "            {**trns_opts, 'chrg_mu_scale': chrg_mu_scale_})\n",
    "        chrg_mu_scale = np.broadcast_to(chrg_mu_scale_0[None, ...],\n",
    "            (n_seeds, chrg_n)).copy()\n",
    "        assert chrg_mu_scale.shape == (n_seeds, chrg_n)\n",
    "        \n",
    "        rnds = rng.normal((n_seeds, chrg_n, dim)).detach().cpu().numpy()\n",
    "        chrg_mu = chrg_mu_loc + rnds * chrg_mu_scale.reshape(n_seeds, chrg_n, 1)\n",
    "        assert chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "    elif chrg_mu_dstr == 'ball':\n",
    "        chrg_mu_c_0 =  get_arr('chrg_mu_c', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'chrg_mu_c': chrg_mu_c_})\n",
    "        chrg_mu_c = np.broadcast_to(chrg_mu_c_0[None, ...],\n",
    "            (n_seeds, chrg_n, dim)).copy()\n",
    "        assert chrg_mu_c.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        chrg_mu_r_0 =  get_arr('chrg_mu_r', '(chrg_n,)', \n",
    "            {**trns_opts, 'chrg_mu_r': chrg_mu_r_})\n",
    "        chrg_mu_r = np.broadcast_to(chrg_mu_r_0[None, ...],\n",
    "            (n_seeds, chrg_n)).copy()\n",
    "        assert chrg_mu_r.shape == (n_seeds, chrg_n)\n",
    "        \n",
    "        rnds1 = rng.normal((n_seeds, chrg_n, dim)).detach().cpu().numpy()\n",
    "        rnds1_tilde = rnds1 / np.sqrt((rnds1**2).sum(axis=-1, keepdims=True))\n",
    "        assert rnds1_tilde.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        rnds2 = rng.uniform((n_seeds, chrg_n)).detach().cpu().numpy()\n",
    "        rnds2_tilde = rnds2 ** (1.0 / dim)\n",
    "        assert rnds2_tilde.shape == (n_seeds, chrg_n)\n",
    "        \n",
    "        rnds3_tilde = (chrg_mu_r * rnds2_tilde).reshape(n_seeds, chrg_n, 1)\n",
    "        assert rnds3_tilde.shape == (n_seeds, chrg_n, 1)\n",
    "        \n",
    "        chrg_mu = chrg_mu_c + rnds1_tilde * rnds3_tilde\n",
    "        assert chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "    else:\n",
    "        raise ValueError(f'chrg_mu_dstr={chrg_mu_dstr} '\n",
    "                         'not implemented.')\n",
    "\n",
    "    # Defining the problem object\n",
    "    problem = DeltaProblem(weights=chrg_w, locations=chrg_mu,\n",
    "        tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "    \n",
    "    #########################################################\n",
    "    ######### Defining the Latent Parameter Search ##########\n",
    "    #########################################################\n",
    "    if srch_dstr in ('mcmc',):\n",
    "        true_problem = problem\n",
    "        \n",
    "        assert srch_prior_mu_dstr == 'normal'\n",
    "        true_chrg_mu = chrg_mu\n",
    "        \n",
    "        srch_prior_mu_loc_0 =  get_arr('srch_prior_mu_loc', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'srch_prior_mu_loc': srch_prior_mu_loc_})\n",
    "        srch_prior_mu_loc = np.broadcast_to(srch_prior_mu_loc_0[None, ...],\n",
    "            (n_seeds, chrg_n, dim)).copy()\n",
    "        assert srch_prior_mu_loc.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        srch_prior_mu_scale_0 =  get_arr('srch_prior_mu_scale', '(chrg_n,)', \n",
    "            {**trns_opts, 'srch_prior_mu_scale': srch_prior_mu_scale_})\n",
    "        srch_prior_mu_scale = np.broadcast_to(srch_prior_mu_scale_0[None, ...],\n",
    "            (n_seeds, chrg_n)).copy()\n",
    "        assert srch_prior_mu_scale.shape == (n_seeds, chrg_n)\n",
    "        \n",
    "        srch_prior_mu_loc_tch = torch.from_numpy(srch_prior_mu_loc)\n",
    "        srch_prior_mu_loc_tch = srch_prior_mu_loc_tch.to(device=tch_device, dtype=tch_dtype)\n",
    "        assert srch_prior_mu_loc_tch.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        srch_prior_mu_scale_tch = torch.from_numpy(srch_prior_mu_scale)\n",
    "        srch_prior_mu_scale_tch = srch_prior_mu_scale_tch.to(device=tch_device, dtype=tch_dtype)\n",
    "        assert srch_prior_mu_scale_tch.shape == (n_seeds, chrg_n)\n",
    "        \n",
    "        rnds = rng.normal((n_seeds, chrg_n, dim))\n",
    "        old_chrg_mu = srch_prior_mu_loc_tch + rnds * srch_prior_mu_scale_tch.reshape(n_seeds, chrg_n, 1)\n",
    "        assert old_chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        chrg_mu = old_chrg_mu.detach().cpu().numpy()\n",
    "        # Re-creating the problem instance with randomized initial parameters for search\n",
    "        problem = DeltaProblem(weights=chrg_w, locations=chrg_mu,\n",
    "            tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "\n",
    "        old_logpri, old_loglike, old_logpost = None, None, None\n",
    "        \n",
    "        # Total number of MCMC search draws\n",
    "        n_srchdraws = (n_epochs - srch_inittrn) // srch_frq\n",
    "        \n",
    "    elif srch_dstr is None:\n",
    "        pass\n",
    "    else:\n",
    "        msg_ = f'srch/dstr=\"{srch_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    if srch_dstr is not None:\n",
    "        n_cpg = n_seeds // n_grps\n",
    "        assert n_seeds == (n_grps * n_cpg)\n",
    "        \n",
    "        assert n_cpg % srch_reset_k == 0\n",
    "        \n",
    "        assert obs_x_dstr == 'uniform'\n",
    "        \n",
    "        obs_x_low_0 =  get_arr('obs_x_low', '(obs_n, dim)', \n",
    "            {**trns_opts, 'obs_x_low': obs_x_low_})\n",
    "        obs_x_low = np.broadcast_to(obs_x_low_0[None, ...],\n",
    "            (n_seeds, obs_n, dim)).copy()\n",
    "        assert obs_x_low.shape == (n_seeds, obs_n, dim)\n",
    "        \n",
    "        obs_x_high_0 =  get_arr('obs_x_high', '(obs_n, dim)', \n",
    "            {**trns_opts, 'obs_x_high': obs_x_high_})\n",
    "        obs_x_high = np.broadcast_to(obs_x_high_0[None, ...],\n",
    "            (n_seeds, obs_n, dim)).copy()\n",
    "        assert obs_x_high.shape == (n_seeds, obs_n, dim)\n",
    "        \n",
    "        rnds = rng.uniform((n_seeds, obs_n, dim))\n",
    "        \n",
    "        # Creating the x observations, and making sure that each \n",
    "        # group gets the same set of observations.\n",
    "        obs_x_low_tch = torch.from_numpy(obs_x_low).to(device=tch_device, dtype=tch_dtype)\n",
    "        obs_x_high_tch = torch.from_numpy(obs_x_high).to(device=tch_device, dtype=tch_dtype)\n",
    "        \n",
    "        obs_x1 = obs_x_low_tch + rnds * (obs_x_high_tch - obs_x_low_tch)\n",
    "        assert obs_x1.shape == (n_seeds, obs_n, dim)\n",
    "        obs_x2 = obs_x1.reshape(n_grps, n_cpg, obs_n, dim)\n",
    "        assert obs_x2.shape == (n_grps, n_cpg, obs_n, dim)\n",
    "        obs_x3 = obs_x2[:, :1, ...]\n",
    "        assert obs_x3.shape == (n_grps, 1, obs_n, dim)\n",
    "        obs_x4 = obs_x3.expand(n_grps, n_cpg, obs_n, dim)\n",
    "        assert obs_x4.shape == (n_grps, n_cpg, obs_n, dim)\n",
    "        obs_x = obs_x4.reshape(n_seeds, obs_n, dim)\n",
    "        assert obs_x.shape == (n_seeds, obs_n, dim)\n",
    "        \n",
    "        if obs_y_dstr == 'potential':\n",
    "            obs_ydim = 1\n",
    "            obs_y = true_problem.potential(obs_x).unsqueeze(-1)\n",
    "            assert obs_y.shape == (n_seeds, obs_n, obs_ydim)\n",
    "            obs_y = obs_y - obs_y.mean(dim=-2, keepdim=True)\n",
    "            assert obs_y.shape == (n_seeds, obs_n, obs_ydim)\n",
    "        elif obs_y_dstr == 'field':\n",
    "            obs_ydim = dim\n",
    "            obs_y = true_problem.field(obs_x)\n",
    "            assert obs_y.shape == (n_seeds, obs_n, obs_ydim)\n",
    "        else:\n",
    "            raise ValueError(f'obs_y_dstr={obs_y_dstr} not implemented.')\n",
    "\n",
    "    #########################################################\n",
    "    ####### Defining the Initial Condition Parameters #######\n",
    "    #########################################################\n",
    "    msg_ = f'ic/dstr = {ic_dstr} is not available/implemented.'\n",
    "    assert ic_dstr in ('sphere', 'trnsrf', None), msg_\n",
    "    msg_ = '\"ic/bpp\" must be either \"bias\" or \"all\".'\n",
    "    assert ic_bpp in ('bias', 'all', None), msg_\n",
    "\n",
    "    if ic_dstr == 'sphere':\n",
    "        if ic_c_dstr == 'fixed':\n",
    "            ic_c_0_np = get_arr('ic_c', '(dim,)', \n",
    "                {**trns_opts, 'ic_c': ic_c_})\n",
    "            ic_c_np = np.broadcast_to(ic_c_0_np[None, ...], \n",
    "                (n_seeds, dim)).copy()\n",
    "            assert ic_c_np.shape == (n_seeds, dim)\n",
    "        else:\n",
    "            raise ValueError(f'ic_c_dstr={ic_c_dstr} '\n",
    "                            'not implemented.')\n",
    "\n",
    "        if ic_r_dstr == 'fixed':\n",
    "            ic_r_0_np = get_arr('ic_r', '()', \n",
    "                {**trns_opts, 'ic_r': ic_r_})\n",
    "            ic_r_np = np.broadcast_to(ic_r_0_np[None, ...], \n",
    "                (n_seeds,)).copy()\n",
    "            assert ic_r_np.shape == (n_seeds,)\n",
    "        else:\n",
    "            raise ValueError(f'ic_r_dstr={ic_r_dstr} '\n",
    "                            'not implemented.')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ic_c = torch.from_numpy(ic_c_np).to(device=tch_device, \n",
    "                dtype=tch_dtype).reshape(n_seeds, 1, dim).expand(n_seeds, ic_n, dim)\n",
    "            assert ic_c.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "            ic_r = torch.from_numpy(ic_r_np).to(device=tch_device, \n",
    "                dtype=tch_dtype).reshape(n_seeds, 1, 1).expand(n_seeds, ic_n, 1)\n",
    "            assert ic_r.shape == (n_seeds, ic_n, 1)\n",
    "    elif ic_dstr in ('trnsrf', 'trnvol', None):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'ic_dstr={ic_dstr} not implemented')\n",
    "\n",
    "    #########################################################\n",
    "    ########## Defining the Volume Sampling Object ##########\n",
    "    #########################################################\n",
    "    msg_ = f'vol/dstr = {vol_dstr} is not available/implemented.'\n",
    "    assert vol_dstr in ('ball',), msg_\n",
    "\n",
    "    if vol_c_dstr == 'uniform':\n",
    "        vol_c_low_0 = get_arr('vol_c_low', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_low': vol_c_low_})\n",
    "        vol_c_low = np.broadcast_to(vol_c_low_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_low.shape == (n_seeds, dim)\n",
    "\n",
    "        vol_c_high_0 = get_arr('vol_c_high', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_high': vol_c_high_})\n",
    "        vol_c_high = np.broadcast_to(vol_c_high_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_high.shape == (n_seeds, dim)\n",
    "        \n",
    "        vol_c_params = dict(low=vol_c_low, high=vol_c_high)\n",
    "    elif vol_c_dstr == 'normal':\n",
    "        vol_c_loc_0 = get_arr('vol_c_loc', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_loc': vol_c_loc_})\n",
    "        vol_c_loc = np.broadcast_to(vol_c_loc_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_loc.shape == (n_seeds, dim)\n",
    "\n",
    "        vol_c_scale_0 = get_arr('vol_c_scale', '()', \n",
    "            {**trns_opts, 'vol_c_scale': vol_c_scale_})\n",
    "        vol_c_scale = np.broadcast_to(vol_c_scale_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_c_scale.shape == (n_seeds,)\n",
    "        \n",
    "        vol_c_params = dict(loc=vol_c_loc, scale=vol_c_scale)\n",
    "    elif vol_c_dstr == 'ball':\n",
    "        vol_c_c_0 = get_arr('vol_c_c', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_c': vol_c_c_})\n",
    "        vol_c_c = np.broadcast_to(vol_c_c_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_c.shape == (n_seeds, dim)\n",
    "\n",
    "        vol_c_r_0 = get_arr('vol_c_r', '()', \n",
    "            {**trns_opts, 'vol_c_r': vol_c_r_})\n",
    "        vol_c_r = np.broadcast_to(vol_c_r_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_c_r.shape == (n_seeds,)\n",
    "        \n",
    "        vol_c_params = dict(c=vol_c_c, r=vol_c_r)\n",
    "    else:\n",
    "        raise ValueError(f'vol_c_dstr={vol_c_dstr} not implemented.')\n",
    "\n",
    "    if vol_r_dstr in ('uniform', 'unifdpow'):\n",
    "        vol_r_low_0 = get_arr('vol_r_low', '()', \n",
    "            {**trns_opts, 'vol_r_low': vol_r_low_, \n",
    "             'vol_r_high': vol_r_high_})\n",
    "        vol_r_low = np.broadcast_to(vol_r_low_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_r_low.shape == (n_seeds,)\n",
    "\n",
    "        vol_r_high_0 = get_arr('vol_r_high', '()', \n",
    "            {**trns_opts, 'vol_r_low': vol_r_low_, \n",
    "             'vol_r_high': vol_r_high_})\n",
    "        vol_r_high = np.broadcast_to(vol_r_high_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_r_high.shape == (n_seeds,)\n",
    "        \n",
    "        vol_r_params = dict(low=vol_r_low, high=vol_r_high)\n",
    "    else:\n",
    "        raise ValueError(f'vol_r_dstr={vol_r_dstr} not implemented.')\n",
    "\n",
    "    volsampler = BallSampler(c_dstr=vol_c_dstr, c_params=vol_c_params,\n",
    "                             r_dstr=vol_r_dstr, r_params=vol_r_params,\n",
    "                             batch_rng=rng)\n",
    "\n",
    "    srfsampler = SphereSampler(batch_rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    #### Evaluation Param Tensorization and Sanitization ####\n",
    "    #########################################################\n",
    "\n",
    "    # The following evaluates the 'eval/*' options and creates \n",
    "    # array parameters for evaluation.\n",
    "    # The input is mainly the `evalcfgs` dictionary, which holds \n",
    "    # some keys and list/string values. The output will be the \n",
    "    # `evalprms` dictionary which has the same keys but with \n",
    "    # np.array values.\n",
    "    # --------------\n",
    "    # Example input: \n",
    "    #   evalcfgs = {'ur': {'dstr': 'uniform', \n",
    "    #       'low': [0], \n",
    "    #       'high': '[sqrt(dim)]'}}\n",
    "    # --------------\n",
    "    # Example output\n",
    "    #   evalprms = {'ur': {'dstr': 'uniform',\n",
    "    #       'low' : torch.tensor([0]).expand(n_seeds, dim)\n",
    "    #       'high': torch.tensor(np.sqrt(dim)).expand(n_seeds, dim))}}\n",
    "\n",
    "    dstr2shapes = {'uniform': {'low':  '(dim,)', \n",
    "                               'high': '(dim,)'},\n",
    "                   'grid':    {'low':  '(dim,)', \n",
    "                               'high': '(dim,)'},\n",
    "                   'ball':    {'c':    '(dim,)', \n",
    "                               'r':    '()'    },\n",
    "                   'trnvol': {}}\n",
    "\n",
    "    assert all('dstr' in eopts for eopts in evalcfgs.values())\n",
    "    assert all('frq'  in eopts for eopts in evalcfgs.values())\n",
    "    assert all('n'    in eopts for eopts in evalcfgs.values())\n",
    "    evalprms = odict()\n",
    "    for eid, eopts_ in evalcfgs.items():\n",
    "        eopts = eopts_.copy()\n",
    "        eparam = odict()\n",
    "        for eopt in ('dstr', 'n', 'frq'):\n",
    "            eparam[eopt] = eopts.pop(eopt)\n",
    "\n",
    "        edstr = eparam['dstr']\n",
    "        msg_  = f'Unknown eval \"{eid}\" dstr -> \"{edstr}\". '\n",
    "        msg_ += f'dstr should be one of {dstr2shapes.keys()}.'\n",
    "        assert edstr in dstr2shapes, msg_\n",
    "\n",
    "        estore_dflt = (edstr == 'grid')\n",
    "        eparam['store'] = eopts.pop('store', estore_dflt)\n",
    "\n",
    "        opts2shape = dstr2shapes[edstr]\n",
    "        for eopt, eoptshpstr in opts2shape.items():\n",
    "            # Example: edstr = 'uniform'\n",
    "            #          eopt = 'low'\n",
    "            #          eoptshpstr = '(dim,)'\n",
    "            #          eoptshp = (dim,)\n",
    "            #          eoptval = \"[sqrt(dim)]\"\n",
    "            #          eopt_pnp0 = np.array([sqrt(dim)]*dim)\n",
    "            #          eopt_pnp0.shape = (dim,)\n",
    "            #          eopt_pnp = eopt_pnp0.expand(n_seeds, dim)\n",
    "            #          eopt_pnp.shape = (n_seeds, dim)\n",
    "            #          eopt_p = torch.from_numpy(eopt_pa0)\n",
    "            #          eopt_p.shape = (n_seeds, dim)\n",
    "            msg_  = f'\"eval/{eid}/{eopt}\" must be fixed and determined.'\n",
    "            msg_ += f' Hierarchical support not available yet.'\n",
    "            assert eopt in eopts, msg_\n",
    "\n",
    "            eoptval = eopts.pop(eopt)\n",
    "            etrns = {eopt: eoptval, 'dim': dim, \n",
    "                     'sqrt': np.sqrt}\n",
    "\n",
    "            eopt_pnp0 = get_arr(eopt, eoptshpstr, etrns)        \n",
    "            eoptshp = eval_formula(eoptshpstr, {'dim': dim})\n",
    "            eopt_pnp = np.broadcast_to(eopt_pnp0[None, ...],\n",
    "                                       (n_seeds, *eoptshp)).copy()\n",
    "            assert eopt_pnp.shape == (n_seeds, *eoptshp)\n",
    "            eopt_pc = torch.from_numpy(eopt_pnp)\n",
    "            eopt_p = eopt_pc.to(device=tch_device, dtype=tch_dtype)\n",
    "            assert eopt_p.shape == (n_seeds, *eoptshp)\n",
    "            eparam[eopt] = eopt_p\n",
    "\n",
    "        assert len(eopts) == 0, f'unused eval items left: {eopts}'\n",
    "        evalprms[eid] = eparam\n",
    "\n",
    "    #########################################################\n",
    "    ############# Evaluation Parameter Creation #############\n",
    "    #########################################################\n",
    "    for eid, eopts in evalprms.items():\n",
    "        edstr = eopts['dstr']\n",
    "        n_evlpnts = eopts['n']\n",
    "        if edstr == 'uniform':\n",
    "            e_low_ = eopts['low']\n",
    "            assert e_low_.shape == (n_seeds, dim)\n",
    "            e_low = e_low_.unsqueeze(dim=-2)\n",
    "            assert e_low.shape == (n_seeds, 1, dim)\n",
    "\n",
    "            e_high_ = eopts['high']\n",
    "            assert e_high_.shape == (n_seeds, dim)\n",
    "            e_high = e_high_.unsqueeze(dim=-2)\n",
    "            assert e_high.shape == (n_seeds, 1, dim)\n",
    "\n",
    "            e_slope = e_high - e_low\n",
    "            assert e_slope.shape == (n_seeds, 1, dim)\n",
    "\n",
    "            eopts['bias'] = e_low\n",
    "            eopts['slope'] = e_slope\n",
    "        elif edstr == 'ball':\n",
    "            e_c_ = eopts['c']\n",
    "            assert e_c_.shape == (n_seeds, dim)\n",
    "            e_c = e_c_.unsqueeze(dim=-2).expand(n_seeds, n_evlpnts, dim)\n",
    "            assert e_c.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "            e_r_ = eopts['r']\n",
    "            assert e_r_.shape == (n_seeds,)\n",
    "\n",
    "            e_r = e_r_.reshape(n_seeds, 1, 1).expand(n_seeds, n_evlpnts, 1)\n",
    "            assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "            eopts['c_xpnd'] = e_c\n",
    "            eopts['r_xpnd'] = e_r\n",
    "        elif edstr == 'trnvol':\n",
    "            pass\n",
    "        elif edstr == 'grid':\n",
    "            n_g = eopts['n']\n",
    "            n_gpd = int(np.ceil(n_g**(1./dim)))\n",
    "            assert n_g == (n_gpd ** dim)\n",
    "\n",
    "            elowt = eopts['low']\n",
    "            assert elowt.shape == (n_seeds, dim)\n",
    "\n",
    "            ehight = eopts['high']\n",
    "            assert ehight.shape == (n_seeds, dim)\n",
    "\n",
    "            assert (elowt[:1] == elowt).all()\n",
    "            assert (ehight[:1] == ehight).all()\n",
    "\n",
    "            elow = elowt.cpu().detach().numpy()[0].tolist()\n",
    "            ehigh = ehight.cpu().detach().numpy()[0].tolist()\n",
    "\n",
    "            gdict = make_grid(elow, ehigh, dim, n_gpd, 'torch')\n",
    "            e_pnts_ = gdict['x']\n",
    "            assert e_pnts_.shape == (n_g, dim)\n",
    "\n",
    "            e_pnts = e_pnts_.reshape(1, n_g, dim).expand(n_seeds, n_g, dim)\n",
    "            assert e_pnts.shape == (n_seeds, n_g, dim)\n",
    "\n",
    "            eopts['pnts'] = e_pnts.to(tch_device, tch_dtype)\n",
    "            eopts['xi_msh_np'] = gdict['xi_msh_np']\n",
    "        else:\n",
    "            raise ValueError(f'\"{edstr}\" not defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e371f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    #### Collecting the Config Columns in the Dataframe #####\n",
    "    #########################################################\n",
    "    # Identifying the hyper-parameter from etc config columns\n",
    "    hppats = ['problem', 'dim', 'vol/n', 'srfpts/n/mdl', \n",
    "        'srfpts/n/trg',  'srfpts/detspc', 'srfpts/dblsmpl',\n",
    "        'trg/w', 'trg/btstrp', 'trg/tau', 'trg/reg/w', 'opt/lr', \n",
    "        'opt/dstr',  'nn/*', 'chrg/*', 'ic/*', 'vol/*', 'eval/*', 'srch/*']\n",
    "    etcpats = ['desc', 'date', 'opt/epoch', 'rng_seed/list', 'io/*']\n",
    "\n",
    "    hpopts = [x for pat in hppats for x in \n",
    "               fnmatch.filter(cfg_dict_input.keys(), pat)]\n",
    "    etcopts = [x for pat in etcpats for x in \n",
    "               fnmatch.filter(cfg_dict_input.keys(), pat)]\n",
    "\n",
    "    err_list = []\n",
    "    for opt in cfg_dict_input:\n",
    "        if (opt in hpopts) and (opt in etcopts):\n",
    "            msg_ = f'\"{opt}\" should both be treated as hp and etc!'\n",
    "            err_list.append(msg_)\n",
    "        if (opt not in hpopts) and (opt not in etcopts):\n",
    "            msg_ = f'\"{opt}\" is neither hp nor etc!'\n",
    "            err_list.append(msg_)\n",
    "    if len(err_list) > 0:\n",
    "        raise RuntimeError(('\\n'+80*'*'+'\\n').join(err_list))\n",
    "\n",
    "    # Converting the list and tuples to strings\n",
    "    hp_dict_ = odict()\n",
    "    etc_dict_ = odict()\n",
    "    for opt, val in cfg_dict_input.items():\n",
    "        val = cfg_dict_input[opt]\n",
    "        if isinstance(val, (int, float, str, bool, type(None))):\n",
    "            srlval = val\n",
    "        elif isinstance(val, (list, tuple)):\n",
    "            srlval = repr(val)\n",
    "        else: \n",
    "            msg_  = f'Not sure how to log \"{opt}\" with '\n",
    "            msg_ += f'a value type of \"{type(val)}\"'\n",
    "            raise RuntimeError(msg_)\n",
    "\n",
    "        if opt in hpopts:\n",
    "            hp_dict_[opt] = srlval\n",
    "        elif opt in etcopts:\n",
    "            etc_dict_[opt] = srlval\n",
    "        else:\n",
    "            raise RuntimeError(f'Not sure how to log \"{opt}\"')\n",
    "\n",
    "    # Few exceptions for the etc directory\n",
    "    etc_dict_['hostname'] = hostname\n",
    "    etc_dict_['commit'] = commit_hash\n",
    "    etc_dict_['date/cfg'] = etc_dict_.pop('date')\n",
    "    etc_dict_['date/run'] = dtnow\n",
    "    etc_dict_['io/dvc_mdl'] = tch_dvcmdl\n",
    "    etc_dict_.pop('io/results_dir')\n",
    "    etc_dict_.pop('io/storage_dir')\n",
    "\n",
    "    # Repeating the values by n_seeds\n",
    "    hp_dict = odict()\n",
    "    for opt, val in hp_dict_.items():\n",
    "        hp_dict[opt] = [val] * n_seeds\n",
    "    etc_dict = odict()\n",
    "    for opt, val in etc_dict_.items():\n",
    "        etc_dict[opt] = [val] * n_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2f04f",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21829bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if results_dir is not None:\n",
    "        pathlib.Path(os.sep.join([results_dir, cfg_tree])\n",
    "                     ).mkdir(parents=True, exist_ok=True)\n",
    "    if storage_dir is not None:\n",
    "        cfgstrgpnt_dir = os.sep.join([storage_dir, cfg_tree, cfg_name])\n",
    "        pathlib.Path(cfgstrgpnt_dir).mkdir(parents=True, exist_ok=True)\n",
    "        strgidx = sum(isdir(f'{cfgstrgpnt_dir}/{x}') for x in os.listdir(cfgstrgpnt_dir))\n",
    "        dtnow_ = dtnow[2:].replace('-', '').replace(':', '').replace('.', '')\n",
    "        cfgstrg_dir = f'{cfgstrgpnt_dir}/{strgidx:02d}_{dtnow_}'\n",
    "        pathlib.Path(cfgstrg_dir).mkdir(parents=True, exist_ok=True)\n",
    "    if do_logtb:\n",
    "        if 'tbwriter' in locals():\n",
    "            tbwriter.close()\n",
    "        tbwriter = tensorboardX.SummaryWriter(cfgstrg_dir)\n",
    "    if do_profile:\n",
    "        profiler = Profiler()\n",
    "        profiler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914234e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initializing the model\n",
    "    model = bffnn(dim, nn_width, nn_hidden, nn_act, (n_seeds,), rng)\n",
    "    if do_bootstrap:\n",
    "        target = bffnn(dim, nn_width, nn_hidden, nn_act, (n_seeds,), rng)\n",
    "        target.load_state_dict(model.state_dict())\n",
    "    else:\n",
    "        target = model\n",
    "\n",
    "    # Set the optimizer\n",
    "    if opt_type == 'adam':\n",
    "        opt = torch.optim.Adam(model.parameters(), lr)\n",
    "    elif opt_type == 'sgd':\n",
    "        opt = torch.optim.SGD(model.parameters(), lr)\n",
    "    else:\n",
    "        raise NotImplementedError(f'opt/dstr=\"{opt_type}\" not implmntd')\n",
    "\n",
    "    # Evaluation tools\n",
    "    erng = rng\n",
    "    last_perfdict = dict()\n",
    "    ema = EMA(gamma=0.999, gamma_sq=0.998)\n",
    "    trn_sttime = time.time()\n",
    "\n",
    "    # Data writer construction\n",
    "    hdfpth = None\n",
    "    if results_dir is not None:\n",
    "        hdfpth = f'{results_dir}/{cfg_tree}/{cfg_name}.h5'\n",
    "    avg_history = odict()\n",
    "    dwriter = DataWriter(flush_period=ioflsh_period*n_seeds, \n",
    "                         compression_level=io_cmprssnlvl)\n",
    "\n",
    "    if storage_dir is not None:\n",
    "        with plt.style.context('default'):\n",
    "            figax_list = [plt.subplots(1, 1, figsize=(3.2, 2.5), dpi=100) for _ in range(3)]\n",
    "            (fig_mdl, ax_mdl), (fig_trg, ax_trg), (fig_gt, ax_gt) = figax_list\n",
    "            cax_list = [make_axes_locatable(ax).append_axes('right', size='5%', pad=0.05) \n",
    "                        for ax in (ax_mdl, ax_trg, ax_gt)]\n",
    "            cax_mdl, cax_trg, cax_gt = cax_list\n",
    "        stat_history = defaultdict(list)\n",
    "        model_history = odict()\n",
    "        target_history = odict()\n",
    "        \n",
    "        if (srch_dstr is not None) and (dim == 2):\n",
    "            with plt.style.context('default'):\n",
    "                fig_srch, ax_srch = plt.subplots(1, 1, figsize=(3.2, 2.5), dpi=100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "506d3256",
   "metadata": {},
   "source": [
    "for epoch in range(n_epochs+1):\n",
    "    if srch_dstr is not None:\n",
    "        update_problem = (epoch >= srch_inittrn) and ((epoch - srch_inittrn) % srch_frq == 0)\n",
    "        if update_problem:\n",
    "            if srch_dstr in ('mcmc',):\n",
    "                prpsd_chrg_mu = old_chrg_mu + srch_sigma * rng.normal((n_seeds, chrg_n, dim))\n",
    "                assert prpsd_chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "                \n",
    "                chrg_mu_tch = prpsd_chrg_mu + 0.05 * rng.normal((n_seeds, chrg_n, dim))\n",
    "                chrg_mu = chrg_mu_tch.detach().cpu().numpy()\n",
    "                assert chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "                problem.locations = chrg_mu\n",
    "                problem.locations_tch = chrg_mu_tch\n",
    "            else:\n",
    "                raise ValueError(f'srch_dstr={srch_dstr} not implemented')\n",
    "    \n",
    "    update_search = False\n",
    "    if srch_dstr is not None:  \n",
    "        update_search = ((epoch - srch_inittrn) % srch_frq == (srch_frq - 1))\n",
    "        update_search = update_search and (epoch >= srch_inittrn)\n",
    "            \n",
    "    if update_search:\n",
    "        assert srch_dstr == 'mcmc'\n",
    "        \n",
    "        need_field = (obs_y_dstr == 'field')\n",
    "        sol_key = {'potential': 'v', 'field': 'e'}[obs_y_dstr]\n",
    "        \n",
    "        # Computing the problem solution\n",
    "        obs_probsol = get_prob_sol(problem, obs_x, n_eval=eval_bs, \n",
    "            get_field=need_field, out_lib='torch')[sol_key]\n",
    "        if (obs_y_dstr == 'potential'):\n",
    "            obs_probsol = obs_probsol.unsqueeze(-1)\n",
    "            obs_probsol = obs_probsol - obs_probsol.mean(dim=-2, keepdim=True)\n",
    "        assert obs_probsol.shape == (n_seeds, obs_n, obs_ydim)\n",
    "\n",
    "        # Computing the model solution\n",
    "        obs_mdlsol = get_nn_sol(model, obs_x, n_eval=eval_bs,\n",
    "            get_field=need_field, out_lib='torch')[sol_key]\n",
    "        if (obs_y_dstr == 'potential'):\n",
    "            obs_mdlsol = obs_mdlsol.unsqueeze(-1)\n",
    "            obs_mdlsol = obs_mdlsol - obs_mdlsol.mean(dim=-2, keepdim=True)\n",
    "        assert obs_mdlsol.shape == (n_seeds, obs_n, obs_ydim)\n",
    "        \n",
    "        ##### BBBBBBBBBBBBAAAAAAAAAAAADDDDDDDDDDDDDDDDDDDD\n",
    "        obs_mdlsol = obs_probsol\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            obs_mdlerr = obs_mdlsol - obs_y\n",
    "            assert obs_mdlerr.shape == (n_seeds, obs_n, obs_ydim)\n",
    "            \n",
    "            assert srch_metric_dstr == 'mse'\n",
    "            \n",
    "            obs_loss = obs_mdlerr.square().sum(dim=-1)\n",
    "            assert obs_loss.shape == (n_seeds, obs_n)\n",
    "                \n",
    "            if (srch_metric_min is not None) or (srch_metric_max is not None):\n",
    "                obs_lossclp = torch.clip(obs_loss, srch_metric_min, srch_metric_max)\n",
    "                assert obs_lossclp.shape == (n_seeds, obs_n)\n",
    "            else:\n",
    "                obs_lossclp = obs_loss\n",
    "                assert obs_lossclp.shape == (n_seeds, obs_n)\n",
    "            \n",
    "            prpsd_loglike = obs_lossclp.mean(dim=-1) * (-srch_metric_coeff)\n",
    "            assert prpsd_loglike.shape == (n_seeds,)\n",
    "            \n",
    "            prpsd_logpri_ = normal_logprob(prpsd_chrg_mu, srch_prior_mu_loc_tch, srch_prior_mu_scale_tch)\n",
    "            assert prpsd_logpri_.shape == (n_seeds, chrg_n)\n",
    "            \n",
    "            prpsd_logpri = prpsd_logpri_.sum(dim=-1)\n",
    "            assert prpsd_logpri.shape == (n_seeds,)\n",
    "            \n",
    "            prpsd_logpost = prpsd_logpri + prpsd_loglike\n",
    "            assert prpsd_logpost.shape == (n_seeds,)\n",
    "            \n",
    "            if old_logpri is None:\n",
    "                old_logpri = prpsd_logpri\n",
    "                assert old_logpri.shape == (n_seeds,)\n",
    "            \n",
    "                old_loglike = prpsd_loglike\n",
    "                assert old_loglike.shape == (n_seeds,)\n",
    "                \n",
    "                old_logpost = prpsd_logpost\n",
    "                assert old_logpost.shape == (n_seeds,)\n",
    "                \n",
    "                best_loglike = old_loglike\n",
    "                assert best_loglike.shape == (n_seeds,)\n",
    "\n",
    "            delta_logpost = prpsd_logpost - old_logpost\n",
    "            assert delta_logpost.shape == (n_seeds,)\n",
    "            \n",
    "            acceptance = (srch_tmprtr * delta_logpost) > rng.uniform((n_seeds, 1)).log().squeeze(-1)\n",
    "            assert acceptance.shape == (n_seeds,)\n",
    "            \n",
    "            # Updating the monte carlo chains and log-probabilities\n",
    "            old_chrg_mu = torch.where(acceptance.reshape(n_seeds, 1, 1), prpsd_chrg_mu, old_chrg_mu)\n",
    "            assert old_chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "            \n",
    "            old_loglike = torch.where(acceptance, prpsd_loglike, old_loglike)\n",
    "            assert old_loglike.shape == (n_seeds,)\n",
    "            \n",
    "            old_logpri = torch.where(acceptance, prpsd_logpri, old_logpri)\n",
    "            assert old_logpri.shape == (n_seeds,)\n",
    "            \n",
    "            old_logpost =  torch.where(acceptance, prpsd_logpost, old_logpost)\n",
    "            assert old_logpost.shape == (n_seeds,)\n",
    "            \n",
    "            best_loglike = torch.where(old_loglike > best_loglike, old_loglike, best_loglike)\n",
    "            assert best_loglike.shape == (n_seeds,)\n",
    "            \n",
    "            # Applying the reset curriculum\n",
    "            i_srchdrwas = (epoch - srch_inittrn) // srch_frq\n",
    "            \n",
    "            if (i_srchdrwas > 0) and (i_srchdrwas % (n_srchdraws // srch_reset_n) == 0):\n",
    "                old_chrg_mu_ = old_chrg_mu.reshape(n_grps, n_cpg, chrg_n, dim)\n",
    "                assert old_chrg_mu_.shape == (n_grps, n_cpg, chrg_n, dim)\n",
    "                \n",
    "                old_logpost_ = old_logpost.reshape(n_grps, n_cpg)\n",
    "                assert old_logpost_.shape == (n_grps, n_cpg)\n",
    "                \n",
    "                top_idx = torch.topk(old_logpost_, srch_reset_k, dim=1, largest=True, sorted=True).indices\n",
    "                assert top_idx.shape == (n_grps, srch_reset_k)\n",
    "                \n",
    "                top_chains = torch.take_along_dim(old_chrg_mu_, top_idx.unsqueeze(-1).unsqueeze(-1), dim=-3)\n",
    "                assert top_chains.shape == (n_grps, srch_reset_k, chrg_n, dim)\n",
    "                \n",
    "                srch_reset_reps = n_cpg // srch_reset_k\n",
    "                old_chrg_mu = top_chains.reshape(n_grps, 1, srch_reset_k, chrg_n, dim)\n",
    "                old_chrg_mu = old_chrg_mu.expand(n_grps, srch_reset_reps, srch_reset_k, chrg_n, dim)\n",
    "                old_chrg_mu = old_chrg_mu.reshape(n_seeds, chrg_n, dim)\n",
    "                assert old_chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "            \n",
    "            if do_logtb:\n",
    "                blm = best_loglike.reshape(n_grps, n_cpg)\n",
    "                tbwriter.add_scalar('search/acceptance', acceptance.float().mean(), epoch)\n",
    "                for q in np.linspace(0, 1, 5):\n",
    "                    tbwriter.add_scalar(f'search/best_loglike/q{int(100*q):02d}', \n",
    "                                        blm.quantile(q, dim=-1).median(), epoch)\n",
    "                tbwriter.add_scalar('search/loglike', old_loglike.mean(), epoch)\n",
    "                tbwriter.add_scalar('search/logpri', old_logpri.mean(), epoch)\n",
    "                tbwriter.add_scalar('search/logpost', old_logpost.mean(), epoch)\n",
    "\n",
    "                if (dim == 2):\n",
    "                    # Finding the best group for scatter plotting\n",
    "                    i_grp = old_loglike.reshape(n_grps, n_cpg).max(dim=-1).values.argmax()\n",
    "                    x_scatt = old_chrg_mu.reshape(n_grps, n_cpg, chrg_n, dim)[i_grp]\n",
    "                    assert x_scatt.shape == (n_cpg, chrg_n, dim)\n",
    "                    x_scatt = x_scatt.reshape(n_cpg * chrg_n, dim)\n",
    "                    assert x_scatt.shape == (n_cpg * chrg_n, dim)\n",
    "                    x_scatt = x_scatt.detach().cpu().numpy()\n",
    "                    assert x_scatt.shape == (n_cpg * chrg_n, dim)\n",
    "\n",
    "                    ax_srch.clear()\n",
    "                    ax_srch.scatter(x_scatt[:, 0], x_scatt[:, 1], s=1)\n",
    "                    \n",
    "                    fig_srch.set_tight_layout(True)\n",
    "                    tbwriter.add_figure(f'viz/srch/mu', fig_srch, epoch)\n",
    "                \n",
    "                tbwriter.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2721fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    for epoch in range(n_epochs+1):\n",
    "        if srch_dstr is not None:\n",
    "            update_problem = (epoch >= srch_inittrn) and ((epoch - srch_inittrn) % srch_frq == 0)\n",
    "            if update_problem:\n",
    "                if srch_dstr in ('mcmc',):\n",
    "                    prpsd_chrg_mu = old_chrg_mu + srch_sigma * rng.normal((n_seeds, chrg_n, dim))\n",
    "                    assert prpsd_chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "                    \n",
    "                    chrg_mu_tch = prpsd_chrg_mu\n",
    "                    chrg_mu = chrg_mu_tch.detach().cpu().numpy()\n",
    "                    assert chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "                    problem.locations = chrg_mu\n",
    "                    problem.locations_tch = chrg_mu_tch\n",
    "                else:\n",
    "                    raise ValueError(f'srch_dstr={srch_dstr} not implemented')\n",
    "                \n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Sampling the volumes\n",
    "        volsamps = volsampler(n=n_srf)\n",
    "\n",
    "        # Sampling the points from the srferes\n",
    "        srfsamps = srfsampler(volsamps, n_points, do_detspacing=do_detspacing)\n",
    "        points = nn.Parameter(srfsamps['points'])\n",
    "        surfacenorms = srfsamps['normals']\n",
    "        areas = srfsamps['areas']\n",
    "        assert points.shape == (n_seeds, n_srf, n_points, dim)\n",
    "        assert surfacenorms.shape == (n_seeds, n_srf, n_points, dim)\n",
    "        assert areas.shape == (n_seeds, n_srf,)\n",
    "\n",
    "        points_mdl = points[:, :, :n_srfpts_mdl, :]\n",
    "        assert points_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "        points_trg = points[:, :, n_srfpts_mdl:, :]\n",
    "        assert points_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "        surfacenorms_mdl = surfacenorms[:, :, :n_srfpts_mdl, :]\n",
    "        assert surfacenorms_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "        surfacenorms_trg = surfacenorms[:, :, n_srfpts_mdl:, :]\n",
    "        assert surfacenorms_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "        # Making surface integral predictions using the reference model\n",
    "        u_mdl = model(points_mdl)\n",
    "        assert u_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, 1)\n",
    "        nabla_x_u_mdl, = torch.autograd.grad(u_mdl.sum(), [points_mdl],\n",
    "            grad_outputs=None, retain_graph=True, create_graph=True,\n",
    "            only_inputs=True, allow_unused=False)\n",
    "        assert nabla_x_u_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "        normprods_mdl = (nabla_x_u_mdl * surfacenorms_mdl).sum(dim=-1)\n",
    "        assert normprods_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl)\n",
    "        if n_srfpts_mdl > 0:\n",
    "            mean_normprods_mdl = normprods_mdl.mean(dim=-1, keepdim=True)\n",
    "            assert mean_normprods_mdl.shape == (n_seeds, n_srf, 1)\n",
    "        else:\n",
    "            mean_normprods_mdl = 0.0\n",
    "\n",
    "        # Making surface integral predictions using the target model\n",
    "        u_trg = target(points_trg)\n",
    "        assert u_trg.shape == (n_seeds, n_srf, n_srfpts_trg, 1)\n",
    "        nabla_x_u_trg, = torch.autograd.grad(u_trg.sum(), [points_trg],\n",
    "            grad_outputs=None, retain_graph=True, create_graph=not(do_bootstrap),\n",
    "            only_inputs=True, allow_unused=False)\n",
    "        assert nabla_x_u_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "        normprods_trg = (nabla_x_u_trg * surfacenorms_trg).sum(dim=-1)\n",
    "        assert normprods_trg.shape == (n_seeds, n_srf, n_srfpts_trg)\n",
    "        if do_dblsampling:\n",
    "            assert n_rsdls == 2\n",
    "\n",
    "            mean_normprods_trg1 = normprods_trg[..., 0::2].mean(\n",
    "                dim=-1, keepdim=True)\n",
    "            assert mean_normprods_trg1.shape == (n_seeds, n_srf, 1)\n",
    "\n",
    "            mean_normprods_trg2 = normprods_trg[..., 1::2].mean(\n",
    "                dim=-1, keepdim=True)\n",
    "            assert mean_normprods_trg2.shape == (n_seeds, n_srf, 1)\n",
    "\n",
    "            mean_normprods_trg = torch.cat(\n",
    "                [mean_normprods_trg1, mean_normprods_trg2], dim=-1)\n",
    "            assert mean_normprods_trg.shape == (n_seeds, n_srf, n_rsdls)\n",
    "        else:\n",
    "            assert n_rsdls == 1\n",
    "\n",
    "            mean_normprods_trg = normprods_trg.mean(dim=-1, keepdim=True)\n",
    "            assert mean_normprods_trg.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Linearly combining the reference and target predictions\n",
    "        mean_normprods = (       w_trg  * mean_normprods_trg +\n",
    "                          (1.0 - w_trg) * mean_normprods_mdl)\n",
    "        assert mean_normprods.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Considering the surface areas\n",
    "        pred_surfintegs = mean_normprods * areas.reshape(n_seeds, n_srf, 1)\n",
    "        assert pred_surfintegs.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Getting the reference volume integrals\n",
    "        ref_volintegs = problem.integrate_volumes(volsamps)\n",
    "        assert ref_volintegs.shape == (n_seeds, n_srf)\n",
    "\n",
    "        # Getting the residual terms\n",
    "        resterms = pred_surfintegs - ref_volintegs.reshape(n_seeds, n_srf, 1)\n",
    "        assert resterms.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Multiplying the residual terms\n",
    "        if do_dblsampling:\n",
    "            resterms_prod = resterms.prod(dim=-1)\n",
    "            assert resterms_prod.shape == (n_seeds, n_srf)\n",
    "        else:\n",
    "            resterms_prod = torch.square(resterms).squeeze(-1)\n",
    "            assert resterms_prod.shape == (n_seeds, n_srf)\n",
    "\n",
    "        # Computing the main loss\n",
    "        loss_main = resterms_prod.mean(-1)\n",
    "        assert loss_main.shape == (n_seeds,)\n",
    "\n",
    "        if do_bootstrap:\n",
    "            with torch.no_grad():\n",
    "                u_mdl_prime = target(points_mdl)\n",
    "            loss_trgreg = torch.square(u_mdl - u_mdl_prime).mean([-3, -2, -1])\n",
    "            assert loss_trgreg.shape == (n_seeds,)\n",
    "        else:\n",
    "            loss_trgreg = torch.zeros(n_seeds, device=tch_device, dtype=tch_dtype)\n",
    "            assert loss_trgreg.shape == (n_seeds,)\n",
    "\n",
    "        # The initial condition loss\n",
    "        renew_ic = (epoch == 0) if (ic_frq == 0) else (epoch % ic_frq == 0)\n",
    "        if ic_needsampling and renew_ic and (ic_dstr is not None):\n",
    "            with torch.no_grad():\n",
    "                if ic_dstr == 'sphere':\n",
    "                    ic_normsamps =rng.normal((n_seeds, ic_n, dim))\n",
    "                    assert ic_normsamps.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_ptstilde = ic_normsamps / ic_normsamps.norm()\n",
    "                    assert ic_ptstilde.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    assert ic_c.shape == (n_seeds, ic_n, dim)\n",
    "                    assert ic_r.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    ic_allpnts = ic_c + ic_ptstilde * ic_r\n",
    "                    assert ic_allpnts.shape == (n_seeds, ic_n, dim)\n",
    "                elif ic_dstr == 'trnvol':\n",
    "                    icvols = volsampler(n=ic_n)\n",
    "                    assert icvols['type'] == 'ball'\n",
    "\n",
    "                    ic_c = icvols['centers']\n",
    "                    assert ic_c.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_r_ = icvols['radii']\n",
    "                    assert ic_r_.shape == (n_seeds, ic_n)\n",
    "\n",
    "                    ic_r = ic_r_.unsqueeze(dim=-1)\n",
    "                    assert ic_r.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    untrd = rng.uniform((n_seeds, ic_n, 1))\n",
    "                    assert untrd.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    untr = untrd.pow(1.0 / dim)\n",
    "                    assert untr.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    ic_pntrs = untr * ic_r\n",
    "                    assert ic_pntrs.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    ic_theta = rng.normal((n_seeds, ic_n, dim))\n",
    "                    assert ic_theta.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_thtilde = ic_theta / ic_theta.norm(dim=-1, keepdim=True)\n",
    "                    assert ic_thtilde.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_allpnts = ic_c + ic_thtilde * ic_pntrs\n",
    "                    assert ic_allpnts.shape == (n_seeds, ic_n, dim)\n",
    "                else:\n",
    "                    raise ValueError(f'ic/dstr={ic_dstr} not defined')\n",
    "\n",
    "                ic_allgtvs = get_prob_sol(problem, ic_allpnts, eval_bs, \n",
    "                    get_field=False, out_lib='torch')['v']\n",
    "                assert ic_allgtvs.shape == (n_seeds, ic_n)\n",
    "\n",
    "        if ic_needsampling:\n",
    "            ic_idxs = ((np.arange(ic_bs) + epoch * ic_bs) % ic_n).tolist()\n",
    "\n",
    "            ic_pnts = ic_allpnts[:, ic_idxs, :]\n",
    "            assert ic_pnts.shape == (n_seeds, ic_bs, dim)\n",
    "\n",
    "            ic_vpreds = model(ic_pnts).squeeze(dim=-1)\n",
    "            assert ic_vpreds.shape == (n_seeds, ic_bs)\n",
    "\n",
    "            ic_gtvs = ic_allgtvs[:, ic_idxs]\n",
    "            assert ic_gtvs.shape == (n_seeds, ic_bs)\n",
    "        elif (ic_dstr is not None):\n",
    "            ic_pnts = points_mdl.reshape(n_seeds, n_srf * n_srfpts_mdl, dim)\n",
    "            assert ic_pnts.shape == (n_seeds, ic_bs, dim)\n",
    "\n",
    "            ic_vpreds = u_mdl.reshape(n_seeds, n_srf * n_srfpts_mdl)\n",
    "            assert ic_vpreds.shape == (n_seeds, ic_bs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ic_gtvs = get_prob_sol(problem, ic_pnts, eval_bs, \n",
    "                    get_field=False, out_lib='torch')['v']\n",
    "            assert ic_gtvs.shape == (n_seeds, ic_bs)\n",
    "\n",
    "        if ic_bpp == 'bias':\n",
    "            mdl_bias = model.layer_last[1].squeeze(dim=-1)\n",
    "            assert mdl_bias.shape == (n_seeds, 1)\n",
    "\n",
    "            ic_vpreds = ic_vpreds.detach() - mdl_bias.detach() + mdl_bias\n",
    "            assert ic_vpreds.shape == (n_seeds, ic_bs)\n",
    "        elif ic_bpp == 'all':\n",
    "            pass\n",
    "        else:\n",
    "            raise RuntimeError(f'ic/bpp={ic_bpp} not defined')\n",
    "\n",
    "        if ic_dstr is not None:\n",
    "            loss_ic = torch.square(ic_vpreds - ic_gtvs).mean(dim=-1)\n",
    "            assert loss_ic.shape == (n_seeds,)\n",
    "        else:\n",
    "            loss_ic = torch.zeros(n_seeds, dtype=tch_dtype, device=tch_device)\n",
    "            assert loss_ic.shape == (n_seeds,)\n",
    "\n",
    "        # The total loss\n",
    "        loss = loss_main + w_trgreg * loss_trgreg + w_ic * loss_ic\n",
    "        assert loss.shape == (n_seeds,)\n",
    "\n",
    "        loss_sum = loss.sum()\n",
    "        loss_sum.backward()\n",
    "\n",
    "        # We will not update in the first epoch so that we will \n",
    "        # record the initialization statistics as well. Instead, \n",
    "        # we will update an extra epoch at the end.\n",
    "        if (epoch > 0):\n",
    "            opt.step()\n",
    "\n",
    "        # Updating the target network\n",
    "        if do_bootstrap and (epoch > 0):\n",
    "            model_sd = model.state_dict()\n",
    "            target_sd = target.state_dict()\n",
    "            newtrg_sd = dict()\n",
    "            with torch.no_grad():\n",
    "                for key, param in model_sd.items():\n",
    "                    param_trg = target_sd[key]\n",
    "                    newtrg_sd[key] = tau * param_trg + (1-tau) * param\n",
    "            target.load_state_dict(newtrg_sd)\n",
    "\n",
    "        # computing the normal product variances\n",
    "        with torch.no_grad(): \n",
    "            normprods = torch.cat([normprods_mdl, normprods_trg], dim=-1)\n",
    "            npvm = (normprods.var(dim=-1)*areas.square()).mean(-1)\n",
    "\n",
    "        # evaluating the performance of the model and target    \n",
    "        perf_dict = dict()\n",
    "        eval_strg = dict()\n",
    "        for eid, eopts in evalprms.items():\n",
    "            edstr = eopts['dstr']\n",
    "            n_evlpnts = eopts['n']\n",
    "            e_frq = eopts['frq']\n",
    "            e_store = eopts['store']\n",
    "\n",
    "            if (epoch % e_frq) > 0:\n",
    "                assert eid in last_perfdict\n",
    "                perf_dict[eid] = last_perfdict[eid]\n",
    "                continue\n",
    "\n",
    "            # Sampling the evaluation points\n",
    "            with torch.no_grad():\n",
    "                if edstr == 'uniform':\n",
    "                    e_bias = eopts['bias']\n",
    "                    assert e_bias.shape == (n_seeds, 1, dim)\n",
    "\n",
    "                    e_slope = eopts['slope']\n",
    "                    assert e_slope.shape == (n_seeds, 1, dim)\n",
    "\n",
    "                    e_unfpnts = erng.uniform((n_seeds, n_evlpnts, dim))\n",
    "                    assert e_unfpnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                    e_pnts = e_bias + e_unfpnts * e_slope\n",
    "                    assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "                elif edstr in ('ball', 'trnvol'):\n",
    "                    if edstr == 'ball':\n",
    "                        e_c = eopts['c_xpnd']\n",
    "                        assert e_c.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                        e_r = eopts['r_xpnd']\n",
    "                        assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "                    elif edstr == 'trnvol':\n",
    "                        evols = volsampler(n=n_evlpnts)\n",
    "                        assert evols['type'] == 'ball'\n",
    "\n",
    "                        e_c = evols['centers']\n",
    "                        assert e_c.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                        e_r_ = evols['radii']\n",
    "                        assert e_r_.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "                        e_r = e_r_.unsqueeze(dim=-1)\n",
    "                        assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "                    else:\n",
    "                        raise RuntimeError(f'case not defined')\n",
    "\n",
    "                    untrd = erng.uniform((n_seeds, n_evlpnts, 1))\n",
    "                    assert untrd.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                    untr = untrd.pow(1.0 / dim)\n",
    "                    assert untr.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                    e_pntrs = untr * e_r\n",
    "                    assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                    etheta = erng.normal((n_seeds, n_evlpnts, dim))\n",
    "                    assert etheta.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                    ethtilde = etheta / etheta.norm(dim=-1, keepdim=True)\n",
    "                    assert ethtilde.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                    e_pnts = e_c + ethtilde * e_pntrs\n",
    "                    assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "                elif edstr in ('grid'):\n",
    "                    e_pnts = eopts['pnts']\n",
    "                    assert e_pnts.shape == (n_seeds, n_g, dim)\n",
    "                else:\n",
    "                    raise RuntimeError(f'eval dstr \"{edstr}\" not implmntd')\n",
    "\n",
    "            # Computing the model, target and ground truth solutions\n",
    "            prob_sol = get_prob_sol(problem, e_pnts, n_eval=eval_bs, \n",
    "                get_field=False, out_lib='torch')\n",
    "\n",
    "            e_prbsol = prob_sol['v']\n",
    "            assert e_prbsol.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "            # Computing the model solution\n",
    "            with torch.no_grad():\n",
    "                mdl_sol = get_nn_sol(model, e_pnts, n_eval=eval_bs,\n",
    "                    get_field=False, out_lib='torch')\n",
    "\n",
    "            e_mdlsol = mdl_sol['v']\n",
    "            assert e_mdlsol.shape == (n_seeds, n_evlpnts)\n",
    "            \n",
    "            # Computing the target solution\n",
    "            if do_bootstrap:\n",
    "                with torch.no_grad():\n",
    "                    trg_sol = get_nn_sol(target, e_pnts, n_eval=eval_bs, \n",
    "                        get_field=False, out_lib='torch')\n",
    "\n",
    "                e_trgsol = trg_sol['v']\n",
    "                assert e_trgsol.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "            eperfs = dict()\n",
    "            eperfs['mdl'] = get_perfdict(e_pnts, e_mdlsol, e_prbsol)\n",
    "            if do_bootstrap:\n",
    "                eperfs['trg'] = get_perfdict(e_pnts, e_trgsol, e_prbsol)\n",
    "            eperfs = deep2hie(eperfs, dictcls=dict)\n",
    "            # Example: eperfs = {'mdl/pln/mse': ...,\n",
    "            #                    'mdl/pln/mae': ...,\n",
    "            #                    'mdl/bc/mse': ...,\n",
    "            #                    'mdl/bc/mae': ...,\n",
    "            #                    'mdl/slc/mse': ...,\n",
    "            #                    'mdl/slc/mae': ...,\n",
    "            #                    'trg/pln/mse': ...,\n",
    "            #                    'trg/pln/mae': ...,\n",
    "            #                    'trg/bc/mse': ...,\n",
    "            #                    'trg/bc/mae': ...,\n",
    "            #                    'trg/slc/mse': ...,\n",
    "            #                    'trg/slc/mae': ...,\n",
    "            #                   }\n",
    "            perf_dict[eid] = eperfs\n",
    "            last_perfdict[eid] = eperfs\n",
    "            \n",
    "            if do_logtb:\n",
    "                for kk, vv in eperfs.items():\n",
    "                    tbwriter.add_scalar(f'perf/{eid}/{kk}', vv.mean(), epoch)\n",
    "            \n",
    "            # Storing the evaluation results\n",
    "            if e_store:\n",
    "                e_strg = dict()\n",
    "                e_strg['sol/mdl'] = e_mdlsol\n",
    "                if do_bootstrap:\n",
    "                    e_strg['sol/trg'] = e_trgsol\n",
    "                e_strg['sol/gt'] = e_prbsol\n",
    "                if edstr != 'grid':\n",
    "                    e_strg['pnts'] = e_pnts\n",
    "                e_strg = {kk: vv.detach().cpu().numpy().astype(np.float16) \n",
    "                          for kk, vv in e_strg.items()}\n",
    "                eval_strg[eid] = e_strg\n",
    "\n",
    "                if do_logtb and (edstr == 'grid') and (dim == 2):\n",
    "                    soltd_list = [('mdl', mdl_sol, fig_mdl, ax_mdl, cax_mdl)]\n",
    "                    if do_bootstrap:\n",
    "                        soltd_list += [('trg', trg_sol, fig_trg, ax_trg, cax_trg)]\n",
    "                    soltd_list += [('gt', prob_sol, fig_gt, ax_gt, cax_gt)]\n",
    "                    for sol_t, sol_dict, fig, ax, cax in soltd_list:\n",
    "                        x1_msh_np, x2_msh_np = eopts['xi_msh_np']\n",
    "                        plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=fig, ax=ax, cax=cax)\n",
    "                        fig.set_tight_layout(True)\n",
    "                        tbwriter.add_figure(f'viz/{eid}/{sol_t}', fig, epoch)\n",
    "                    tbwriter.flush()\n",
    "        \n",
    "        # Applying search on the latent problem parameters\n",
    "        update_search = False\n",
    "        if srch_dstr is not None:  \n",
    "            update_search = ((epoch - srch_inittrn) % srch_frq == (srch_frq - 1))\n",
    "            update_search = update_search and (epoch >= srch_inittrn)\n",
    "        \n",
    "        if update_search:\n",
    "            assert srch_dstr == 'mcmc'\n",
    "        \n",
    "            need_field = (obs_y_dstr == 'field')\n",
    "            sol_key = {'potential': 'v', 'field': 'e'}[obs_y_dstr]\n",
    "            \n",
    "            # Computing the problem solution\n",
    "            obs_probsol = get_prob_sol(problem, obs_x, n_eval=eval_bs, \n",
    "                get_field=need_field, out_lib='torch')[sol_key]\n",
    "            if (obs_y_dstr == 'potential'):\n",
    "                obs_probsol = obs_probsol.unsqueeze(-1)\n",
    "                obs_probsol = obs_probsol - obs_probsol.mean(dim=-2, keepdim=True)\n",
    "            assert obs_probsol.shape == (n_seeds, obs_n, obs_ydim)\n",
    "\n",
    "            # Computing the model solution\n",
    "            obs_mdlsol = get_nn_sol(model, obs_x, n_eval=eval_bs,\n",
    "                get_field=need_field, out_lib='torch')[sol_key]\n",
    "            if (obs_y_dstr == 'potential'):\n",
    "                obs_mdlsol = obs_mdlsol.unsqueeze(-1)\n",
    "                obs_mdlsol = obs_mdlsol - obs_mdlsol.mean(dim=-2, keepdim=True)\n",
    "            assert obs_mdlsol.shape == (n_seeds, obs_n, obs_ydim)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                obs_mdlerr = obs_mdlsol - obs_y\n",
    "                assert obs_mdlerr.shape == (n_seeds, obs_n, obs_ydim)\n",
    "                \n",
    "                assert srch_metric_dstr == 'mse'\n",
    "                \n",
    "                obs_loss = obs_mdlerr.square().sum(dim=-1)\n",
    "                assert obs_loss.shape == (n_seeds, obs_n)\n",
    "                    \n",
    "                if (srch_metric_min is not None) or (srch_metric_max is not None):\n",
    "                    obs_lossclp = torch.clip(obs_loss, srch_metric_min, srch_metric_max)\n",
    "                    assert obs_lossclp.shape == (n_seeds, obs_n)\n",
    "                else:\n",
    "                    obs_lossclp = obs_loss\n",
    "                    assert obs_lossclp.shape == (n_seeds, obs_n)\n",
    "                \n",
    "                prpsd_loglike = obs_lossclp.mean(dim=-1) * (-srch_metric_coeff)\n",
    "                assert prpsd_loglike.shape == (n_seeds,)\n",
    "                \n",
    "                prpsd_logpri_ = normal_logprob(prpsd_chrg_mu, srch_prior_mu_loc_tch, srch_prior_mu_scale_tch)\n",
    "                assert prpsd_logpri_.shape == (n_seeds, chrg_n)\n",
    "                \n",
    "                prpsd_logpri = prpsd_logpri_.sum(dim=-1)\n",
    "                assert prpsd_logpri.shape == (n_seeds,)\n",
    "                \n",
    "                prpsd_logpost = prpsd_logpri + prpsd_loglike\n",
    "                assert prpsd_logpost.shape == (n_seeds,)\n",
    "                \n",
    "                if old_logpri is None:\n",
    "                    old_logpri = prpsd_logpri\n",
    "                    assert old_logpri.shape == (n_seeds,)\n",
    "                \n",
    "                    old_loglike = prpsd_loglike\n",
    "                    assert old_loglike.shape == (n_seeds,)\n",
    "                    \n",
    "                    old_logpost = prpsd_logpost\n",
    "                    assert old_logpost.shape == (n_seeds,)\n",
    "                    \n",
    "                    best_loglike = old_loglike\n",
    "                    assert best_loglike.shape == (n_seeds,)\n",
    "\n",
    "                delta_logpost = prpsd_logpost - old_logpost\n",
    "                assert delta_logpost.shape == (n_seeds,)\n",
    "                \n",
    "                acceptance = (srch_tmprtr * delta_logpost) > rng.uniform((n_seeds, 1)).log().squeeze(-1)\n",
    "                assert acceptance.shape == (n_seeds,)\n",
    "                \n",
    "                # Updating the monte carlo chains and log-probabilities\n",
    "                old_chrg_mu = torch.where(acceptance.reshape(n_seeds, 1, 1), prpsd_chrg_mu, old_chrg_mu)\n",
    "                assert old_chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "                \n",
    "                old_loglike = torch.where(acceptance, prpsd_loglike, old_loglike)\n",
    "                assert old_loglike.shape == (n_seeds,)\n",
    "                \n",
    "                old_logpri = torch.where(acceptance, prpsd_logpri, old_logpri)\n",
    "                assert old_logpri.shape == (n_seeds,)\n",
    "                \n",
    "                old_logpost =  torch.where(acceptance, prpsd_logpost, old_logpost)\n",
    "                assert old_logpost.shape == (n_seeds,)\n",
    "                \n",
    "                best_loglike = torch.where(old_loglike > best_loglike, old_loglike, best_loglike)\n",
    "                assert best_loglike.shape == (n_seeds,)\n",
    "                \n",
    "                # Applying the reset curriculum\n",
    "                i_srchdrwas = (epoch - srch_inittrn) // srch_frq\n",
    "                \n",
    "                if (i_srchdrwas % (n_srchdraws // srch_reset_n) == 0):\n",
    "                    old_chrg_mu_ = old_chrg_mu.reshape(n_grps, n_cpg, chrg_n, dim)\n",
    "                    assert old_chrg_mu_.shape == (n_grps, n_cpg, chrg_n, dim)\n",
    "                    \n",
    "                    old_logpost_ = old_logpost.reshape(n_grps, n_cpg)\n",
    "                    assert old_logpost_.shape == (n_grps, n_cpg)\n",
    "                    \n",
    "                    top_idx = torch.topk(old_logpost_, srch_reset_k, dim=1, largest=True, sorted=True).indices\n",
    "                    assert top_idx.shape == (n_grps, srch_reset_k)\n",
    "                    \n",
    "                    top_chains = torch.take_along_dim(old_chrg_mu_, top_idx.unsqueeze(-1).unsqueeze(-1), dim=-3)\n",
    "                    assert top_chains.shape == (n_grps, srch_reset_k, chrg_n, dim)\n",
    "                    \n",
    "                    # resetting the mu\n",
    "                    srch_reset_reps = n_cpg // srch_reset_k\n",
    "                    old_chrg_mu = top_chains.reshape(n_grps, 1, srch_reset_k, chrg_n, dim)\n",
    "                    old_chrg_mu = old_chrg_mu.expand(n_grps, srch_reset_reps, srch_reset_k, chrg_n, dim)\n",
    "                    old_chrg_mu = old_chrg_mu.reshape(n_seeds, chrg_n, dim)\n",
    "                    assert old_chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "                    \n",
    "                    # resetting the models\n",
    "                    mdlrepsd = {name: replicate_top(param, top_idx) \n",
    "                                for name, param in model.state_dict().items()}\n",
    "                    model.load_state_dict(mdlrepsd)\n",
    "\n",
    "                    if do_bootstrap:\n",
    "                        trgrepsd = {name: replicate_top(param, top_idx) \n",
    "                            for name, param in target.state_dict().items()}\n",
    "                        target.load_state_dict(trgrepsd)\n",
    "                    \n",
    "                    # resetting the optimizer\n",
    "                    if opt_type == 'sgd':\n",
    "                        pass\n",
    "                    elif opt_type == 'adam':\n",
    "                        optrepsd = deepcopy(opt.state_dict())\n",
    "                        for kk, vv in optrepsd['state'].items():\n",
    "                            vv['exp_avg'] = replicate_top(vv['exp_avg'], top_idx)\n",
    "                            vv['exp_avg_sq'] = replicate_top(vv['exp_avg_sq'], top_idx)\n",
    "                        opt.load_state_dict(optrepsd)\n",
    "                    else:\n",
    "                        raise ValueError(f'opt_type={opt_type} not defined')\n",
    "                \n",
    "                if do_logtb:\n",
    "                    blm = best_loglike.reshape(n_grps, n_cpg)\n",
    "                    tbwriter.add_scalar('search/acceptance', acceptance.float().mean(), epoch)\n",
    "                    for q in np.linspace(0, 1, 5):\n",
    "                        tbwriter.add_scalar(f'search/best_loglike/q{int(100*q):02d}', \n",
    "                                            blm.quantile(q, dim=-1).median(), epoch)\n",
    "                    tbwriter.add_scalar('search/loglike', old_loglike.mean(), epoch)\n",
    "                    tbwriter.add_scalar('search/logpri', old_logpri.mean(), epoch)\n",
    "                    tbwriter.add_scalar('search/logpost', old_logpost.mean(), epoch)\n",
    "\n",
    "                    if (dim == 2):\n",
    "                        # Finding the best group for scatter plotting\n",
    "                        i_grp = old_loglike.reshape(n_grps, n_cpg).max(dim=-1).values.argmax()\n",
    "                        x_scatt = old_chrg_mu.reshape(n_grps, n_cpg, chrg_n, dim)[i_grp]\n",
    "                        assert x_scatt.shape == (n_cpg, chrg_n, dim)\n",
    "                        x_scatt = x_scatt.reshape(n_cpg * chrg_n, dim)\n",
    "                        assert x_scatt.shape == (n_cpg * chrg_n, dim)\n",
    "                        x_scatt = x_scatt.detach().cpu().numpy()\n",
    "                        assert x_scatt.shape == (n_cpg * chrg_n, dim)\n",
    "\n",
    "                        ax_srch.clear()\n",
    "                        ax_srch.scatter(x_scatt[:, 0], x_scatt[:, 1], s=1)\n",
    "                        \n",
    "                        fig_srch.set_tight_layout(True)\n",
    "                        tbwriter.add_figure(f'viz/srch/mu', fig_srch, epoch)\n",
    "                    \n",
    "                    tbwriter.flush()\n",
    "  \n",
    "        # monitoring the resource utilization \n",
    "        if epoch % iomon_period == 0:\n",
    "            s_rsrc = resource.getrusage(resource.RUSAGE_SELF)\n",
    "            c_rsrc = resource.getrusage(resource.RUSAGE_CHILDREN)\n",
    "            \n",
    "            psmem = psutil.virtual_memory()\n",
    "            pscpu = psutil.cpu_times()\n",
    "            pscpuload = psutil.getloadavg()\n",
    "            mon_dict = {'cpu/mem/tot': [psmem.total] * n_seeds, \n",
    "                'cpu/mem/avail': [psmem.available] * n_seeds, \n",
    "                'cpu/mem/used': [psmem.used] * n_seeds,\n",
    "                'cpu/mem/free': [psmem.free] * n_seeds,\n",
    "                'cpu/time/user/ps': [pscpu.user] * n_seeds,\n",
    "                'cpu/time/sys/ps': [pscpu.system] * n_seeds,\n",
    "                'cpu/time/idle/ps': [pscpu.idle] * n_seeds,\n",
    "                'cpu/load/1m': [pscpuload[0]] * n_seeds,\n",
    "                'cpu/load/5m': [pscpuload[1]] * n_seeds,\n",
    "                'cpu/load/15m': [pscpuload[2]] * n_seeds,\n",
    "                'cpu/time/train': [time.time()   - trn_sttime] * n_seeds,\n",
    "                'cpu/time/sys/py':   [s_rsrc.ru_stime  + c_rsrc.ru_stime] * n_seeds,\n",
    "                'cpu/time/user/py':  [s_rsrc.ru_utime  + c_rsrc.ru_utime] * n_seeds,\n",
    "                'n_seeds': [n_seeds] * n_seeds}\n",
    "            if 'cuda' in device_name:\n",
    "                t_gpumem = torch.cuda.get_device_properties(tch_device).total_memory\n",
    "                r_gpumem = torch.cuda.memory_reserved(tch_device)\n",
    "                a_gpumem = torch.cuda.memory_allocated(tch_device)\n",
    "                f_gpumem = r_gpumem - a_gpumem\n",
    "                mon_dict.update({'gpu/mem/tot':   [t_gpumem] * n_seeds,\n",
    "                                 'gpu/mem/res':   [r_gpumem] * n_seeds,\n",
    "                                 'gpu/mem/alloc': [a_gpumem] * n_seeds,\n",
    "                                 'gpu/mem/free':  [f_gpumem] * n_seeds})\n",
    "\n",
    "        # pushing the results to the data writer\n",
    "        psld = deep2hie({'perf': perf_dict}, odict)\n",
    "        slst = [('loss/total',  loss.tolist()),\n",
    "                ('loss/main',   loss_main.tolist()),\n",
    "                ('loss/trgreg', loss_trgreg.tolist()),\n",
    "                ('loss/ic',     loss_ic.tolist()),\n",
    "                ('npvm',        npvm.tolist()),\n",
    "                *list(psld.items())]\n",
    "        stat_dict = odict(slst)\n",
    "        for stat_name, stat_vals in stat_dict.items():\n",
    "            avg_history.setdefault(stat_name, [])\n",
    "            avg_history[stat_name].append(stat_vals)\n",
    "\n",
    "        dtups = []\n",
    "        if epoch % io_avgfrq == 0:\n",
    "            avg_statlst  = [('epoch',       [epoch] * n_seeds),\n",
    "                            ('rng_seed',    rng_seeds.tolist())]\n",
    "            avg_statlst += [(name, np.stack(svl, axis=0).mean(axis=0).tolist())\n",
    "                             for name, svl in avg_history.items()]\n",
    "            avg_statdict = odict(avg_statlst)\n",
    "\n",
    "            dtups += [('hp',    hp_dict,      'pd.cat'),\n",
    "                      ('stat',  avg_statdict, 'pd.qnt'),\n",
    "                      ('etc',   etc_dict,     'pd.cat')]\n",
    "            avg_history = odict()\n",
    "            \n",
    "        for eid, e_strg in eval_strg.items():\n",
    "            msg_ =  f'eval/{eid} requires storage, thus \"eval/{eid}/frq\" '\n",
    "            msg_ += f'% \"io/avg/frq\" == 0 should hold.'\n",
    "            assert epoch % io_avgfrq == 0, msg_\n",
    "            dtups += [(f'var/eval/{eid}', e_strg, 'np.arr')]\n",
    "        \n",
    "        if epoch % iomon_period == 0:\n",
    "            assert epoch % io_avgfrq == 0\n",
    "            dtups += [('mon', mon_dict, 'pd.qnt')]\n",
    "\n",
    "        if epoch % chkpnt_period == 0:\n",
    "            assert epoch % io_avgfrq == 0\n",
    "            mdl_sdnp = {k: v.detach().cpu().numpy() \n",
    "                for k, v in model.state_dict().items()}\n",
    "            dtups += [('mdl',   mdl_sdnp, 'np.arr')]\n",
    "            if do_bootstrap:\n",
    "                trg_sdnp = {k: v.detach().cpu().numpy() \n",
    "                    for k, v in target.state_dict().items()}\n",
    "                dtups += [('trg',   trg_sdnp, 'np.arr')]\n",
    "                \n",
    "        if update_search:\n",
    "            srch_strg = {'prpsd/mu': chrg_mu_tch, 'prpsd/loglike': prpsd_loglike, \n",
    "                         'prpsd/logpri': prpsd_logpri, 'prpsd/logpost': prpsd_logpost,\n",
    "                         'old/mu': old_chrg_mu, 'old/loglike': old_loglike,\n",
    "                         'old/logpri': old_logpri, 'old/logpost': old_logpost,\n",
    "                         'acceptance': acceptance}\n",
    "            srch_strg = {k: v.detach().cpu().numpy() for k,v in srch_strg.items()}\n",
    "            dtups += [(f'var/srch', srch_strg, 'np.arr')]\n",
    "\n",
    "        dwriter.add(data_tups=dtups, file_path=hdfpth)\n",
    "\n",
    "        # Computing the loss moving averages\n",
    "        loss_ema_mean, loss_ema_std_mean = ema('loss', loss)\n",
    "        npvm_ema_mean, npvm_ema_std_mean = ema('npvm', npvm)\n",
    "        if (epoch % 1000 == 0) and (results_dir is not None):\n",
    "            print_str = f'Epoch {epoch}, EMA loss = {loss_ema_mean:.4f}'\n",
    "            print_str += f' +/- {2*loss_ema_std_mean:.4f}'\n",
    "            print_str += f', EMA Field-Norm Product Variance = {npvm_ema_mean:.4f}'\n",
    "            print_str += f' +/- {2*npvm_ema_std_mean:.4f} ({time.time()-trn_sttime:0.1f} s)'\n",
    "            print(print_str, flush=True)\n",
    "\n",
    "        if do_logtb:\n",
    "            import logging\n",
    "            logging.getLogger(\"tensorboardX.x2num\").setLevel(logging.CRITICAL)  \n",
    "            tbwriter.add_scalar('loss/total', loss.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/main', loss_main.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/trgreg', loss_trgreg.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/ic', loss_ic.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/npvm', npvm.mean(), epoch)\n",
    "\n",
    "        if do_tchsave and (epoch % chkpnt_period == 0):\n",
    "            model_history[epoch] = deepcopy({k: v.cpu() for k, v\n",
    "                in model.state_dict().items()})\n",
    "            target_history[epoch] = deepcopy({k: v.cpu() for k, v\n",
    "                in target.state_dict().items()})\n",
    "            \n",
    "\n",
    "    if results_dir is not None:\n",
    "        print(f'Training finished in {time.time() - trn_sttime:.1f} seconds.')\n",
    "    dwriter.close()\n",
    "    if do_logtb:\n",
    "        tbwriter.flush()\n",
    "    \n",
    "    outdict = dict()\n",
    "    tchmemusage = profmem()\n",
    "    assert str(tch_device) in tchmemusage\n",
    "    if 'cuda' in device_name:\n",
    "        tch_dvcmem = torch.cuda.get_device_properties(tch_device).total_memory\n",
    "    else:\n",
    "        tch_dvcmem = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
    "    outdict['dvc/mem/alloc'] = tchmemusage[str(tch_device)]\n",
    "    outdict['dvc/mem/total'] = tch_dvcmem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig = None\n",
    "    has_grid = any(eopts.get('dstr', '') == 'grid' \n",
    "                   for eid, eopts in evalprms.items())\n",
    "    if (storage_dir is not None) and has_grid:\n",
    "        soltd_list = [('mdl', mdl_sol, fig_mdl, ax_mdl, cax_mdl)]\n",
    "        eopts = list(eopts for eid, eopts in evalprms.items()\n",
    "                     if eopts.get('dstr', '') == 'grid')[0]\n",
    "        e_pnts = eopts['pnts']\n",
    "        x1_msh_np, x2_msh_np = eopts['xi_msh_np']\n",
    "\n",
    "        n_rows, n_cols = 1, 2 + do_bootstrap\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(\n",
    "            n_cols * 3.5, n_rows * 3), dpi=72)\n",
    "        cax = None\n",
    "\n",
    "        # Computing the model, target and ground truth solutions\n",
    "        prob_sol = get_prob_sol(problem, e_pnts, n_eval=eval_bs, get_field=False)\n",
    "        with torch.no_grad():\n",
    "            mdl_sol = get_nn_sol(model, e_pnts, n_eval=eval_bs, get_field=False) \n",
    "            if do_bootstrap:\n",
    "                trg_sol = get_nn_sol(target, e_pnts, n_eval=eval_bs, get_field=False)\n",
    "\n",
    "        soltd_list = [('gt', prob_sol, axes[0], 'Ground Truth'),\n",
    "                      ('mdl', mdl_sol, axes[1], 'Prediction')]\n",
    "        if do_bootstrap:\n",
    "            soltd_list += [('trg', trg_sol, axes[2], 'Target')]\n",
    "        for sol_t, sol_dict, ax, ttl in soltd_list:\n",
    "            plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=fig, ax=ax, cax=cax)\n",
    "            ax.set_title(ttl)\n",
    "    fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b71ad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    if do_tchsave:\n",
    "        torch.save(model_history, f'{cfgstrg_dir}/ckpt_mdl.pt')\n",
    "        if do_bootstrap:\n",
    "            torch.save(target_history, f'{cfgstrg_dir}/ckpt_trg.pt')\n",
    "    if storage_dir is not None:\n",
    "        shutil.copy2(hdfpth, f'{cfgstrg_dir}/progress.h5')\n",
    "        if fig is not None:\n",
    "            fig.savefig(f'{cfgstrg_dir}/finalpred.pdf', dpi=144, bbox_inches=\"tight\")   \n",
    "    if do_profile:\n",
    "        profiler.stop()\n",
    "        html = profiler.output_html()\n",
    "        htmlpath = f'{cfgstrg_dir}/profiler.html'\n",
    "        with open(htmlpath, 'w') as fp:\n",
    "            fp.write(html.encode('ascii', errors='ignore').decode('ascii'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87913cc3",
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "source": [
    "    return outdict\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    use_argparse = True\n",
    "    if use_argparse:\n",
    "        import argparse\n",
    "        my_parser = argparse.ArgumentParser()\n",
    "        my_parser.add_argument('-c', '--configid', action='store', type=str, required=True)\n",
    "        my_parser.add_argument('-d', '--device',   action='store', type=str, required=True)\n",
    "        my_parser.add_argument('-s', '--nodesize', action='store', type=int, default=1)\n",
    "        my_parser.add_argument('-r', '--noderank', action='store', type=int, default=0)\n",
    "        my_parser.add_argument('-i', '--rsmindex', action='store', type=str, default=\"0.0\")\n",
    "        my_parser.add_argument('--dry-run', action='store_true')\n",
    "        args = my_parser.parse_args()\n",
    "        args_configid = args.configid\n",
    "        args_device_name = args.device\n",
    "        args_nodesize = args.nodesize\n",
    "        args_noderank = args.noderank\n",
    "        arsg_rsmindex = args.rsmindex\n",
    "        args_dryrun = args.dry_run\n",
    "    else:\n",
    "        args_configid = 'lvl1/lvl2/poiss2d'\n",
    "        args_device_name = 'cuda:0'\n",
    "        args_nodesize = 1\n",
    "        args_noderank = 0\n",
    "        arsg_rsmindex = 0\n",
    "        args_dryrun = True\n",
    "\n",
    "    assert args_noderank < args_nodesize\n",
    "    cfgidsplit = args_configid.split('/')\n",
    "    # Example: args_configid == 'lvl1/lvl2/poiss2d'\n",
    "    config_id = cfgidsplit[-1]\n",
    "    # Example: config_id == 'poiss2d'\n",
    "    config_tree = '/'.join(cfgidsplit[:-1])\n",
    "    # Example: config_tree == 'lvl1/lvl2'\n",
    "\n",
    "    import os\n",
    "    os.makedirs(configs_dir, exist_ok=True)\n",
    "    # Example: configs_dir == '.../code_bspinn/config'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    # Example: results_dir == '.../code_bspinn/result'\n",
    "    # os.makedirs(storage_dir, exist_ok=True)\n",
    "    # Example: storage_dir == '.../code_bspinn/storage'\n",
    "    \n",
    "    if args_dryrun:\n",
    "        print('>> Running in dry-run mode', flush=True)\n",
    "\n",
    "    cfg_path = f'{configs_dir}/{config_tree}/{config_id}.json'\n",
    "    print(f'>> Reading configuration from {cfg_path}', flush=True)\n",
    "    with open(cfg_path) as fp:\n",
    "        json_cfgdict = json.load(fp, object_pairs_hook=odict)\n",
    "    \n",
    "    if args_dryrun:\n",
    "        import tempfile\n",
    "        temp_resdir = tempfile.TemporaryDirectory()\n",
    "        temp_strdir = tempfile.TemporaryDirectory()\n",
    "        print(f'>> [dry-run] Temporary results dir placed at {temp_resdir.name}')\n",
    "        print(f'>> [dry-run] Temporary storage dir placed at {temp_strdir.name}')\n",
    "        results_dir = temp_resdir.name\n",
    "        storage_dir = temp_strdir.name\n",
    "        \n",
    "        dr_maxfrq = 10\n",
    "        dr_opts = {'opt/epoch': dr_maxfrq, 'io/cmprssn_lvl': 0}\n",
    "        for opt in dr_opts:\n",
    "            assert opt in json_cfgdict\n",
    "            json_cfgdict[opt] = dr_opts[opt]\n",
    "        for opt in fnmatch.filter(json_cfgdict.keys(), '*/frq'):\n",
    "            if json_cfgdict[opt] > dr_maxfrq:\n",
    "                json_cfgdict[opt] = dr_opts[opt] = dr_maxfrq\n",
    "        print(f'>> [dry-run] The following options were made overriden:', flush=True)\n",
    "        for opt, val in dr_opts.items():\n",
    "            print(f'>>           {opt}: {val}', flush=True)\n",
    "            \n",
    "        \n",
    "    nodepstfx = '' if args_nodesize == 1 else f'_{args_noderank:02d}'\n",
    "    # Example: nodepstfx in ('', '_01')\n",
    "    json_cfgdict['io/config_id'] = f'{config_id}{nodepstfx}'\n",
    "    # Example: ans in ('poiss2d', 'poiss2d_01')\n",
    "    json_cfgdict['io/results_dir'] = f'{results_dir}/{config_tree}'\n",
    "    # Example: ans == '.../code_bspinn/result/lvl1/lv2/poiss2d'\n",
    "    json_cfgdict['io/storage_dir'] = None # f'{storage_dir}/{config_tree}'\n",
    "    # Example: ans == '.../code_bspinn/storage/lvl1/lv2/poiss2d'\n",
    "    json_cfgdict['io/tch/device'] = args_device_name\n",
    "    # Example: args_device_name == 'cuda:0'\n",
    "\n",
    "    # Pre-processing and applying the looping processes\n",
    "    all_cfgdicts = preproc_cfgdict(json_cfgdict)\n",
    "    \n",
    "    # Selecting this node's config dict subset\n",
    "    node_cfgdicts = [cfg for i, cfg in enumerate(all_cfgdicts) \n",
    "                     if (i % args_nodesize == args_noderank)]\n",
    "    n_nodecfgs = len(node_cfgdicts)\n",
    "    \n",
    "    # Going over the config dicts one-by-one\n",
    "    rsmidx, rsmprt = tuple(int(x) for x in arsg_rsmindex.split('.'))\n",
    "    for cfgidx, config_dict in enumerate(node_cfgdicts):\n",
    "        if cfgidx < rsmidx:\n",
    "            continue\n",
    "        # Getting a single seed run to estimate the memory usage\n",
    "        tempcfg = config_dict.copy()\n",
    "        tempcfg['io/results_dir'] = None\n",
    "        tempcfg['io/storage_dir'] = None\n",
    "        tempcfg['rng_seed/list'] = [0]\n",
    "        tempcfg['opt/epoch'] = 0\n",
    "        tod = main(tempcfg)\n",
    "        allocmem, totmem = tod['dvc/mem/alloc'], tod['dvc/mem/total']\n",
    "        nsd_max = int(0.5 * totmem / allocmem)\n",
    "        \n",
    "        # Computing how many parts we must split the original config into\n",
    "        cfg_seeds = config_dict['rng_seed/list']\n",
    "        nprts = int(np.ceil(len(cfg_seeds) / nsd_max))\n",
    "        print(f'>> Config index {cfgidx} takes {allocmem/1e6:.1f} ' + \n",
    "              f'MB/seed (out of {totmem/1e9:.1f} GB)', flush=True)\n",
    "        print(f'>> Config index {cfgidx} must be ' + \n",
    "              f'devided into {nprts} parts.', flush=True)\n",
    "        \n",
    "        # Looping over each part of the config\n",
    "        for iprt in range(nprts):\n",
    "            if (cfgidx == rsmidx) and (iprt < rsmprt):\n",
    "                continue\n",
    "            print(f'>>> Started Working on config index {cfgidx}.{iprt}' + \n",
    "                  f' (out of {nprts} parts and {n_nodecfgs} configs).', flush=True)\n",
    "            iprtcfgseeds = cfg_seeds[(iprt*nsd_max):((iprt+1)*nsd_max)]\n",
    "            iprtcfgdict = config_dict.copy()\n",
    "            iprtcfgdict['rng_seed/list'] = iprtcfgseeds\n",
    "            main(iprtcfgdict)\n",
    "            print('-' * 40, flush=True)\n",
    "        print(f'>> Finished working on config index {cfgidx} ' + \n",
    "              f'(out of {n_nodecfgs} configs).', flush=True)\n",
    "        print('='*80, flush=True)\n",
    "        \n",
    "    if args_dryrun:\n",
    "        print(f'>> [dry-run] Cleaning up {temp_resdir.name}')\n",
    "        temp_resdir.cleanup()\n",
    "        print(f'>> [dry-run] Cleaning up {temp_strdir.name}')\n",
    "        temp_strdir.cleanup()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "jupytext": {
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Feb 24 2021, 21:46:12) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4aec7983bc6059d1b5d440a2253fd0eef7d09b7a26ee33cf7f8716a3ef03c04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
