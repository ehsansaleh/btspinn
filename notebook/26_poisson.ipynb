{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e827df9",
   "metadata": {},
   "source": [
    "## The Poisson Problem Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d52d6d",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "%matplotlib inline\n",
    "if importlib.util.find_spec(\"matplotlib_inline\") is not None:\n",
    "    import matplotlib_inline\n",
    "    matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
    "else:\n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    set_matplotlib_formats('retina')\n",
    "\n",
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import socket\n",
    "import random\n",
    "import chaospy\n",
    "import pathlib\n",
    "import fnmatch\n",
    "import datetime\n",
    "import resource\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboardX\n",
    "import psutil\n",
    "from pyinstrument import Profiler\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "from scipy.special import gamma\n",
    "from os.path import exists, isdir\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict as odict\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a370bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bspinn.io_utils import DataWriter\n",
    "from bspinn.io_utils import get_git_commit\n",
    "from bspinn.io_utils import preproc_cfgdict\n",
    "from bspinn.io_utils import hie2deep, deep2hie\n",
    "\n",
    "from bspinn.tch_utils import isscalar\n",
    "from bspinn.tch_utils import EMA\n",
    "from bspinn.tch_utils import BatchRNG\n",
    "from bspinn.tch_utils import bffnn\n",
    "from bspinn.tch_utils import profmem\n",
    "\n",
    "from bspinn.io_cfg import configs_dir\n",
    "from bspinn.io_cfg import results_dir\n",
    "from bspinn.io_cfg import storage_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429fa7a",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Consider the $d$-dimensional space $\\mathbb{R}^{d}$, and the following charge:\n",
    "\n",
    "$$\\rho(x) = \\delta^d(x).$$\n",
    "\n",
    "For $d \\neq 2$, the analytical solution to the system\n",
    "\n",
    "$$\\nabla \\cdot \\vec{E} = \\rho$$\n",
    "\n",
    "$$\\nabla V = \\vec{E}$$\n",
    "\n",
    "can be defined as \n",
    "\n",
    "$$V_{\\vec{x}} = \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\|\\vec{x}\\|^{2-d}, $$\n",
    "\n",
    "$$\\vec{E}_{\\vec{x}} = \\frac{\\Gamma(d/2)}{2\\cdot \\pi^{d/2}\\cdot \\|\\vec{x}\\|^{d}} \\vec{x}.$$\n",
    "\n",
    "For $d=2$, $\\vec{E}_{\\vec{x}}$ is the same, but for $V_{\\vec{x}}$ we have\n",
    "\n",
    "$$V_{\\vec{x}} = \\frac{1}{2\\pi} \\ln(\\|\\vec{x}\\|).$$\n",
    "\n",
    "We want to solve this system using the divergence theorem:\n",
    "\n",
    "$$\\iint_{S_{d-1}(V)} \\vec{E}\\cdot \\hat{n}\\text{ d}S = \\iiint_{V_d} \\nabla.\\vec{E}\\text{ d}V.$$\n",
    "\n",
    "Keep in mind that the $d-1$-dimensional surface of a $d$-dimensional shpere with radius $r$ is \n",
    "$$\\iint_{S_{d-1}(V^{\\text{d-Ball}}_{r})} 1\\text{ d}S = \\frac{2\\cdot \\pi^{d/2}}{\\Gamma(d/2)}\\cdot r^{d-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697fb22",
   "metadata": {},
   "source": [
    "### Dimensionality Scaling\n",
    "\n",
    "We will assume that our domain of solution is a d-Ball centerred at zero with a radius of $r_b$.\n",
    "$$C_1 := \\int_{V_{r_b}^{d\\text{-Ball}}} 1 d\\vec{x} = \\frac{2\\pi^{d/2}}{d\\cdot\\Gamma(d/2)} r_b^d$$\n",
    "\n",
    "#### The Expectation of the Anlytical Solution\n",
    "\n",
    "$$E_v := \\int_{V_r^{d\\text{-Ball}}} V_{\\vec{x}} d\\vec{x} = \\int \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\|\\vec{x}\\|^{2-d} d\\vec{x}$$\n",
    "\n",
    "$$ = C_1 \\cdot \\int \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\|\\vec{x}\\|^{2-d} \\cdot \\frac{1}{C_1} d\\vec{x} $$\n",
    "\n",
    "$$ = C_1 \\cdot \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\int \\|\\vec{x}\\|^{2-d} \\cdot \\frac{1}{C_1} d\\vec{x} $$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\int \\|\\vec{x}\\|^{2-d} \\cdot \\frac{1}{C_1} d\\vec{x} $$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\mathbb{E}_{\\vec{x}} [\\|\\vec{x}\\|^{2-d}] $$\n",
    "\n",
    "By defining the radius of $\\vec{x}$ as $r=\\|\\vec{x}\\|$, the distribution of $r$ is\n",
    "\n",
    "$$Pr(\\|\\vec{x}\\|<r) = (\\frac{r}{r_b})^d$$\n",
    "\n",
    "$$P(\\|\\vec{x}\\|=r) = \\frac{(d-1) \\cdot r^d}{r_b^d}$$\n",
    "\n",
    "Therefore, we have\n",
    "\n",
    "$$E_v = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\mathbb{E}_{\\vec{x}} [r^{2-d}] $$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\int_{r=0}^{r_b} r^{2-d} \\frac{(d-1) \\cdot r^d}{r_b^d} dr$$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\frac{d}{r_b^d} \\int_{r=0}^{r_b} r dr$$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\frac{d}{r_b^d} \\int_{r=0}^{r_b} r dr$$\n",
    "\n",
    "$$ = \\frac{r_b^2}{2\\cdot(2-d)}$$\n",
    "\n",
    "#### The Expectation of the Volume Ratio\n",
    "\n",
    "$$\\mathbb{E}_{r\\sim U[r_l, r_h]}[(\\frac{r}{r_b})^d] = \\frac{1}{r_h - r_l} \\int_{r_l}^{r_h} (\\frac{r}{r_b})^d dr$$\n",
    "\n",
    "$$=\\frac{1}{d+1} \\cdot \\frac{1}{r_b^d} \\frac{r_h^{d+1} - r_l^{d+1}}{r_h - r_l}.$$\n",
    "\n",
    "By setting $r_h=r_b$ and $r_l < r_h$, the above value closes in on $$\\frac{1}{d+1}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b78d8",
   "metadata": {},
   "source": [
    "#### Spherical Coordinates\n",
    "\n",
    "The spherical coordinate system in $d$ dimensions is relates $[x_1, \\cdots, x_d]$ to $[r, \\phi_1, \\phi_2, \\cdots, \\phi_{d-1}]$ such that\n",
    "\n",
    "$$x_1 = r \\cos(\\phi_1),$$\n",
    "$$x_2 = r \\sin(\\phi_1)\\cos(\\phi_2),$$\n",
    "$$x_2 = r \\sin(\\phi_1)\\sin(\\phi_2)\\cos(\\phi_3),$$\n",
    "$$\\ldots$$\n",
    "$$x_{d-1} = r \\sin(\\phi_1)\\sin(\\phi_2)\\cdots \\cos(\\phi_{d-1}),$$\n",
    "$$x_d = r \\sin(\\phi_1)\\sin(\\phi_2)\\cdots \\sin(\\phi_{d-1}).$$\n",
    "\n",
    "We also have \n",
    "$$\\phi_1, \\cdots, \\phi_{d-2} \\in [0, \\pi],$$ \n",
    "$$\\phi_{d-1} \\in [0, 2\\pi).$$\n",
    "\n",
    "The Jacobian determinent is therefore\n",
    "\n",
    "$$\\det([\\frac{\\partial x_i}{\\partial(r, \\phi_j)}]_{i,j}) = r^{d-1} \\sin^{d-2}(\\phi_1) \\sin^{d-3}(\\phi_2) \\cdots \\sin(\\phi_{d-2}).$$\n",
    "\n",
    "In other words, we have\n",
    "\n",
    "$$ \\text{d}^{d}(V) = r^{d-1} \\sin^{d-2}(\\phi_1) \\sin^{d-3}(\\phi_2) \\cdots \\sin(\\phi_{d-2}) \\text{d}r \\text{d}\\phi_1 \\cdots \\text{d} \\phi_{d-1}.$$\n",
    "\n",
    "\n",
    "#### Uniform Sphere Sampling\n",
    "Define $F_n$ such that \n",
    "\n",
    "$$F_n(u) = \\frac{1}{\\int_{0}^{\\pi} \\sin^{n}(\\phi) \\text{d}{\\phi}} \\int_{0}^{\\pi u} \\sin^{n}(\\phi) \\text{d}{\\phi}.$$\n",
    "\n",
    "Note that $F_n(0) = 0$ and $F_n(1)=1$, and that $F_n$ is basically a CDF for a $\\sin^{n}$-like PDF.\n",
    "\n",
    "To sample points uniformly from the sphere of the $d$-dimensional unit ball, here is one process:\n",
    "\n",
    "1. Sample $u = [u_1, u_2, \\cdots, u_{d-1}]$ uniformly from $[0, 1]^{d-1}$.\n",
    "\n",
    "2. Construct an inverse map for $F_n$ where $1 \\leq n \\leq {d-2}$. This can be done using a lookup table and `torch.searchsorted` for instance.\n",
    "\n",
    "3. Compute the following:\n",
    "\n",
    "$$\\phi_1 = \\pi \\cdot F_{d-2}^{-1}(u_1),$$\n",
    "$$\\phi_2 = \\pi \\cdot F_{d-3}^{-1}(u_2),$$\n",
    "$$\\ldots$$\n",
    "$$\\phi_{d-2} = \\pi \\cdot F_{1}^{-1}(u_{d-2}),$$\n",
    "$$\\phi_{d-1} = 2 \\pi \\cdot u_{d-1}.$$\n",
    "\n",
    "4. Translate $[r, \\phi_1, \\phi_2, \\cdots, \\phi_{d-1}]$ to the cartesian system $[x_1, \\cdots, x_d]$.\n",
    "\n",
    "Resources: \n",
    "\n",
    "1. https://en.wikipedia.org/wiki/N-sphere\n",
    "\n",
    "2. http://corysimon.github.io/articles/uniformdistn-on-sphere/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e4bd0",
   "metadata": {},
   "source": [
    "### Defining the Problem and the Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cfae0c",
   "metadata": {
    "code_folding": [
     0,
     15,
     66
    ]
   },
   "outputs": [],
   "source": [
    "class DeltaProblem:\n",
    "    def __init__(self, weights, locations, tch_device, tch_dtype):\n",
    "        # weights          -> np.array -> shape=(n_bch, n_chrg)\n",
    "        # locations.shape  -> np.array -> shape=(n_bch, n_chrg, d)\n",
    "        self.weights = weights\n",
    "        self.locations = locations\n",
    "        self.n_bch, self.n_chrg = self.weights.shape\n",
    "        self.d = self.locations.shape[-1]\n",
    "        assert self.weights.shape == (self.n_bch, self.n_chrg,)\n",
    "        assert self.locations.shape == (self.n_bch, self.n_chrg, self.d)\n",
    "        self.weights_tch = torch.from_numpy(\n",
    "            self.weights).to(tch_device, tch_dtype)\n",
    "        self.locations_tch = torch.from_numpy(\n",
    "            self.locations).to(tch_device, tch_dtype)\n",
    "        self.shape = (self.n_bch,)\n",
    "        self.ndim = 1\n",
    "        self.tch_pi = torch.tensor(np.pi, device=tch_device, dtype=tch_dtype)\n",
    "\n",
    "    def integrate_volumes(self, volumes):\n",
    "        # volumes -> dictionary\n",
    "        assert volumes['type'] == 'ball'\n",
    "        centers = volumes['centers']\n",
    "        radii = volumes['radii']\n",
    "        n_v = radii.shape[-1]\n",
    "        n_bch, n_chrg, d = self.n_bch, self.n_chrg, self.d\n",
    "        assert radii.shape == (n_bch, n_v,)\n",
    "        assert centers.shape == (n_bch, n_v, d)\n",
    "        lib = torch if torch.is_tensor(centers) else np\n",
    "        mu = self.locations_tch if torch.is_tensor(centers) else self.locations\n",
    "        w = self.weights_tch if torch.is_tensor(centers) else self.weights\n",
    "\n",
    "        c_diff_mu = centers.reshape(\n",
    "            n_bch, n_v, 1, d) - mu.reshape(n_bch, 1, n_chrg, d)\n",
    "        assert c_diff_mu.shape == (n_bch, n_v, n_chrg, d)\n",
    "        distl2 = lib.sqrt(lib.square(c_diff_mu).sum(-1))\n",
    "        assert distl2.shape == (n_bch, n_v, n_chrg)\n",
    "        integ = ((distl2 < radii.reshape(n_bch, n_v, 1))\n",
    "                 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "        assert integ.shape == (n_bch, n_v)\n",
    "        return integ\n",
    "\n",
    "    def potential(self, x):\n",
    "        lib = torch if torch.is_tensor(x) else np\n",
    "        lib_pi = self.tch_pi if torch.is_tensor(x) else np.pi\n",
    "        w = self.weights_tch if torch.is_tensor(x) else self.weights\n",
    "        mu = self.locations_tch if torch.is_tensor(x) else self.locations\n",
    "        n_bch, n_chrg, d = self.n_bch, self.n_chrg, self.d\n",
    "        n_x = x.shape[-2]\n",
    "        assert x.shape == (\n",
    "            n_bch, n_x, d), f'x.shape={x.shape}, (n_bch, n_x, d)={(n_bch, n_x, d)}'\n",
    "        x_diff_mu = x.reshape(n_bch, n_x, 1, d) - \\\n",
    "            mu.reshape(self.n_bch, 1, n_chrg, d)\n",
    "        assert x_diff_mu.shape == (n_bch, n_x, n_chrg, d)\n",
    "        x_dists = lib.sqrt(lib.square(x_diff_mu).sum(-1))\n",
    "        assert x_dists.shape == (n_bch, n_x, n_chrg)\n",
    "        if d != 2:\n",
    "            poten1 = (x_dists**(2-d))\n",
    "            assert poten1.shape == (n_bch, n_x, n_chrg)\n",
    "            poten2 = (poten1 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "            assert poten2.shape == (n_bch, n_x)\n",
    "            cst = gamma(d/2) / (2*(lib_pi**(d/2)))\n",
    "            cst = cst / (2-d)\n",
    "            assert isscalar(cst)\n",
    "            poten = cst * poten2\n",
    "            assert poten.shape == (n_bch, n_x)\n",
    "        else:\n",
    "            poten1 = lib.log(x_dists)\n",
    "            assert poten1.shape == (n_bch, n_x, n_chrg)\n",
    "            poten2 = (poten1 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "            assert poten2.shape == (n_bch, n_x)\n",
    "            poten = poten2 / (2*lib_pi)\n",
    "            assert poten.shape == (n_bch, n_x)\n",
    "        return poten\n",
    "\n",
    "    def field(self, x):\n",
    "        lib = torch if torch.is_tensor(x) else np\n",
    "        lib_pi = self.tch_pi if torch.is_tensor(x) else np.pi\n",
    "        w = self.weights_tch if torch.is_tensor(x) else self.weights\n",
    "        mu = self.locations_tch if torch.is_tensor(x) else self.locations\n",
    "        n_bch, n_chrg, d = self.n_bch, self.n_chrg, self.d\n",
    "        n_x = x.shape[-2]\n",
    "        assert x.shape == (n_bch, n_x, d)\n",
    "        x_diff_mu = x.reshape(n_bch, n_x, 1, d) - \\\n",
    "            mu.reshape(n_bch, 1, n_chrg, d)\n",
    "        assert x_diff_mu.shape == (n_bch, n_x, n_chrg, d)\n",
    "        x_dists = lib.sqrt(lib.square(x_diff_mu).sum(-1))\n",
    "        assert x_dists.shape == (n_bch, n_x, n_chrg)\n",
    "        poten1 = (x_dists**(-d))\n",
    "        assert poten1.shape == (n_bch, n_x, n_chrg)\n",
    "        poten2 = (poten1 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "        assert poten2.shape == (n_bch, n_x)\n",
    "        cst = gamma(d/2) / (2*(lib_pi**(d/2)))\n",
    "        assert isscalar(cst)\n",
    "        poten = cst * poten2\n",
    "        assert poten.shape == (n_bch, n_x)\n",
    "        field = poten.reshape(n_bch, n_x, 1) * x\n",
    "        assert field.shape == (n_bch, n_x, d)\n",
    "        return field\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3319be1",
   "metadata": {},
   "source": [
    "### Defining the Volume Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5270d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BallSampler:\n",
    "    def __init__(self, c_dstr, c_params, r_dstr, r_params, batch_rng):\n",
    "        assert isinstance(c_params, dict)\n",
    "        for name, param in c_params.items():\n",
    "            msg_ = f'center param {name} is not np.array'\n",
    "            assert isinstance(param, np.ndarray), msg_\n",
    "        \n",
    "        assert isinstance(r_params, dict)\n",
    "        for name, param in r_params.items():\n",
    "            msg_ = f'radius param {name} is not np.array'\n",
    "            assert isinstance(param, np.ndarray), msg_\n",
    "\n",
    "        self.batch_rng = batch_rng\n",
    "        self.lib = batch_rng.lib\n",
    "        \n",
    "        ##############################################################\n",
    "        ################# Center Sampling Parameters #################\n",
    "        ##############################################################\n",
    "        c_params_ = c_params.copy()\n",
    "        self.c_dstr = c_dstr\n",
    "        if c_dstr == 'uniform':\n",
    "            c_low = c_params_.pop('low')\n",
    "            c_high = c_params_.pop('high')\n",
    "            \n",
    "            n_bch, dim = c_low.shape\n",
    "            \n",
    "            self.c_low_np = c_low.reshape(n_bch, 1, dim)\n",
    "            self.c_high_np = c_high.reshape(n_bch, 1, dim)\n",
    "            self.c_size_np = (self.c_high_np - self.c_low_np)\n",
    "\n",
    "            if self.lib == 'torch':\n",
    "                self.c_low_tch = torch.from_numpy(self.c_low_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_high_tch = torch.from_numpy(self.c_high_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_size_tch = torch.from_numpy(self.c_size_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            \n",
    "            self.c_low = self.c_low_np if self.lib == 'numpy' else self.c_low_tch\n",
    "            self.c_size = self.c_size_np if self.lib == 'numpy' else self.c_size_tch\n",
    "        elif c_dstr == 'normal':\n",
    "            c_loc = c_params_.pop('loc')\n",
    "            c_scale = c_params_.pop('scale')\n",
    "            \n",
    "            n_bch, dim = c_loc.shape\n",
    "            self.c_loc_np = c_loc.reshape(n_bch, 1, dim)\n",
    "            self.c_scale_np = c_scale.reshape(n_bch, 1, 1)\n",
    "            \n",
    "            if self.lib == 'torch':\n",
    "                self.c_loc_tch = torch.from_numpy(self.c_loc_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_scale_tch = torch.from_numpy(self.c_scale_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                \n",
    "            self.c_loc = self.c_loc_np if self.lib == 'numpy' else self.c_loc_tch\n",
    "            self.c_scale = self.c_scale_np if self.lib == 'numpy' else self.c_scale_tch\n",
    "        elif c_dstr == 'ball':\n",
    "            c_cntr = c_params_.pop('c')\n",
    "            c_radi = c_params_.pop('r')\n",
    "            \n",
    "            n_bch, dim = c_cntr.shape\n",
    "            self.c_cntr_np = c_cntr.reshape(n_bch, 1, dim)\n",
    "            self.c_radi_np = c_radi.reshape(n_bch, 1, 1)\n",
    "            \n",
    "            if self.lib == 'torch':\n",
    "                self.c_cntr_tch = torch.from_numpy(self.c_cntr_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_radi_tch = torch.from_numpy(self.c_radi_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                \n",
    "            self.c_cntr = self.c_cntr_np if self.lib == 'numpy' else self.c_cntr_tch\n",
    "            self.c_radi = self.c_radi_np if self.lib == 'numpy' else self.c_radi_tch\n",
    "        else:\n",
    "            raise ValueError(f'c_dstr=\"{c_dstr}\" not implemented')\n",
    "        \n",
    "        msg_ = f'Some center parameters were left unused: {list(c_params_.keys())}'\n",
    "        assert len(c_params_) == 0, msg_\n",
    "            \n",
    "        self.n_bch, self.d = n_bch, dim\n",
    "        \n",
    "        ##############################################################\n",
    "        ################# Radius Sampling Parameters #################\n",
    "        ##############################################################\n",
    "        r_params_ = r_params.copy()\n",
    "        r_low = r_params_.pop('low')\n",
    "        r_high = r_params_.pop('high')\n",
    "        \n",
    "        if r_dstr == 'uniform':\n",
    "            self.r_upow = 1.0\n",
    "        elif r_dstr == 'unifdpow':\n",
    "            self.r_upow = 1.0 / self.d\n",
    "        else:\n",
    "            raise ValueError(f'r_dstr={r_dstr} not implemented')\n",
    "\n",
    "        r_low_rshp = r_low.reshape(self.n_bch, 1)\n",
    "        r_high_rshp = r_high.reshape(self.n_bch, 1)\n",
    "        assert (r_low >= 0.0).all()\n",
    "        assert (r_high >= r_low).all()\n",
    "        \n",
    "        self.r_dstr = r_dstr\n",
    "        self.r_low_np = np.power(r_low_rshp, 1.0/self.r_upow)\n",
    "        self.r_high_np = np.power(r_high_rshp, 1.0/self.r_upow)\n",
    "        self.r_size_np = (self.r_high_np - self.r_low_np)\n",
    "        \n",
    "        if self.lib == 'torch':\n",
    "            self.r_low_tch = torch.from_numpy(self.r_low_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            self.r_high_tch = torch.from_numpy(self.r_high_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            self.r_size_tch = torch.from_numpy(self.r_size_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            \n",
    "        self.r_low = self.r_low_np if self.lib == 'numpy' else self.r_low_tch\n",
    "        self.r_size = self.r_size_np if self.lib == 'numpy' else self.r_size_tch\n",
    "        \n",
    "        msg_ = f'Some center parameters were left unused: {list(r_params_.keys())}'\n",
    "        assert len(r_params_) == 0, msg_\n",
    "\n",
    "    def __call__(self, n=1):\n",
    "        radii = self.r_low + self.r_size * \\\n",
    "            self.batch_rng.uniform((self.n_bch, n))\n",
    "        radii = radii ** self.r_upow\n",
    "        \n",
    "        if self.c_dstr == 'uniform':\n",
    "            centers = self.batch_rng.uniform((self.n_bch, n, self.d))\n",
    "            centers = centers * self.c_size + self.c_low\n",
    "        elif self.c_dstr == 'normal':\n",
    "            centers = self.batch_rng.normal((self.n_bch, n, self.d))\n",
    "            centers = centers * self.c_scale + self.c_loc\n",
    "        elif self.c_dstr == 'ball':\n",
    "            rnd1 = self.batch_rng.normal((self.n_bch, n, self.d))\n",
    "            rnd1 = rnd1 / ((rnd1**2).sum(-1, keepdims=True)**0.5)\n",
    "            \n",
    "            rnd2 = self.batch_rng.uniform((self.n_bch, n, 1))\n",
    "            rnd2 = rnd2 ** (1./self.d)\n",
    "            \n",
    "            centers = self.c_radi * rnd2 * rnd1 + self.c_cntr\n",
    "        else:\n",
    "            raise ValueError(f'c_dstr=\"{self.c_dstr}\" not implemented')\n",
    "        \n",
    "        d = dict()\n",
    "        d['type'] = 'ball'\n",
    "        d['centers'] = centers\n",
    "        d['radii'] = radii\n",
    "        return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a80044",
   "metadata": {},
   "source": [
    "### Sruface Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c371ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cube2Sphere:\n",
    "    def __init__(self, n_cdfint, dim, tch_device, tch_dtype):\n",
    "        self.n_cdfint = n_cdfint\n",
    "        self.dim = dim\n",
    "        self.tch_device = tch_device\n",
    "        self.tch_dtype = tch_dtype\n",
    "        self.cdftab = self.get_cdftab(n_cdfint, dim)\n",
    "        \n",
    "    def tch_exlinspace(self, start, end, n):\n",
    "        assert n >= 1\n",
    "        a = torch.linspace(start, end, n+1,\n",
    "                           device=self.tch_device,\n",
    "                           dtype=self.tch_dtype)[:-1]\n",
    "        b = a + 0.5 * (end - a[-1])\n",
    "        return b\n",
    "        \n",
    "    def get_cdftab(self, n_cdfint, dim):\n",
    "        r\"\"\"\n",
    "        This function computes the necessary CDF functions $F_n$ for $1\\leq n \\leq dim-1$ such that \n",
    "\n",
    "            $$F_n(u) = \\frac{1}{\\int_{0}^{\\pi} \\sin^{n}(\\phi) \\text{d}{\\phi}} \\int_{0}^{\\pi u} \\sin^{n}(\\phi) \\text{d}{\\phi}.$$\n",
    "\n",
    "        Note that $F_n(0) = 0$ and $F_n(1)=1$, and that $F_n$ is basically a CDF for a $\\sin^{n}$-like PDF.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_cdfint: (int) The number of points for CDF integration and table lookup.\n",
    "\n",
    "        dim: (int) The dimension of the space. The unit sphere should be a \n",
    "            `dim-1` dimensional manifold.\n",
    "        \"\"\"\n",
    "        # Step 0: Defining the inverse CDF mapping\n",
    "        thunif1d = self.tch_exlinspace(0.0, np.pi, n_cdfint)\n",
    "        assert thunif1d.shape == (n_cdfint,)\n",
    "\n",
    "        thunif = thunif1d.reshape(1, n_cdfint).expand(dim-1, n_cdfint)\n",
    "        assert thunif.shape == (dim-1, n_cdfint)\n",
    "\n",
    "        sinpow = torch.arange(dim-2, -1, -1, device=self.tch_device).reshape(dim-1, 1)\n",
    "        assert sinpow.shape == (dim-1, 1)\n",
    "\n",
    "        thsinpow = torch.sin(thunif) ** sinpow\n",
    "        assert thsinpow.shape == (dim-1, n_cdfint)\n",
    "\n",
    "        cdftab = thsinpow.cumsum(dim=-1)\n",
    "        cdftab = cdftab / cdftab[:, -1:]\n",
    "        assert cdftab.shape == (dim-1, n_cdfint)\n",
    "        \n",
    "        return cdftab\n",
    "        \n",
    "    def __call__(self, unifs):\n",
    "        \"\"\"\n",
    "        Takes a set of uniform random values in the $[0, 1]^{dim-1}$ cube, and transforms it \n",
    "        to the points on the surface of the $d$-dimensional unit-ball.\n",
    "        \n",
    "        The transformation is designed in a way such that uniform inputs in the rectangle lead\n",
    "        to uniform points on the surface of the unit-ball.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        unifs: (torch.tensor) An input tensor with all values between 0 and 1. This input can \n",
    "            be batched. The shape of this tensor must end with `dim-1`.\n",
    "            \n",
    "            Example:\n",
    "                `dim = 5`\n",
    "                `unifs = torch.rand(100, 10, 25, 4)`\n",
    "            \n",
    "        Output\n",
    "        ------\n",
    "        x_cart: (torch.tensor) The output points on the unit sphere in the cartesian system.\n",
    "            This will have the same batch dimensions as the input.\n",
    "            \n",
    "            Example:\n",
    "                `dim = 5`\n",
    "                `unifs = torch.rand(100, 10, 25, 4)`\n",
    "                `assert x_cart.shape == (100, 10, 25, 5)`\n",
    "        \"\"\"\n",
    "        dim, cdftab, n_cdfint = self.dim, self.cdftab, self.n_cdfint\n",
    "        tch_dtype, tch_device = self.tch_dtype, self.tch_device\n",
    "        assert unifs.shape[-1] == dim-1\n",
    "        \n",
    "        u_mbdims = unifs.shape[:-1]\n",
    "        n_samp = np.prod(u_mbdims)\n",
    "        \n",
    "        # Step 2: Applying the inverse CDF mapping\n",
    "        unifs_T = unifs.reshape(n_samp, dim-1).T.contiguous()\n",
    "        assert unifs_T.shape == (dim-1, n_samp)\n",
    "\n",
    "        cdfranks_T = torch.searchsorted(cdftab, unifs_T)\n",
    "        assert cdfranks_T.shape == (dim-1, n_samp)\n",
    "\n",
    "        cdfranks = cdfranks_T.T.reshape(n_samp, dim-1)\n",
    "        assert cdfranks.shape == (n_samp, dim-1)\n",
    "\n",
    "        cdfinvu = (cdfranks / n_cdfint).to(unifs.dtype)\n",
    "        assert cdfinvu.shape == (n_samp, dim-1)\n",
    "\n",
    "        # Step 3: Scaling the uniform values to the phi ranges\n",
    "        #   Note: $\\phi_{d-1}$ should be in the $[0, 2\\pi]$ range, unlike \n",
    "        #         the $[0, \\pi]$ range for the rest of the coordinates.\n",
    "        phi_scale = torch.tensor([np.pi]*(dim-2) + [2*np.pi]).to(\n",
    "            dtype=tch_dtype, device=tch_device).reshape(1, dim-1)\n",
    "        assert phi_scale.shape == (1, dim-1)\n",
    "\n",
    "        phi = cdfinvu * phi_scale\n",
    "        assert phi.shape == (n_samp, dim-1)\n",
    "\n",
    "        # Step 4: Translating to cartesian coordinates\n",
    "        phi_cos_ = torch.cos(phi)\n",
    "        assert phi_cos_.shape == (n_samp, dim-1)\n",
    "\n",
    "        phi_cos = torch.cat([phi_cos_, torch.ones(n_samp, 1, \n",
    "            dtype=tch_dtype, device=tch_device)], dim=-1)\n",
    "        assert phi_cos.shape == (n_samp, dim)\n",
    "\n",
    "        phi_sin = torch.sin(phi)\n",
    "        assert phi_sin.shape == (n_samp, dim-1)\n",
    "\n",
    "        phi_sincumprod_ = phi_sin.cumprod(dim=-1)\n",
    "        assert phi_sincumprod_.shape == (n_samp, dim-1)\n",
    "\n",
    "        phi_sincumprod = torch.cat([torch.ones(n_samp, 1, dtype=tch_dtype, \n",
    "            device=tch_device), phi_sincumprod_], dim=-1)\n",
    "        assert phi_sincumprod.shape == (n_samp, dim)\n",
    "\n",
    "        x_cart_ = phi_sincumprod * phi_cos\n",
    "        assert x_cart_.shape == (n_samp, dim)\n",
    "\n",
    "        # Making sure the points lie on the unit sphere\n",
    "        assert x_cart_.square().sum(dim=-1).allclose(torch.ones(1, dtype=tch_dtype, \n",
    "            device=tch_device))\n",
    "\n",
    "        x_cart = x_cart_.reshape(*u_mbdims, dim)\n",
    "        assert x_cart.shape == (*u_mbdims, dim)\n",
    "        \n",
    "        return x_cart\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d890c38",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SphereSampler:\n",
    "    def __init__(self, batch_rng):\n",
    "        self.tch_dtype = batch_rng.dtype\n",
    "        self.tch_device = batch_rng.device\n",
    "        self.batch_rng = batch_rng\n",
    "\n",
    "    def np_exlinspace(self, start, end, n):\n",
    "        assert n >= 1\n",
    "        a = np.linspace(start, end, n, endpoint=False)\n",
    "        b = a + 0.5 * (end - a[-1])\n",
    "        return b\n",
    "\n",
    "    def tch_exlinspace(self, start, end, n):\n",
    "        assert n >= 1\n",
    "        a = torch.linspace(start, end, n+1,\n",
    "                           device=self.tch_device,\n",
    "                           dtype=self.tch_dtype)[:-1]\n",
    "        b = a + 0.5 * (end - a[-1])\n",
    "        return b\n",
    "\n",
    "    def __call__(self, volumes, n, trnsfrm_params, samp_params, do_randrots=True, do_shflpts=True):\n",
    "        # volumes -> dictionary\n",
    "        assert volumes['type'] == 'ball'\n",
    "        centers = volumes['centers']\n",
    "        radii = volumes['radii']\n",
    "        n_bch, n_v, d = centers.shape\n",
    "        use_np = not torch.is_tensor(centers)\n",
    "        assert centers.shape == (n_bch, n_v, d)\n",
    "        assert radii.shape == (n_bch, n_v)\n",
    "        assert not (use_np) or (self.batch_rng.lib == 'numpy')\n",
    "        assert use_np or (self.batch_rng.device == centers.device)\n",
    "        assert use_np or (self.batch_rng.dtype == centers.dtype)\n",
    "        assert self.batch_rng.shape == (n_bch,)\n",
    "        exlinspace = self.np_exlinspace if use_np else self.tch_exlinspace\n",
    "        meshgrid = np.meshgrid if use_np else torch.meshgrid\n",
    "        sin = np.sin if use_np else torch.sin\n",
    "        cos = np.cos if use_np else torch.cos\n",
    "        arccos = np.arccos if use_np else torch.arccos\n",
    "        matmul = np.matmul if use_np else torch.matmul\n",
    "        \n",
    "        # Phase 0: Input arguments processing\n",
    "        \n",
    "        # Taking shallow copies\n",
    "        trnsfrm_params, samp_params = dict(trnsfrm_params), dict(samp_params)\n",
    "        trnsfrm_mthd = trnsfrm_params.pop('dstr')\n",
    "        if trnsfrm_mthd == 'cube2sphr':\n",
    "            cube2sphr = trnsfrm_params.pop('cube2sphr')\n",
    "            rv_dim = d - 1\n",
    "        elif trnsfrm_mthd == 'normscale':\n",
    "            rv_dim = d\n",
    "        else:\n",
    "            raise RuntimeError('Not implemented')\n",
    "        assert len(trnsfrm_params) == 0, f'unknown params: {trnsfrm_params}'\n",
    "        \n",
    "        samp_mthd = samp_params.pop('dstr')\n",
    "        if samp_mthd == 'quad':\n",
    "            quad_x = samp_params.pop('x')\n",
    "            assert quad_x.shape == (n, rv_dim)\n",
    "            quad_w = samp_params.pop('w')\n",
    "            assert quad_w.shape == (n,)\n",
    "            n_bch_, n_v_ = 1, 1\n",
    "        elif samp_mthd == 'qmc':\n",
    "            qmc_x = samp_params.pop('x')\n",
    "            assert qmc_x.shape == (n, rv_dim)\n",
    "            n_bch_, n_v_ = 1, 1\n",
    "        elif samp_mthd == 'rng':\n",
    "            n_bch_, n_v_ = n_bch, n_v\n",
    "        elif samp_mthd == 'grid':\n",
    "            n_bch_, n_v_ = 1, 1\n",
    "        else:\n",
    "            raise RuntimeError('Not implemented')\n",
    "        assert len(samp_params) == 0, f'unknown params: {samp_params}'\n",
    "        en_randrots = do_randrots and (samp_mthd in ('grid', 'quad', 'qmc'))\n",
    "        en_shflpts = do_shflpts and (samp_mthd in ('quad', 'qmc', 'grid'))\n",
    "        \n",
    "        # Phase 1: Creating the `norms`, `unifs`, and `weights` variables.\n",
    "        #          We either create or sample these variables, or process \n",
    "        #          the input arguments to create them.\n",
    "        if (samp_mthd == 'grid') and (trnsfrm_mthd == 'normscale'):\n",
    "            n_droot = int(np.round(n ** (1.0 / d)))\n",
    "            assert n == (n_droot ** d), f'N={n} should have an integer {d} root (n_droot={n_droot})'\n",
    "            u1d = exlinspace(0, 1, n_droot)\n",
    "            assert u1d.shape == (n_droot,)\n",
    "            n1d = torch.erfinv(2 * u1d - 1) * np.sqrt(2)\n",
    "            norms = torch.cartesian_prod(*([n1d] * d)).reshape(1, 1, n, d)\n",
    "            assert norms.shape == (n_bch_, n_v_, n, d)\n",
    "            weights = torch.ones(1, 1, 1, dtype=self.tch_dtype, \n",
    "                device=self.tch_device).expand(n_bch, n_v, n)\n",
    "            assert weights.shape == (n_bch, n_v, n)\n",
    "        elif (samp_mthd == 'grid') and (trnsfrm_mthd == 'cube2sphr'):\n",
    "            n_droot = int(np.round(n ** (1.0 / (d-1))))\n",
    "            assert n == (n_droot ** (d-1)), f'N={n} should have an integer {d-1} root'\n",
    "            u1d = exlinspace(0, 1, n_droot)\n",
    "            assert u1d.shape == (n_droot,)\n",
    "            unifs = torch.cartesian_prod(*([u1d] * (d-1))).reshape(1, 1, n, d-1)\n",
    "            assert unifs.shape == (n_bch_, n_v_, n, d-1)\n",
    "            weights = torch.ones(1, 1, 1, dtype=self.tch_dtype, \n",
    "                device=self.tch_device).expand(n_bch, n_v, n)\n",
    "            assert weights.shape == (n_bch, n_v, n)\n",
    "        elif (samp_mthd == 'rng') and (trnsfrm_mthd == 'normscale'):\n",
    "            norms = self.batch_rng.normal((n_bch, n_v, n, d))\n",
    "            assert norms.shape == (n_bch_, n_v_, n, d)\n",
    "            weights = torch.ones(1, 1, 1, dtype=self.tch_dtype, \n",
    "                device=self.tch_device).expand(n_bch, n_v, n)\n",
    "            assert weights.shape == (n_bch, n_v, n)\n",
    "        elif (samp_mthd == 'rng') and (trnsfrm_mthd == 'cube2sphr'):\n",
    "            unifs = self.batch_rng.uniform((n_bch, n_v, n, d-1))\n",
    "            assert unifs.shape == (n_bch_, n_v_, n, d-1)\n",
    "            weights = torch.ones(1, 1, 1, dtype=self.tch_dtype, \n",
    "                device=self.tch_device).expand(n_bch, n_v, n)\n",
    "            assert weights.shape == (n_bch, n_v, n)\n",
    "        elif (samp_mthd == 'quad') and (trnsfrm_mthd == 'normscale'):\n",
    "            norms = quad_x.reshape(1, 1, n, d)\n",
    "            assert norms.shape == (n_bch_, n_v_, n, d)\n",
    "            weights = quad_w.reshape(1, 1, n).expand(n_bch, n_v, n)\n",
    "            assert weights.shape == (n_bch, n_v, n)\n",
    "        elif (samp_mthd  == 'quad') and (trnsfrm_mthd == 'cube2sphr'):            \n",
    "            unifs = quad_x.reshape(1, 1, n, d-1)\n",
    "            assert unifs.shape == (n_bch_, n_v_, n, d-1)\n",
    "            weights = quad_w.reshape(1, 1, n).expand(n_bch, n_v, n)\n",
    "            assert weights.shape == (n_bch, n_v, n)\n",
    "        elif (samp_mthd == 'qmc') and (trnsfrm_mthd == 'normscale'):\n",
    "            # norms = torch.erfinv(2 * qmc_x.reshape(1, 1, n, d) - 1) * np.sqrt(2)\n",
    "            norms = qmc_x.reshape(1, 1, n, d)\n",
    "            assert norms.shape == (n_bch_, n_v_, n, d)\n",
    "            weights = torch.ones(1, 1, 1, dtype=self.tch_dtype, \n",
    "                device=self.tch_device).expand(n_bch, n_v, n)\n",
    "            assert weights.shape == (n_bch, n_v, n)\n",
    "        elif (samp_mthd == 'qmc') and (trnsfrm_mthd == 'cube2sphr'):\n",
    "            unifs = qmc_x.reshape(1, 1, n, d-1)\n",
    "            assert unifs.shape == (n_bch_, n_v_, n, d-1)\n",
    "            weights = torch.ones(1, 1, 1, dtype=self.tch_dtype, \n",
    "                device=self.tch_device).expand(n_bch, n_v, n)\n",
    "            assert weights.shape == (n_bch, n_v, n)\n",
    "        else:\n",
    "            raise RuntimeError('Not implemented yet!')\n",
    "\n",
    "        # End of Phase 1. At this point, we should have the following \n",
    "        # satisfied under all conditions.\n",
    "        assert weights.shape == (n_bch, n_v, n)\n",
    "        if (trnsfrm_mthd == 'cube2sphr'):\n",
    "            assert unifs.shape == (n_bch_, n_v_, n, d-1)\n",
    "            assert (unifs >= 0.0).all()\n",
    "            assert (unifs <= 1.0).all()\n",
    "        else:\n",
    "            assert norms.shape == (n_bch_, n_v_, n, d)\n",
    "            assert (norms.square().sum(dim=-1) > 0).all()\n",
    "            \n",
    "        # Phase 2: Transforming `norms`/`unifs` to points on the unit-sphere.\n",
    "        #          Inputs: `norms` and `unifs`.\n",
    "        #          Outputs: `x_tilde_`\n",
    "        if trnsfrm_mthd == 'normscale':\n",
    "            norms_l2 = torch.sqrt(torch.square(norms).sum(dim=-1))\n",
    "            x_tilde_ = norms / norms_l2.reshape(n_bch_, n_v_, n, 1)\n",
    "            assert x_tilde_.shape == (n_bch_, n_v_, n, d)\n",
    "        elif trnsfrm_mthd == 'cube2sphr':\n",
    "            if cube2sphr is not None:\n",
    "                assert cube2sphr is not None\n",
    "                x_tilde_ = cube2sphr(unifs)\n",
    "                assert x_tilde_.shape == (n_bch_, n_v_, n, d) \n",
    "            elif (cube2sphr is None) and (d in [2, 3]):\n",
    "                if d == 2:\n",
    "                    theta_2d = unifs * (2 * np.pi)\n",
    "                    assert theta_2d.shape == (n_bch_, n_v_, n, 1)\n",
    "                    x_tilde_list = [cos(theta_2d), sin(theta_2d)]\n",
    "                elif d == 3:\n",
    "                    u1_2d, u2_2d = unifs[..., :1], unifs[..., 1:]\n",
    "                    assert u1_2d.shape == (n_bch_, n_v_, n, 1)\n",
    "                    assert u2_2d.shape == (n_bch_, n_v_, n, 1)\n",
    "                    phi_2d = arccos(1- 2 * u2_2d)\n",
    "                    theta_2d = u1_2d * (2 * np.pi)\n",
    "                    x_tilde_list = [sin(phi_2d) * cos(theta_2d),\n",
    "                                    sin(phi_2d) * sin(theta_2d), \n",
    "                                    cos(phi_2d)]\n",
    "                else:\n",
    "                    raise RuntimeError('Not implemented yet!')\n",
    "                if use_np:\n",
    "                    x_tilde_ = np.concatenate(x_tilde_list, axis=-1)\n",
    "                else:\n",
    "                    x_tilde_ = torch.cat(x_tilde_list, dim=-1)\n",
    "                assert x_tilde_.shape == (n_bch_, n_v_, n, d)\n",
    "            else:\n",
    "                raise RuntimeError('Not implemented yet!')\n",
    "        else:\n",
    "            raise RuntimeError('Not implemented yet!')\n",
    "        \n",
    "        # End of Phase 2. At this point, we should have the following\n",
    "        # satisfied under all conditions\n",
    "        assert x_tilde_.shape == (n_bch_, n_v_, n, d)\n",
    "        \n",
    "        # Phase 3: Final touches: random rotations, shuffling, constant calculations, etc.\n",
    "        x_tilde = x_tilde_.expand(n_bch, n_v, n, d)\n",
    "        assert x_tilde.shape == (n_bch, n_v, n, d)\n",
    "        \n",
    "        if en_shflpts:\n",
    "            rngunifs = self.batch_rng.uniform((n_bch, n_v, n))\n",
    "            assert rngunifs.shape == (n_bch, n_v, n)\n",
    "            randperm3d = rngunifs.argsort(dim=-1)\n",
    "            assert randperm3d.shape == (n_bch, n_v, n)\n",
    "            randperm4d = randperm3d.reshape(n_bch, n_v, n, 1)\n",
    "            assert randperm4d.shape == (n_bch, n_v, n, 1)\n",
    "            \n",
    "            x_tilde_shfld = torch.take_along_dim(x_tilde, randperm4d, dim=-2)\n",
    "            assert x_tilde_shfld.shape == (n_bch, n_v, n, d)\n",
    "            weights_shfld = torch.take_along_dim(weights, randperm3d, dim=-1)\n",
    "            assert weights_shfld.shape == (n_bch, n_v, n)\n",
    "        else:\n",
    "            x_tilde_shfld = x_tilde\n",
    "            assert x_tilde_shfld.shape == (n_bch, n_v, n, d)\n",
    "            weights_shfld = weights\n",
    "            assert weights_shfld.shape == (n_bch, n_v, n)\n",
    "\n",
    "        if en_randrots:\n",
    "            rot_mats = self.batch_rng.so_n((n_bch, n_v, d, d))\n",
    "            assert rot_mats.shape == (n_bch, n_v, d, d)\n",
    "            \n",
    "        if en_randrots:\n",
    "            x_tilde_rot = matmul(x_tilde_shfld, rot_mats)\n",
    "        else:\n",
    "            x_tilde_rot = x_tilde_shfld\n",
    "        assert x_tilde_rot.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        points = x_tilde_rot * \\\n",
    "            radii.reshape(n_bch, n_v, 1, 1) + centers.reshape(n_bch, n_v, 1, d)\n",
    "        assert points.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        if use_np:\n",
    "            x_tilde_bc = np.broadcast_to(x_tilde_shfld, (n_bch, n_v, n, d))\n",
    "        else:\n",
    "            x_tilde_bc = x_tilde_shfld.expand(n_bch, n_v, n, d)\n",
    "\n",
    "        if en_randrots:\n",
    "            rot_x_tilde = matmul(x_tilde_bc, rot_mats)\n",
    "        else:\n",
    "            rot_x_tilde = x_tilde_bc\n",
    "        assert rot_x_tilde.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        cst = (2*(np.pi**(d/2))) / gamma(d/2)\n",
    "        csts = cst * (radii**(d-1))\n",
    "        assert csts.shape == (n_bch, n_v)\n",
    "\n",
    "        ret_dict = dict(points=points, normals=rot_x_tilde, weights=weights_shfld, areas=csts)\n",
    "        return ret_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae26e3",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c393f4d9",
   "metadata": {
    "code_folding": [
     0,
     57,
     66,
     142,
     225
    ]
   },
   "outputs": [],
   "source": [
    "def get_nn_sol(model, x, n_eval=None, get_field=True, \n",
    "    out_lib='numpy'):\n",
    "    \"\"\"\n",
    "    Gets a model and evaluates it minibatch-wise on the tensor x. \n",
    "    The minibatch size is capped at n_eval. The output will have the \n",
    "    predicted potentials and the vector fields at them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: (nn.module) the batched neural network.\n",
    "\n",
    "    x: (torch.tensor) the evaluation points. This array should be \n",
    "        >2-dimensional and have a shape of `(..., x_rows, x_cols)`.\n",
    "\n",
    "    n_eval: (int or None) the maximum mini-batch size. If None is \n",
    "        given, `x_rows` will be used as `n_eval`.\n",
    "        \n",
    "    out_lib: (str) determines the output tensor type. Should be either \n",
    "        'numpy' or 'torch'.\n",
    "    \n",
    "    Output Dictionary\n",
    "    ----------\n",
    "    v: (np.array or torch.tensor) the evaluated potentials \n",
    "        with a shape of `(*model.shape, x_rows)` where\n",
    "        model.shape is the batch dimensions of the model. \n",
    "\n",
    "    e: (np.array or torch.tensor) the evaluated vector fields \n",
    "        with a shape of `(*model.shape, x_rows, x_cols)` where\n",
    "        model.shape is the batch dimensions of the model.\n",
    "    \"\"\"\n",
    "    x_rows, x_cols = tuple(x.shape)[-2:]\n",
    "    x_bd_ = tuple(x.shape)[:-2]\n",
    "    x_bd = (1,) if len(x_bd_) == 0 else x_bd_\n",
    "    msg_ = f'Cannot have {x.shape} fed to {model.shape}'\n",
    "    assert len(x_bd) <= model.ndim, msg_\n",
    "    if len(x_bd) < model.ndim:\n",
    "        x_bd = tuple([1] * (model.ndim-len(x_b)) + list(x_bd))\n",
    "    assert all((a == b) or (a == 1) or (b == 1) \n",
    "               for a, b in zip(x_bd, model.shape)), msg_\n",
    "    n_eval = x_rows if n_eval is None else n_eval\n",
    "    if out_lib == 'numpy':\n",
    "        to_lib = lambda a: a.detach().cpu().numpy()\n",
    "        lib_cat = lambda al: np.concatenate(al, axis=1)\n",
    "        lpf = '_np'\n",
    "    elif out_lib == 'torch':\n",
    "        to_lib = lambda a: a\n",
    "        lib_cat = lambda al: torch.cat(al, dim=1)\n",
    "        lpf = ''\n",
    "    else:\n",
    "        raise ValueError(f'outlib={outlib} not defined.')\n",
    "\n",
    "    n_batches = int(np.ceil(x_rows / n_eval))\n",
    "    v_pred_list = []\n",
    "    e_pred_list = []\n",
    "    for i in range(n_batches):\n",
    "        x_i = x[..., (i*n_eval):((i+1)*n_eval), :]\n",
    "        xi_rows = x_i.shape[-2]\n",
    "        x_ii = x_i.reshape(*x_bd, xi_rows, x_cols)\n",
    "        x_iii = x_ii.expand(*model.shape, xi_rows, x_cols)\n",
    "        x_iiii = nn.Parameter(x_iii)\n",
    "        v_pred_i = model(x_iiii).squeeze(-1)\n",
    "        v_pred_ii = to_lib(v_pred_i.detach())\n",
    "        v_pred_list.append(v_pred_ii)\n",
    "        if get_field:\n",
    "            e_pred_i, = torch.autograd.grad(v_pred_i.sum(), [x_iiii],\n",
    "                grad_outputs=None, retain_graph=False, create_graph=False,\n",
    "                only_inputs=True, allow_unused=False)\n",
    "            e_pred_ii = to_lib(e_pred_i.squeeze(-1).detach())\n",
    "            e_pred_list.append(e_pred_ii)\n",
    "\n",
    "    v_pred = lib_cat(v_pred_list)\n",
    "    if get_field:\n",
    "        e_pred = lib_cat(e_pred_list)\n",
    "    else:\n",
    "        e_pred = None\n",
    "\n",
    "    outdict = {f'v{lpf}': v_pred, f'e{lpf}': e_pred}\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def get_prob_sol(problem, x, n_eval=None, get_field=True, \n",
    "    out_lib='numpy'):\n",
    "    \"\"\"\n",
    "    Gets a problem and evaluates the analytical solution to its \n",
    "    potentials and vector fields minibatch-wise on the tensor x. \n",
    "    The minibatch size is capped at n_eval. The output will have the \n",
    "    predicted potentials and the vector fields at them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problem: (object) the problem with both the `potential` and \n",
    "        `field` methods for analytical solution evaluation.\n",
    "\n",
    "    x: (torch.tensor) the evaluation points. This array should be \n",
    "        >2-dimensional and have a shape of `(..., x_rows, x_cols)`.\n",
    "\n",
    "    n_eval: (int or None) the maximum mini-batch size. If None is \n",
    "        given, `x_rows` will be used as `n_eval`.\n",
    "\n",
    "    Output Dictionary\n",
    "    ----------\n",
    "    v_np: (np.array) the evaluated potentials with a shape of\n",
    "        `(..., x_rows)`. \n",
    "\n",
    "    e_np: (np.array) the evaluated vector fields with a shape of\n",
    "        `(..., x_rows, x_cols)`.\n",
    "    \"\"\"\n",
    "\n",
    "    assert hasattr(problem, 'potential')\n",
    "    assert callable(problem.potential)\n",
    "    assert hasattr(problem, 'field')\n",
    "    assert callable(problem.field)\n",
    "\n",
    "    x_rows, x_cols = tuple(x.shape)[-2:]\n",
    "    x_bd_ = tuple(x.shape)[:-2]\n",
    "    x_bd = (1,) if len(x_bd_) == 0 else x_bd_\n",
    "    msg_ = f'Cannot have {x.shape} fed to {problem.shape}'\n",
    "    assert len(x_bd) <= problem.ndim, msg_\n",
    "    if len(x_bd) < problem.ndim:\n",
    "        x_bd = tuple([1] * (problem.ndim-len(x_b)) + list(x_bd))\n",
    "    assert all((a == b) or (a == 1) or (b == 1) \n",
    "               for a, b in zip(x_bd, problem.shape)), msg_\n",
    "    n_eval = x_rows if n_eval is None else n_eval\n",
    "    if out_lib == 'numpy':\n",
    "        to_lib = lambda a: a.detach().cpu().numpy()\n",
    "        lib_cat = lambda al: np.concatenate(al, axis=1)\n",
    "        lpf = '_np'\n",
    "    elif out_lib == 'torch':\n",
    "        to_lib = lambda a: a\n",
    "        lib_cat = lambda al: torch.cat(al, dim=1)\n",
    "        lpf = ''\n",
    "    else:\n",
    "        raise ValueError(f'outlib={outlib} not defined.')\n",
    "\n",
    "    n_batches = int(np.ceil(x_rows / n_eval))\n",
    "    v_list = []\n",
    "    e_list = []\n",
    "    for i in range(n_batches):\n",
    "        x_i = x[..., (i*n_eval):((i+1)*n_eval), :]\n",
    "        xi_rows = x_i.shape[-2]\n",
    "        x_ii = x_i.reshape(*x_bd, xi_rows, x_cols)\n",
    "        x_iii = x_ii.expand(*problem.shape, xi_rows, x_cols)\n",
    "        v_i = problem.potential(x_iii)\n",
    "        v_list.append(to_lib(v_i))\n",
    "        if get_field:\n",
    "            e_i = problem.field(x_iii)\n",
    "            e_list.append(to_lib(e_i))\n",
    "\n",
    "    v = lib_cat(v_list)\n",
    "    if get_field:\n",
    "        e = lib_cat(e_list)\n",
    "    else:\n",
    "        e = None\n",
    "    outdict = {f'v{lpf}': v, f'e{lpf}': e}\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def make_grid(x_low, x_high, dim, n_gpd, lib):\n",
    "    \"\"\"\n",
    "    Creates a grid of points using the mesgrid functions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_low: (list) a list of length `dim` with floats \n",
    "        representing the lower limits of the grid.\n",
    "    \n",
    "    x_high: (list) a list of length `dim` with floats \n",
    "        representing the higher limits of the grid.\n",
    "    \n",
    "    dim: (int) the dimension of the grid space.\n",
    "    \n",
    "    n_gpd: (int) the number of points in each \n",
    "        grid dimension. This yields a total of \n",
    "        `n_gpd**dim` points in the total grid.\n",
    "        \n",
    "    lib: (str) either 'torch' or 'numpy'. This determines \n",
    "        the type of `x` output.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    x: (torch.tensor or np.array) a 2-d tensor or array \n",
    "        with the shape of `(n_gpd**dim, dim)`. \n",
    "    \n",
    "    xi_msh_np: (list of np.array) a list of length `dim` \n",
    "        with meshgrid tensors each with a shape of \n",
    "        `[n_gpd] * dim`.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert dim == 2, 'not implemented yet'\n",
    "    assert len(x_low) == dim\n",
    "    assert len(x_high) == dim\n",
    "    assert lib in ('torch', 'numpy')\n",
    "    library = torch if lib == 'torch' else np\n",
    "    tnper = lambda a: a.cpu().detach().numpy()\n",
    "    nper = tnper if lib == 'torch' else lambda a: a\n",
    "    \n",
    "    x1_low, x2_low = x_low\n",
    "    x1_high, x2_high = x_high\n",
    "    n_g_plt = n_gpd ** dim\n",
    "\n",
    "    x1_1d = library.linspace(x1_low, x1_high, n_gpd)\n",
    "    assert x1_1d.shape == (n_gpd,)\n",
    "\n",
    "    x2_1d = library.linspace(x2_low, x2_high, n_gpd)\n",
    "    assert x2_1d.shape == (n_gpd,)\n",
    "\n",
    "    x1_msh, x2_msh = library.meshgrid(x1_1d, x2_1d)\n",
    "    assert x1_msh.shape == (n_gpd, n_gpd)\n",
    "    assert x2_msh.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x1 = x1_msh.reshape(n_g_plt, 1)\n",
    "    assert x1.shape == (n_g_plt, 1)\n",
    "\n",
    "    x2 = x2_msh.reshape(n_g_plt, 1)\n",
    "    assert x2.shape == (n_g_plt, 1)\n",
    "\n",
    "    x1_1d_c = x1_1d.reshape(n_gpd, 1)\n",
    "    assert x1_1d_c.shape == (n_gpd, 1)\n",
    "\n",
    "    x2_1d_c = x2_1d.reshape(n_gpd, 1)\n",
    "    assert x2_1d_c.shape == (n_gpd, 1)\n",
    "\n",
    "    x1_msh_np = nper(x1_msh)\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x2_msh_np = nper(x2_msh)\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x = torch.cat([x1, x2], dim=1)\n",
    "    assert x.shape == (n_g_plt, dim)\n",
    "\n",
    "    x_np = nper(x)\n",
    "    assert x_np.shape == (n_g_plt, dim)\n",
    "    \n",
    "    xi_msh_np = [x1_msh_np, x2_msh_np]\n",
    "    outdict = dict(x=x, xi_msh_np=xi_msh_np)\n",
    "\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=None, ax=None, cax=None):\n",
    "    n_gpd, dim = x1_msh_np.shape[0], x1_msh_np.ndim\n",
    "    assert dim == 2, f'dim={dim}, x1_msh_np.shape={x1_msh_np.shape}'\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "    assert x2_msh_np.shape == (n_gpd, n_gpd)\n",
    "    n_g = (n_gpd ** dim)\n",
    "   \n",
    "    if fig is None:\n",
    "        assert ax is None\n",
    "        assert cax is None\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(3.0, 2.5), dpi=72)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    else:\n",
    "        assert ax is not None\n",
    "   \n",
    "    e_percentile_cap = 90\n",
    "    if 'v_np' in sol_dict:\n",
    "        v_np = sol_dict['v_np']\n",
    "    else:\n",
    "        v_np = sol_dict['v'].detach().cpu().numpy()\n",
    "    assert v_np.shape[-1] == n_g\n",
    "    \n",
    "    v_msh_np = v_np.reshape(-1, n_gpd, n_gpd).mean(axis=0)\n",
    "    im = ax.pcolormesh(x1_msh_np, x2_msh_np, v_msh_np,\n",
    "                        shading='auto', cmap='RdBu')\n",
    "    if cax is not None:\n",
    "        fig.colorbar(im, cax=cax)\n",
    "\n",
    "    if 'e_np' in sol_dict:\n",
    "        e_msh_np = sol_dict['e_np']\n",
    "    else:\n",
    "        e_msh_np = sol_dict['e']\n",
    "        if e_msh_np is not None:\n",
    "            e_msh_np = e_msh_np.detach().cpu().numpy()\n",
    "    \n",
    "    if e_msh_np is not None:\n",
    "        assert e_msh_np.shape[-2:] == (n_g, dim)\n",
    "        e_msh_np = e_msh_np.reshape(-1, n_gpd,\n",
    "            n_gpd, dim).mean(axis=0)\n",
    "        if e_percentile_cap is not None:\n",
    "            e_size = np.sqrt((e_msh_np**2).sum(axis=-1))\n",
    "            e_size_cap = np.percentile(a=e_size, \n",
    "                q=e_percentile_cap, axis=None)\n",
    "            cap_coef = np.ones_like(e_size)\n",
    "            cap_coef[e_size > e_size_cap] = e_size_cap / \\\n",
    "                e_size[e_size > e_size_cap]\n",
    "            e_msh_capped = e_msh_np * \\\n",
    "                cap_coef.reshape(*e_msh_np.shape[:-1], 1)\n",
    "        else:\n",
    "            e_msh_capped = e_msh_np\n",
    "\n",
    "        ax.quiver(x1_msh_np, x2_msh_np,\n",
    "            e_msh_capped[:, :, 0], e_msh_capped[:, :, 1])\n",
    "    return fig, ax, cax\n",
    "\n",
    "\n",
    "def get_perfdict(e_pnts, e_mdlsol, e_prbsol):\n",
    "    \"\"\"\n",
    "    Computes the biased, bias-corrected, and slope-corrected error \n",
    "    metrics for the solutions of a Poisson problem.\n",
    "    \n",
    "    This function computes three types of MSE and MAE statistics:\n",
    "        \n",
    "        1. Plain: just take the model and ground truth solution\n",
    "            and subtract them to get the errors. No bias- or slope-correction \n",
    "            is applied to offset those degrees of freedom.\n",
    "            \n",
    "            shorthand: 'pln'\n",
    "            \n",
    "        2. Bias-corrected: subtracts the average value from both the model \n",
    "            and ground truth solutions, and then computes the errors.\n",
    "            \n",
    "            shorthand: 'bc'\n",
    "            \n",
    "        3. Slope-corrected: Since any linear function can be added to the\n",
    "            Poisson solutions without violating the poisson equation, this\n",
    "            function fits an ordinary least squares to both the model and\n",
    "            ground truth solutions, and then subtracts it from them. This\n",
    "            way, even the arbitrary-slope issue can be addressed.\n",
    "            \n",
    "            shorthand: 'slc'\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    e_pnts: (torch.tensor) The input points to the model and the ground truth.\n",
    "        This should have a shape of (n_seeds, n_evlpnts, dim).\n",
    "        \n",
    "    e_mdlsol: (torch.tensor) The model solution with a\n",
    "        (n_seeds, n_evlpnts) shape.\n",
    "    \n",
    "    e_prbsol: (torch.tensor) The ground truth solution with a\n",
    "        (n_seeds, n_evlpnts) shape.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    outdict: (dict) A mapping between the error keys and their numpy arrays.\n",
    "        The error keys are the cartesian product of ('pln', 'bc', 'slc') \n",
    "        and ('mse', 'mae').\n",
    "    \"\"\"\n",
    "    n_seeds, n_evlpnts, dim = e_pnts.shape\n",
    "    assert e_mdlsol.shape == (n_seeds, n_evlpnts)\n",
    "    assert e_prbsol.shape == (n_seeds, n_evlpnts)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # The plain non-processed error matrix\n",
    "        err_pln = e_mdlsol - e_prbsol\n",
    "        assert err_pln.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # The bias-corrected error matrix\n",
    "        e_mdlsol2 = e_mdlsol - e_mdlsol.mean(dim=-1, keepdims=True)\n",
    "        assert e_mdlsol2.shape == (n_seeds, n_evlpnts)\n",
    "        e_prbsol2 = e_prbsol - e_prbsol.mean(dim=-1, keepdims=True)\n",
    "        assert e_prbsol2.shape == (n_seeds, n_evlpnts)\n",
    "        err_bc = e_mdlsol2 - e_prbsol2\n",
    "        assert err_bc.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # The bias-corrected and normalized error matrix\n",
    "        e_mdlsol3 = e_mdlsol2 / e_mdlsol2.square().mean(dim=-1, keepdims=True).sqrt()\n",
    "        assert e_mdlsol3.shape == (n_seeds, n_evlpnts)\n",
    "        e_prbsol3 = e_prbsol2 / e_prbsol2.square().mean(dim=-1, keepdims=True).sqrt()\n",
    "        assert e_prbsol3.shape == (n_seeds, n_evlpnts)\n",
    "        err_bcn = e_mdlsol3 - e_prbsol3\n",
    "        assert err_bcn.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # The slope-corrected error matrix\n",
    "        e_pntstrans = e_pnts.transpose(-1, -2)\n",
    "        assert e_pntstrans.shape == (n_seeds, dim, n_evlpnts)\n",
    "        e_pntsig = e_pntstrans.matmul(e_pnts)\n",
    "        assert e_pntsig.shape == (n_seeds, dim, dim)\n",
    "        e_pntsiginv = torch.pinverse(e_pntsig)\n",
    "        assert e_pntsiginv.shape == (n_seeds, dim, dim)\n",
    "        e_pntpinv = e_pntsiginv.matmul(e_pntstrans)\n",
    "        assert e_pntpinv.shape == (n_seeds, dim, n_evlpnts)\n",
    "        \n",
    "        # e_pntpinv = torch.pinverse(e_pnts)\n",
    "        # assert e_pntpinv.shape == (n_seeds, dim, n_evlpnts)\n",
    "\n",
    "        e_mdlbeta = e_pntpinv.matmul(e_mdlsol2.unsqueeze(-1))\n",
    "        assert e_mdlbeta.shape == (n_seeds, dim, 1)\n",
    "        e_mdlslpcrc = e_pnts.matmul(e_mdlbeta)\n",
    "        assert e_mdlslpcrc.shape == (n_seeds, n_evlpnts, 1)\n",
    "        e_mdlsol4 = e_mdlsol2 - e_mdlslpcrc.squeeze(-1)\n",
    "        assert e_mdlsol4.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        e_prbbeta = e_pntpinv.matmul(e_prbsol2.unsqueeze(-1))\n",
    "        assert e_prbbeta.shape == (n_seeds, dim, 1)\n",
    "        e_prbslpcrc = e_pnts.matmul(e_prbbeta)\n",
    "        assert e_prbslpcrc.shape == (n_seeds, n_evlpnts, 1)\n",
    "        e_prbsol4 = e_prbsol2 - e_prbslpcrc.squeeze(-1)\n",
    "        assert e_prbsol4.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        err_slc = e_mdlsol4 - e_prbsol4\n",
    "        assert err_slc.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # The normalized slope-corrected error matrix\n",
    "        e_mdlsol5 = e_mdlsol4 - e_mdlsol4.mean(dim=-1, keepdims=True)\n",
    "        assert e_mdlsol5.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        e_mdlsol6 = e_mdlsol5 / e_mdlsol5.square().mean(dim=-1, keepdims=True).sqrt()\n",
    "        assert e_mdlsol6.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        e_prbsol5 = e_prbsol4 - e_prbsol4.mean(dim=-1, keepdims=True)\n",
    "        assert e_prbsol5.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        e_prbsol6 = e_prbsol5 / e_prbsol5.square().mean(dim=-1, keepdims=True).sqrt()\n",
    "        assert e_prbsol6.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        err_scn = e_mdlsol6 - e_prbsol6\n",
    "        assert err_scn.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # Computing the mse and mae values\n",
    "        e_plnmse = err_pln.square().mean(dim=-1)\n",
    "        assert e_plnmse.shape == (n_seeds,)\n",
    "        e_plnmae = err_pln.abs().mean(dim=-1)\n",
    "        assert e_plnmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_bcmse = err_bc.square().mean(dim=-1)\n",
    "        assert e_bcmse.shape == (n_seeds,)\n",
    "        e_bcmae = err_bc.abs().mean(dim=-1)\n",
    "        assert e_bcmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_bcnmse = err_bcn.square().mean(dim=-1)\n",
    "        assert e_bcnmse.shape == (n_seeds,)\n",
    "        e_bcnmae = err_bcn.abs().mean(dim=-1)\n",
    "        assert e_bcnmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_slcmse = err_slc.square().mean(dim=-1)\n",
    "        assert e_slcmse.shape == (n_seeds,)\n",
    "        e_slcmae = err_slc.abs().mean(dim=-1)\n",
    "        assert e_slcmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_scnmse = err_scn.square().mean(dim=-1)\n",
    "        assert e_scnmse.shape == (n_seeds,)\n",
    "        e_scnmae = err_scn.abs().mean(dim=-1)\n",
    "        assert e_scnmse.shape == (n_seeds,)\n",
    "    \n",
    "        outdict = {'pln/mse': e_plnmse.detach().cpu().numpy(),\n",
    "                   'pln/mae': e_plnmae.detach().cpu().numpy(),\n",
    "                   'bc/mse': e_bcmse.detach().cpu().numpy(),\n",
    "                   'bc/mae': e_bcmae.detach().cpu().numpy(),\n",
    "                   'bcn/mse': e_bcnmse.detach().cpu().numpy(),\n",
    "                   'bcn/mae': e_bcnmae.detach().cpu().numpy(),\n",
    "                   'slc/mse': e_slcmse.detach().cpu().numpy(),\n",
    "                   'slc/mae': e_slcmae.detach().cpu().numpy(),\n",
    "                   'scn/mse': e_scnmse.detach().cpu().numpy(),\n",
    "                   'scn/mae': e_scnmae.detach().cpu().numpy()}\n",
    "    \n",
    "    return outdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ea5e7",
   "metadata": {},
   "source": [
    "## Optional Visualization Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3e68d",
   "metadata": {},
   "source": [
    "### Visualizing the True Potential and Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e74ea5",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "ex_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "ex_tchdevice = torch.device(ex_device)\n",
    "ex_tchdtype = torch.double\n",
    "prob2d_ex1 = DeltaProblem(weights=np.array([[1.0, 1.0, 1.0]]),\n",
    "                          locations=np.array([[[0.0,  0.0],\n",
    "                                               [-0.5, -0.5],\n",
    "                                               [0.5,  0.5]]]),\n",
    "                          tch_device=ex_tchdevice,\n",
    "                          tch_dtype=ex_tchdtype)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3.0, 2.5), dpi=72)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "ex_gdict = make_grid(x_low=[-1, -1], x_high=[1., 1.], \n",
    "    dim=2 , n_gpd=50, lib='torch')\n",
    "x = ex_gdict['x'].to(ex_tchdevice, ex_tchdtype)\n",
    "x1_msh_np, x2_msh_np = ex_gdict['xi_msh_np']\n",
    "\n",
    "ex_sol = get_prob_sol(prob2d_ex1, x, n_eval=200, get_field=True)\n",
    "fig, ax, cax = plot_sol(x1_msh_np, x2_msh_np, ex_sol, \n",
    "    fig=fig, ax=ax, cax=cax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e1605",
   "metadata": {},
   "source": [
    "### Visualizing the Sampler and Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1d8e8",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "n_bch = 5\n",
    "rng = BatchRNG(shape=(n_bch,), lib='torch', device=ex_tchdevice, dtype=ex_tchdtype,\n",
    "               unif_cache_cols=100, norm_cache_cols=500)\n",
    "rng.seed(np.broadcast_to(12345+np.arange(n_bch), rng.shape))\n",
    "\n",
    "prob2d_ex2 = DeltaProblem(weights=np.broadcast_to([1.0, 1.0, 1.0], (n_bch, 3)).copy(),\n",
    "                          locations=np.broadcast_to([[0.0,  0.0],\n",
    "                                                     [-0.5, -0.5],\n",
    "                                                     [0.5,  0.5]], (n_bch, 3, 2)).copy(),\n",
    "                          tch_device=ex_tchdevice, tch_dtype=ex_tchdtype)\n",
    "\n",
    "volsampler_2d = BallSampler(c_dstr='uniform', c_params=dict(\n",
    "                            low=np.broadcast_to([-1.0, -1.0], (n_bch, 2)).copy(),\n",
    "                            high=np.broadcast_to([1.0,  1.0], (n_bch, 2)).copy()),\n",
    "                            r_dstr='uniform', r_params=dict(\n",
    "                            low=np.broadcast_to([0.1], (n_bch,)).copy(),\n",
    "                            high=np.broadcast_to([1.5], (n_bch,)).copy()),\n",
    "                            batch_rng=rng)\n",
    "\n",
    "vols = volsampler_2d(n=10)\n",
    "integs = prob2d_ex2.integrate_volumes(vols)\n",
    "for key, val in vols.items():\n",
    "    if torch.is_tensor(val):\n",
    "        vols[key] = val.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd410b",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=36)\n",
    "ax = plt.gca()\n",
    "\n",
    "i = 0\n",
    "max_integ = prob2d_ex2.weights[i][prob2d_ex2.weights[i] > 0].sum()\n",
    "min_integ = prob2d_ex2.weights[i][prob2d_ex2.weights[i] < 0].sum()\n",
    "cmap = mpl.cm.get_cmap('RdBu')\n",
    "cnorm = mpl.colors.Normalize(vmin=min_integ, vmax=max_integ)\n",
    "\n",
    "ax.scatter(prob2d_ex2.locations[i, :, 0],\n",
    "           prob2d_ex2.locations[i, :, 1], marker='*', color='black', s=150)\n",
    "for center, radius, integ in zip(vols['centers'][i], vols['radii'][i], integs[i]):\n",
    "    circle = plt.Circle(center, radius, fill=False,\n",
    "                        color=cmap(1.0-cnorm(integ.item())))\n",
    "    ax.add_patch(circle)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01766402",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "n_bch = 5\n",
    "rng = BatchRNG(shape=(n_bch,), lib='torch', device=ex_tchdevice, dtype=ex_tchdtype,\n",
    "               unif_cache_cols=100, norm_cache_cols=500)\n",
    "rng.seed(np.broadcast_to(12345+np.arange(n_bch), rng.shape))\n",
    "\n",
    "prob2d_ex3 = DeltaProblem(weights=np.broadcast_to([1.0, 1.0, 1.0], (n_bch, 3)).copy(),\n",
    "                          locations=np.broadcast_to([[0.0,  0.0],\n",
    "                                                     [-0.5, -0.5],\n",
    "                                                     [0.5,  0.5]], (n_bch, 3, 2)).copy(),\n",
    "                          tch_device=ex_tchdevice,\n",
    "                          tch_dtype=ex_tchdtype)\n",
    "\n",
    "\n",
    "volsampler_2d = BallSampler(c_dstr='uniform', c_params=dict(\n",
    "                            low=np.broadcast_to([-1.0, -1.0], (n_bch, 2)).copy(),\n",
    "                            high=np.broadcast_to([1.0,  1.0], (n_bch, 2)).copy()),\n",
    "                            r_dstr='uniform', r_params=dict(\n",
    "                            low=np.broadcast_to([0.1], (n_bch,)).copy(),\n",
    "                            high=np.broadcast_to([1.5], (n_bch,)).copy()),\n",
    "                            batch_rng=rng)\n",
    "\n",
    "sphsampler_2d = SphereSampler(batch_rng=rng)\n",
    "\n",
    "vols = volsampler_2d(n=10)\n",
    "sphsamps2d = sphsampler_2d(vols, 100, \n",
    "    trnsfrm_params=dict(dstr='cube2sphr', cube2sphr=None), \n",
    "    samp_params=dict(dstr='grid'), do_randrots=True, do_shflpts=False)\n",
    "points = sphsamps2d['points']\n",
    "surfacenorms = sphsamps2d['normals']\n",
    "if torch.is_tensor(points):\n",
    "    points = points.detach().cpu().numpy()\n",
    "if torch.is_tensor(surfacenorms):\n",
    "    surfacenorms = surfacenorms.detach().cpu().numpy()\n",
    "points.shape, surfacenorms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5aa16a",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=36)\n",
    "ax = plt.gca()\n",
    "\n",
    "i = 0\n",
    "\n",
    "max_integ = prob2d_ex3.weights[i][prob2d_ex3.weights[i] > 0].sum()\n",
    "min_integ = prob2d_ex3.weights[i][prob2d_ex3.weights[i] < 0].sum()\n",
    "cmap = mpl.cm.get_cmap('RdBu')\n",
    "cnorm = mpl.colors.Normalize(vmin=min_integ, vmax=max_integ)\n",
    "\n",
    "ax.scatter(prob2d_ex3.locations[i, :, 0], prob2d_ex3.locations[i, :, 1],\n",
    "           marker='*', color='black', s=150)\n",
    "for pnts, srfnrms, center, radius, integ in zip(points[i],\n",
    "                                                surfacenorms[i], vols['centers'][i], vols['radii'][i], integs[i]):\n",
    "    ax.scatter(pnts[:, 0], pnts[:, 1], marker='o',\n",
    "               color=cmap(1.0-cnorm(integ.item())), s=1)\n",
    "    ax.quiver(pnts[:, 0], pnts[:, 1], srfnrms[:, 0],\n",
    "              srfnrms[:, 1], width=0.002)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607cb2f",
   "metadata": {},
   "source": [
    "### Visualizing the Unit Cube to Unit Sphere Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711cbc0",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "dim = 3\n",
    "n_bch, n_v, n = 1, 1, 100\n",
    "\n",
    "ex_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "ex_tchdevice = torch.device(ex_device)\n",
    "ex_tchdtype = torch.double\n",
    "\n",
    "rng = BatchRNG(shape=(n_bch,), lib='torch', device=ex_tchdevice, dtype=ex_tchdtype,\n",
    "               unif_cache_cols=1000, norm_cache_cols=5000)\n",
    "rng.seed(np.broadcast_to(12345+np.arange(n_bch), rng.shape))\n",
    "\n",
    "\n",
    "ex_cube2sphr = Cube2Sphere(n_cdfint=10000, dim=dim, tch_device=ex_tchdevice,\n",
    "    tch_dtype=ex_tchdtype)\n",
    "\n",
    "# Step 1: Sampling uniform numbers\n",
    "#   Option 1: we can use truly random numbers in the [0,1] intervals.\n",
    "ex_unifs = rng.uniform((n_bch, n_v, n, dim-1))\n",
    "assert ex_unifs.shape == (n_bch, n_v, n, dim-1)\n",
    "\n",
    "#   Option 2: we could pass a meshgrid as uniform samples.\n",
    "n_droot = int(n**(1.0/(dim-1)))\n",
    "assert n == n_droot ** (dim-1)\n",
    "u1d = ex_cube2sphr.tch_exlinspace(0, 1, n_droot)\n",
    "ex_unifs_ = torch.cartesian_prod(*([u1d] * (dim-1))).reshape(n, dim-1)\n",
    "assert ex_unifs_.shape == (n, dim-1)\n",
    "ex_unifs = ex_unifs_.reshape(1, 1, n, dim-1).expand(n_bch, n_v, n, dim-1)\n",
    "assert ex_unifs.shape == (n_bch, n_v, n, dim-1)\n",
    "\n",
    "# Applying the uniform cube to uniform sphere transformation\n",
    "sphr_x = ex_cube2sphr(ex_unifs)\n",
    "assert sphr_x.shape == (n_bch, n_v, n, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463216ad",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=36)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "x_np = sphr_x.reshape(n_bch * n_v * n, dim).detach().cpu().numpy()\n",
    "ax.scatter(x_np[:, 2], x_np[:, 1], x_np[:, 0], marker='o', s=20)\n",
    "\n",
    "ax.grid(False)\n",
    "ax.set_box_aspect((2, 2, 2))\n",
    "ax.set_xticks([-1.0, 0.0, 1.0])\n",
    "ax.set_xticklabels([-1.0, 0.0, 1.0])\n",
    "ax.set_yticks([]); ax.set_yticklabels([])\n",
    "ax.set_zticks([]); ax.set_zticklabels([])\n",
    "for saxis in [ax.xaxis, ax.yaxis, ax.zaxis]:\n",
    "    saxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "    saxis.line.set_color((1.0, 1.0, 1.0, 0.0))\n",
    "ax.set_xlim3d(-1, 1); ax.set_ylim3d(-1, 1); ax.set_zlim3d(-1, 1)\n",
    "ax.quiver(-1.0, -1.0, -1.0, 2.4, 0.0, 0.0, color='k', lw=3, arrow_length_ratio=0.1)\n",
    "ax.quiver(-1.0, -1.0, -1.0, 0.0, 2.4, 0.0, color='k', lw=3, arrow_length_ratio=0.1)\n",
    "ax.quiver(-1.0, -1.0, -1.0, 0.0, 0.0, 2.4, color='k', lw=3, arrow_length_ratio=0.1)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', pad=-5)\n",
    "uu, vv = np.mgrid[0:2*np.pi:20j, 0:np.pi:10j]\n",
    "ax.plot_wireframe(np.cos(uu)*np.sin(vv), np.sin(uu)*np.sin(vv), np.cos(vv), color=\"black\", lw=0.1)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa95d5",
   "metadata": {},
   "source": [
    "## Utility Functions for Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f525f",
   "metadata": {
    "code_folding": [
     8,
     60,
     85
    ]
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "########### Sanity Checking Utility Functions ###########\n",
    "#########################################################\n",
    "\n",
    "msg_bcast = '{} should be np broadcastable to {}={}. '\n",
    "msg_bcast += 'However, it has an inferred shape of {}.'\n",
    "\n",
    "\n",
    "def get_arr(name, trgshp_str, trns_opts):\n",
    "    \"\"\"\n",
    "    Gets a list of values, and checks if it is broadcastable to a \n",
    "    target shape. If the shape does not match, it will raise a proper\n",
    "    assertion error with a meaninful message. The output is a numpy \n",
    "    array that is guaranteed to be broadcastable to the target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: (str) name of the option / hyper-parameter.\n",
    "\n",
    "    trgshp_str: (str) the target shape elements representation. Must be a \n",
    "        valid python expression where the needed elements .\n",
    "\n",
    "    trns_opts: (dict) a dictionary containing the variables needed \n",
    "        for the string to list translation of val.\n",
    "\n",
    "    Key Variables\n",
    "    -------------\n",
    "    `val = trns_opts[name]`: (list or str) list of values read \n",
    "        from the config file. If a string is provided, python's \n",
    "        `eval` function will be used to translate it into a list.\n",
    "        \n",
    "    `trg_shape = eval_formula(trgshp_str, trns_opts)`: (tuple) \n",
    "        the target shape.\n",
    "    \n",
    "    Output\n",
    "    ----------\n",
    "    val_np: (np.array) the numpy array of val. \n",
    "    \"\"\"\n",
    "    msg_ =  f'\"{name}\" must be in trns_opts but it isnt: {trns_opts}'\n",
    "    assert name in trns_opts, msg_\n",
    "    val = trns_opts[name]\n",
    "    \n",
    "    if isinstance(val, str):\n",
    "        val_list = eval_formula(val, trns_opts)\n",
    "    else:\n",
    "        val_list = val\n",
    "    val_np = np.array(val_list)\n",
    "    src_shape = val_np.shape\n",
    "    trg_shape = eval_formula(trgshp_str, trns_opts)\n",
    "    msg_ = msg_bcast.format(name, trgshp_str, trg_shape, src_shape)\n",
    "\n",
    "    assert len(val_np.shape) == len(trg_shape), msg_\n",
    "\n",
    "    is_bcastble = all((x == y or x == 1 or y == 1) for x, y in\n",
    "                      zip(src_shape, trg_shape))\n",
    "    assert is_bcastble, msg_\n",
    "\n",
    "    return val_np\n",
    "\n",
    "\n",
    "def eval_formula(formula, variables):\n",
    "    \"\"\"\n",
    "    Gets a string formula and uses the `eval` function of python to  \n",
    "    translate it into a python variable. The necessary variables for \n",
    "    translation are provided through the `variables` argument.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    formula (str): a string that can be passed to `eval`.\n",
    "        Example: \"[np.sqrt(dim), 'a', None]\"\n",
    "\n",
    "    variables (dict): a dictionary of variables used in the formula.\n",
    "        Example: {\"dim\": 4}\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    pyobj (object): the translated formula into a python object\n",
    "        Example: [2.0, 'a', None]\n",
    "\n",
    "    \"\"\"\n",
    "    locals().update(variables)\n",
    "    pyobj = eval(formula)\n",
    "    return pyobj\n",
    "\n",
    "\n",
    "def chck_dstrargs(opt, cfgdict, dstr2args, opt2req, parnt_optdstr=None):\n",
    "    \"\"\"\n",
    "    Checks if the distribution arguments are provided correctly. Works \n",
    "    with hirarchical models through recursive applications. Proper error \n",
    "    messages are displayed if one of the checks fails.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    opt: (str) the option name.\n",
    "\n",
    "    cfgdict: (dict) the config dictionary.\n",
    "\n",
    "    dstr2args: (dict) a mapping between distribution and their \n",
    "        required arguments.\n",
    "        \n",
    "    opt2req: (dict) required arguments for an option itself, not \n",
    "        necessarily required by the option's distribution.\n",
    "    \"\"\"\n",
    "    opt_dstr = cfgdict.get(f'{opt}/dstr', 'fixed')\n",
    "\n",
    "    msg_ = f'Unknown {opt}_dstr: it should be one of {list(dstr2args.keys())}'\n",
    "    assert opt_dstr in dstr2args, msg_\n",
    "\n",
    "    opt2req = dict() if opt2req is None else opt2req\n",
    "    optreqs = opt2req.get(opt, tuple())\n",
    "    must_spec = list(dstr2args[opt_dstr]) + list(optreqs)\n",
    "    avid_spec = list(chain.from_iterable(\n",
    "        v for k, v in dstr2args.items() if k != opt_dstr))\n",
    "    avid_spec = [k for k in avid_spec if k not in must_spec]\n",
    "\n",
    "    if opt_dstr == 'fixed':\n",
    "        # To avoid infinite recursive calls, we should end this here.\n",
    "        msg_ = f'\"{opt}\" must be specified.'\n",
    "        if parnt_optdstr is not None:\n",
    "            parnt_opt, parnt_dstr = parnt_optdstr\n",
    "            msg_ += f'\"{parnt_opt}\" was specified as \"{parnt_dstr}\", and'\n",
    "        msg_ += f' \"{opt}\" was specified as \"{opt_dstr}\".'\n",
    "        if len(optreqs) > 0:\n",
    "            msg_ += f' Also, \"{opt}\" requires \"{optreqs}\" to be specified.'\n",
    "        opt_val = cfgdict.get(opt, None)\n",
    "        assert opt_val is not None, msg_\n",
    "    else:\n",
    "        for arg in must_spec:\n",
    "            opt_arg = f'{opt}{arg}'\n",
    "            chck_dstrargs(opt_arg, cfgdict, dstr2args, opt2req, (opt, opt_dstr))\n",
    "\n",
    "    for arg in avid_spec:\n",
    "        opt_arg = f'{opt}{arg}'\n",
    "        opt_arg_val = cfgdict.get(opt_arg, None)\n",
    "        msg_ = f'\"{opt_arg}\" should not be specified, since \"{opt}\" '\n",
    "        msg_ += f'appears to follow the \"{opt_dstr}\" distribution.'\n",
    "        assert opt_arg_val is None, msg_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184b2f0",
   "metadata": {},
   "source": [
    "## JSON Config Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976278b",
   "metadata": {
    "code_folding": [
     1
    ],
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "json_cfgpath = f'../configs/01_poisson/20_hindim.yml'\n",
    "! rm -rf \"./23_poisson/results/20_hindim.h5\"\n",
    "! rm -rf \"./23_poisson/storage/20_hindim\"\n",
    "if json_cfgpath.endswith('.json'):\n",
    "    with open(json_cfgpath, 'r') as fp:\n",
    "        json_cfgdict = json.load(fp, object_pairs_hook=odict)\n",
    "elif json_cfgpath.endswith('.yml'):\n",
    "    with open(json_cfgpath, \"r\") as fp:\n",
    "        json_cfgdict = odict(yaml.safe_load(fp))\n",
    "else:\n",
    "    raise RuntimeError(f'unknown config extension: {json_cfgpath}')\n",
    "json_cfgdict['io/config_id'] = '20_hindim'\n",
    "json_cfgdict['io/results_dir'] = './23_poisson/results'\n",
    "json_cfgdict['io/storage_dir'] = './23_poisson/storage'\n",
    "json_cfgdict['io/tch/device'] = 'cuda:0'\n",
    "\n",
    "all_cfgdicts = preproc_cfgdict(json_cfgdict)\n",
    "cfg_dict_input = all_cfgdicts[6]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a080649e",
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "source": [
    "def main(cfg_dict_input):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ed4c3",
   "metadata": {},
   "source": [
    "## Retrieving Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cfg_dict = cfg_dict_input.copy()\n",
    "\n",
    "    #########################################################\n",
    "    #################### Ignored Options ####################\n",
    "    #########################################################\n",
    "    cfgdesc = cfg_dict.pop('desc', None)\n",
    "    cfgdate = cfg_dict.pop('date', None)\n",
    "\n",
    "    #########################################################\n",
    "    ################### Mandatory Options ###################\n",
    "    #########################################################\n",
    "    prob_type = cfg_dict.pop('problem')\n",
    "    rng_seed_list = cfg_dict.pop('rng_seed/list')\n",
    "    dim = cfg_dict.pop('dim')\n",
    "\n",
    "    n_srf = cfg_dict.pop('vol/n')\n",
    "    n_srfpts_mdl = cfg_dict.pop('srfpts/n/mdl')\n",
    "    n_srfpts_trg = cfg_dict.pop('srfpts/n/trg')\n",
    "    do_dblsampling = cfg_dict.pop('srfpts/dblsmpl')\n",
    "    \n",
    "    do_bootstrap = cfg_dict.pop('trg/btstrp')\n",
    "    if do_bootstrap:\n",
    "        tau = cfg_dict.pop('trg/tau')\n",
    "        w_trgreg = cfg_dict.pop('trg/reg/w')\n",
    "    else:\n",
    "        w_trgreg = 0.0\n",
    "    w_trg = cfg_dict.pop('trg/w', None)\n",
    "\n",
    "    opt_type = cfg_dict.pop('opt/dstr')\n",
    "    n_epochs = cfg_dict.pop('opt/epoch')\n",
    "    lr = cfg_dict.pop('opt/lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927366fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    ############### Surface Sampling Options ################\n",
    "    #########################################################\n",
    "    do_detspacing = cfg_dict.pop('srfpts/detspc', None)\n",
    "    detspc_cfg = dict()\n",
    "    if do_detspacing is None:\n",
    "        pass\n",
    "    elif do_detspacing == True:\n",
    "        detspc_cfg['srfpts/trnsfrm/dstr'] = 'cube2sphr'\n",
    "        detspc_cfg['srfpts/trnsfrm/n_cdfint'] = int(1e6)\n",
    "        detspc_cfg['srfpts/samp/dstr'] = 'grid'\n",
    "        detspc_cfg['srfpts/samp/shflpts'] = True\n",
    "    elif do_detspacing == False:\n",
    "        detspc_cfg['srfpts/trnsfrm/dstr'] = 'normscale'\n",
    "        detspc_cfg['srfpts/samp/dstr'] = 'rng'\n",
    "        detspc_cfg['srfpts/samp/shflpts'] = True\n",
    "    else:\n",
    "        msg_ = f'srfpts/detspc=\"{do_detspacing}\" not implemented yet!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    for key in detspc_cfg:\n",
    "        assert key not in cfg_dict, f'cannot specify \"{key}\" with \"srfpts/detspc\"'\n",
    "    cfg_dict.update(detspc_cfg)\n",
    "    cfg_dict_input.update(detspc_cfg)\n",
    "\n",
    "    srf_tnsfrm = cfg_dict.pop('srfpts/trnsfrm/dstr')\n",
    "    if srf_tnsfrm == 'cube2sphr':\n",
    "        n_cdfint = cfg_dict.pop('srfpts/trnsfrm/n_cdfint')\n",
    "    elif srf_tnsfrm == 'normscale':\n",
    "        pass\n",
    "    else:\n",
    "        msg_ = f'srfpts/trnsfrm/dstr=\"{srf_tnsfrm}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    srf_samp = cfg_dict.pop('srfpts/samp/dstr')\n",
    "    srf_shflpts = cfg_dict.pop('srfpts/samp/shflpts')\n",
    "    if srf_samp == 'quad':\n",
    "        quad_order = cfg_dict.pop('srfpts/samp/order')\n",
    "        quad_rule = cfg_dict.pop('srfpts/samp/rule')\n",
    "        quad_sprs = cfg_dict.pop('srfpts/samp/sparse')\n",
    "        quad_rcuralg = cfg_dict.pop('srfpts/samp/rcuralg')\n",
    "    elif srf_samp == 'qmc':\n",
    "        qmc_rule = cfg_dict.pop('srfpts/samp/rule')\n",
    "        qmc_antithetic = cfg_dict.pop('srfpts/samp/antithetic')\n",
    "    elif srf_samp in ('grid', 'rng'):\n",
    "        pass\n",
    "    else:\n",
    "        msg_ = f'srfpts/samp/dstr=\"{srf_samp}\" not defined!'\n",
    "        raise ValueError(msg_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a05c3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    ################## Neural Spec Options ##################\n",
    "    #########################################################\n",
    "    nn_dstr = cfg_dict.pop('nn/dstr')\n",
    "    if nn_dstr == 'mlp':\n",
    "        nn_width = cfg_dict.pop('nn/width')\n",
    "        nn_hidden = cfg_dict.pop('nn/hidden')\n",
    "        nn_act = cfg_dict.pop('nn/act')\n",
    "    else:\n",
    "        msg_ = f'nn/dstr=\"{nn_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    #########################################################\n",
    "    ############## Charge Distribution Options ##############\n",
    "    #########################################################\n",
    "    chrg_dstr = cfg_dict.pop('chrg/dstr')\n",
    "    chrg_n = cfg_dict.pop('chrg/n')\n",
    "\n",
    "    chrg_w_dstr = cfg_dict.pop('chrg/w/dstr', 'fixed')\n",
    "    if chrg_w_dstr == 'fixed':\n",
    "        chrg_w_ = cfg_dict.pop('chrg/w')\n",
    "    elif chrg_w_dstr == 'uniform':\n",
    "        chrg_w_low_ = cfg_dict.pop('chrg/w/low', None)\n",
    "        chrg_w_high_ = cfg_dict.pop('chrg/w/high', None)\n",
    "    elif chrg_w_dstr == 'normal':\n",
    "        chrg_w_loc_ = cfg_dict.pop('chrg/w/loc', None)\n",
    "        chrg_w_scale_ = cfg_dict.pop('chrg/w/scale', None)\n",
    "    else:\n",
    "        msg_ = f'chrg/w/dstr=\"{chrg_w_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    chrg_mu_dstr = cfg_dict.pop('chrg/mu/dstr', 'fixed')\n",
    "    if chrg_mu_dstr == 'fixed':\n",
    "        chrg_mu_ = cfg_dict.pop('chrg/mu')\n",
    "    elif chrg_mu_dstr == 'uniform':\n",
    "        chrg_mu_low_ = cfg_dict.pop('chrg/mu/low')\n",
    "        chrg_mu_high_ = cfg_dict.pop('chrg/mu/high')\n",
    "    elif chrg_mu_dstr == 'normal':\n",
    "        chrg_mu_loc_ = cfg_dict.pop('chrg/mu/loc')\n",
    "        chrg_mu_scale_ = cfg_dict.pop('chrg/mu/scale')\n",
    "    elif chrg_mu_dstr == 'ball':\n",
    "        chrg_mu_c_ = cfg_dict.pop('chrg/mu/c')\n",
    "        chrg_mu_r_ = cfg_dict.pop('chrg/mu/r')\n",
    "    else:\n",
    "        msg_ = f'chrg/mu/dstr=\"{chrg_mu_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    #########################################################\n",
    "    ############### Surface Sampling Options ################\n",
    "    #########################################################\n",
    "    vol_dstr = cfg_dict.pop('vol/dstr')\n",
    "\n",
    "    vol_c_dstr = cfg_dict.pop('vol/c/dstr', 'fixed')\n",
    "    if vol_c_dstr == 'fixed':\n",
    "        vol_c_ = cfg_dict.pop('vol/c')\n",
    "    elif vol_c_dstr == 'uniform':\n",
    "        vol_c_low_ = cfg_dict.pop('vol/c/low')\n",
    "        vol_c_high_ = cfg_dict.pop('vol/c/high')\n",
    "    elif vol_c_dstr == 'normal':\n",
    "        vol_c_loc_ = cfg_dict.pop('vol/c/loc')\n",
    "        vol_c_scale_ = cfg_dict.pop('vol/c/scale')\n",
    "    elif vol_c_dstr == 'ball':\n",
    "        vol_c_c_ = cfg_dict.pop('vol/c/c')\n",
    "        vol_c_r_ = cfg_dict.pop('vol/c/r')\n",
    "    else:\n",
    "        msg_ = f'vol/c/dstr=\"{vol_c_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    vol_r_dstr = cfg_dict.pop('vol/r/dstr', 'fixed')\n",
    "    if vol_r_dstr == 'fixed':\n",
    "        vol_r_ = cfg_dict.pop('vol/r')\n",
    "    elif vol_r_dstr in ('uniform', 'unifdpow'):\n",
    "        vol_r_low_ = cfg_dict.pop('vol/r/low')\n",
    "        vol_r_high_ = cfg_dict.pop('vol/r/high')\n",
    "    else:\n",
    "        msg_ = f'vol/r/dstr=\"{vol_r_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    #########################################################\n",
    "    ############## Initial Condition Options ###############\n",
    "    #########################################################\n",
    "    ic_dstr = cfg_dict.pop('ic/dstr', None)\n",
    "    if ic_dstr in ('sphere', 'trnvol'):\n",
    "        w_ic = cfg_dict.pop('ic/w')\n",
    "        ic_bpp = cfg_dict.pop('ic/bpp')\n",
    "        ic_n = cfg_dict.pop('ic/n')\n",
    "        ic_frq = cfg_dict.pop('ic/frq')\n",
    "        ic_bs = cfg_dict.pop('ic/bs')\n",
    "        ic_needsampling = True\n",
    "    elif ic_dstr in ('trnsrf',):\n",
    "        w_ic = cfg_dict.pop('ic/w')\n",
    "        ic_bpp = cfg_dict.pop('ic/bpp')\n",
    "        ic_n = n_srf * n_srfpts_mdl\n",
    "        ic_frq = cfg_dict.pop('ic/frq')\n",
    "        ic_bs = ic_n\n",
    "        ic_needsampling = False\n",
    "    elif ic_dstr is None:\n",
    "        ic_needsampling = False\n",
    "        ic_frq = 1\n",
    "        w_ic, ic_bpp = 0, 'all'\n",
    "    else:\n",
    "        msg_ = f'ic/dstr={ic_dstr} not defined'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    if ic_dstr == 'sphere':\n",
    "        ic_c_dstr = cfg_dict.pop('ic/c/dstr', 'fixed')\n",
    "        if ic_c_dstr == 'fixed':\n",
    "            ic_c_ = cfg_dict.pop('ic/c')\n",
    "        elif ic_c_dstr == 'uniform':\n",
    "            ic_c_low_ = cfg_dict.pop('ic/c/low')\n",
    "            ic_c_high_ = cfg_dict.pop('ic/c/high')\n",
    "        elif ic_c_dstr == 'normal':\n",
    "            ic_c_loc_ = cfg_dict.pop('ic/c/loc')\n",
    "            ic_c_scale_ = cfg_dict.pop('ic/c/scale')\n",
    "        else:\n",
    "            msg_ = f'ic/c/dstr=\"{ic_c_dstr}\" not defined!'\n",
    "            raise ValueError(msg_)\n",
    "\n",
    "        ic_r_dstr = cfg_dict.pop('ic/r/dstr', 'fixed')\n",
    "        if ic_r_dstr == 'fixed':\n",
    "            ic_r_ = cfg_dict.pop('ic/r')\n",
    "        elif ic_r_dstr in ('uniform', 'unifdpow'):\n",
    "            ic_r_low_ = cfg_dict.pop('ic/r/low')\n",
    "            ic_r_high_ = cfg_dict.pop('ic/r/high')\n",
    "        else:\n",
    "            msg_ = f'ic/r/dstr=\"{ic_r_dstr}\" not defined!'\n",
    "            raise ValueError(msg_)\n",
    "    elif ic_dstr in ('trnsrf', 'trnvol', None):\n",
    "        pass\n",
    "    else:\n",
    "        msg_ = f'ic/dstr=\"{ic_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    #########################################################\n",
    "    ########### Evaluation Point Sampling Options ###########\n",
    "    #########################################################\n",
    "    eid_list_dup = [opt.split('/')[1] for opt in cfg_dict\n",
    "                    if opt.startswith('eval/')]\n",
    "    eid_list = list(odict.fromkeys(eid_list_dup))\n",
    "    evalcfgs = odict()\n",
    "    for eid in eid_list:\n",
    "        evalcfgs[eid] = odict()\n",
    "        cfgopts = list(cfg_dict.keys())\n",
    "        for opt in cfgopts:\n",
    "            prfx = f'eval/{eid}/'\n",
    "            if opt.startswith(prfx):\n",
    "                optn = opt[len(prfx):]\n",
    "                optv = cfg_dict.pop(opt)\n",
    "                evalcfgs[eid][optn] = optv\n",
    "\n",
    "    #########################################################\n",
    "    ################# I/O Logistics Options #################\n",
    "    #########################################################\n",
    "    config_id = cfg_dict.pop('io/config_id')\n",
    "    results_dir = cfg_dict.pop('io/results_dir')\n",
    "    storage_dir = cfg_dict.pop('io/storage_dir', None)\n",
    "    io_avgfrq = cfg_dict.pop('io/avg/frq')\n",
    "    ioflsh_period = cfg_dict.pop('io/flush/frq')\n",
    "    chkpnt_period = cfg_dict.pop('io/ckpt/frq')\n",
    "    device_name = cfg_dict.pop('io/tch/device')\n",
    "    dtype_name = cfg_dict.pop('io/tch/dtype')\n",
    "    iomon_period = cfg_dict.pop('io/mon/frq')\n",
    "    io_cmprssnlvl = cfg_dict.pop('io/cmprssn_lvl')\n",
    "    eval_bs = cfg_dict.pop('io/eval/bs', None)\n",
    "\n",
    "    dtnow = datetime.datetime.now().isoformat(timespec='seconds')\n",
    "    hostname = socket.gethostname()\n",
    "    commit_hash = get_git_commit()\n",
    "    cfg_tree = '/'.join(config_id.split('/')[:-1])\n",
    "    cfg_name = config_id.split('/')[-1]\n",
    "    #########################################################\n",
    "    ##################### Sanity Checks #####################\n",
    "    #########################################################\n",
    "\n",
    "    # Making sure the specified option distributions are implemented.\n",
    "    fixed_opts = ['desc', 'date', 'rng_seed/list', 'problem',\n",
    "                  'dim', 'vol/n',  'srfpts/n/mdl', 'srfpts/n/trg', \n",
    "                  'srfpts/detspc', 'srfpts/dblsmpl', 'trg/btstrp', \n",
    "                  'trg/w', 'trg/tau', 'opt/lr', 'opt/epoch',\n",
    "                  'srfpts/trnsfrm/n_cdfint', 'srfpts/samp/shflpts', \n",
    "                  'srfpts/samp/order', 'srfpts/samp/rule', 'srfpts/samp/sparse', \n",
    "                  'srfpts/samp/rcuralg', 'srfpts/samp/antithetic']\n",
    "\n",
    "    opt2availdstrs = {**{opt: ('fixed',) for opt in fixed_opts},\n",
    "        'chrg': ('dmm',), 'chrg/n': ('fixed',), 'chrg/w': ('fixed',), \n",
    "        'chrg/mu': ('fixed', 'uniform', 'normal', 'ball'),\n",
    "        'vol': ('ball',), 'vol/c': ('uniform', 'ball', 'normal'), \n",
    "        'vol/r': ('uniform', 'unifdpow'), \n",
    "        'ic': ('sphere', 'trnsrf', 'trnvol', 'fixed'), \n",
    "        'ic/c': ('fixed',), 'nn': ('mlp',), 'ic/r': ('fixed',), \n",
    "        'vol/c/low': ('fixed',), 'vol/c/high': ('fixed',),\n",
    "        'vol/c/loc': ('fixed',), 'vol/c/scale': ('fixed',),\n",
    "        'vol/c/c': ('fixed',),   'vol/c/r': ('fixed',),\n",
    "        'vol/r/low': ('fixed',), 'vol/r/high': ('fixed',),\n",
    "        'srfpts/trnsfrm': ('cube2sphr', 'normscale'),\n",
    "        'srfpts/samp': ('quad', 'qmc', 'rng', 'grid'),\n",
    "        **{f'eval/{eid}': ('uniform', 'grid', 'ball', 'trnvol')\n",
    "            for eid in eid_list}}\n",
    "\n",
    "    for opt, avail_dstrs in opt2availdstrs.items():\n",
    "        opt_dstr = cfg_dict_input.get(f'{opt}/dstr', 'fixed')\n",
    "        msg_  = f'\"{opt}\" cannot follow the \"{opt_dstr}\" distribution or type '\n",
    "        msg_ += f' since it is not implemented or available at least yet. The '\n",
    "        msg_ += f'only available options for \"{opt}\" are {avail_dstrs}.'\n",
    "        assert opt_dstr in avail_dstrs, msg_\n",
    "\n",
    "    # Making sure no other options are left unused.\n",
    "    if len(cfg_dict) > 0:\n",
    "        msg_ = f'The following settings were left unused:\\n'\n",
    "        for key, val in cfg_dict.items():\n",
    "            msg_ += f'  {key}: {val}'\n",
    "        raise RuntimeError(msg_)\n",
    "\n",
    "    # Making sure that all \"*_dstr\" options are valid\n",
    "    dstr2args = {'uniform':  ('/low', '/high'),\n",
    "                 'unifdpow': ('/low', '/high'),\n",
    "                 'normal':   ('/loc', '/scale'),\n",
    "                 'dmm':      ('/n', '/w', '/mu'),\n",
    "                 'ball':     ('/c', '/r'),\n",
    "                 'sphere':   ('/c', '/r'),\n",
    "                 'fixed':    ('',),\n",
    "                 'trnvol':   (),\n",
    "                 'trnsrf':   ()}\n",
    "\n",
    "    key2req = {'vol': ('/n',)}\n",
    "    if ic_dstr is not None:\n",
    "        key2req['ic'] = (*key2req.get('ic', []), '/w')\n",
    "    if ic_needsampling: \n",
    "        key2req['ic'] = (*key2req.get('ic', []), '/n', '/frq')\n",
    "\n",
    "    for key in ['chrg', 'vol']:\n",
    "        if key in key2req:\n",
    "            chck_dstrargs(key, cfg_dict_input, dstr2args, key2req)\n",
    "\n",
    "    edstr2args = {'uniform': ('/low', '/high', '/n', '/frq'),\n",
    "                  'grid':    ('/low', '/high', '/n', '/frq'),\n",
    "                  'ball':    ('/c', '/r', '/n', '/frq'),\n",
    "                  'fixed':   ('', '/n', '/frq'),\n",
    "                  'trnvol':  ('/n', '/frq')}\n",
    "    for eid in eid_list:\n",
    "        chck_dstrargs(f'eval/{eid}', cfg_dict_input, \n",
    "            edstr2args, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa88d1",
   "metadata": {},
   "source": [
    "## Problem Objects Construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c153ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Derived options and assertions\n",
    "    n_points = n_srfpts_mdl + n_srfpts_trg\n",
    "\n",
    "    assert not (do_dblsampling) or (n_srfpts_trg > 1)\n",
    "    if w_trg is None:\n",
    "        w_trg = n_srfpts_trg / n_points\n",
    "    assert not (n_srfpts_mdl == 0) or (w_trg == 1.0)\n",
    "    n_rsdls = 2 if do_dblsampling else 1\n",
    "\n",
    "    if eval_bs is None:\n",
    "        eval_bs = max(n_srfpts_mdl, n_srfpts_trg) * n_srf\n",
    "        \n",
    "    #########################################################\n",
    "    ########### I/O-Related Options and Operations ##########\n",
    "    #########################################################\n",
    "\n",
    "    name2dtype = dict(float64=torch.double,\n",
    "                      float32=torch.float32,\n",
    "                      float16=torch.float16)\n",
    "    tch_device = torch.device(device_name)\n",
    "    tch_dtype = name2dtype[dtype_name]\n",
    "\n",
    "    tch_dvcmdl = device_name\n",
    "    if device_name.startswith('cuda'):\n",
    "        tch_dvcmdl = torch.cuda.get_device_name(tch_device)\n",
    "\n",
    "    # Reserving 15.596 GB of memory for later usage\n",
    "    # t_gpumem = torch.cuda.get_device_properties(tch_device).total_memory\n",
    "    # tdt_elsize = torch.tensor(1).to(tch_device, tch_dtype).element_size()\n",
    "    # nuslss = int((0.90 * t_gpumem) / tdt_elsize)\n",
    "    # useless_tensor = torch.empty((nuslss,), device=tch_device, dtype=tch_dtype)\n",
    "    # del useless_tensor\n",
    "    \n",
    "    msg_ = f'\"io/mon/frq\" % \"io/avg/frq\" != 0'\n",
    "    assert iomon_period % io_avgfrq == 0, msg_\n",
    "    msg_ = f'\"io/ckpt/frq\" % \"io/avg/frq\" != 0'\n",
    "    assert chkpnt_period % io_avgfrq == 0, msg_\n",
    "    \n",
    "    do_logtb = storage_dir is not None\n",
    "    do_profile = storage_dir is not None\n",
    "    do_tchsave = storage_dir is not None\n",
    "    \n",
    "    assert not(do_logtb) or (storage_dir is not None)\n",
    "    assert not(do_profile) or (storage_dir is not None)\n",
    "    assert not(do_tchsave) or (storage_dir is not None)\n",
    "\n",
    "    #########################################################\n",
    "    ########### Constructing the Batch RNG Object ###########\n",
    "    #########################################################\n",
    "    n_seeds = len(rng_seed_list)\n",
    "    rng_seeds = np.array(rng_seed_list)\n",
    "    rng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "                   device=tch_device, dtype=tch_dtype,\n",
    "                   unif_cache_cols=1_000_000,\n",
    "                   norm_cache_cols=5_000_000)\n",
    "    rng.seed(np.broadcast_to(rng_seeds, rng.shape))\n",
    "    erng = rng\n",
    "\n",
    "    #########################################################\n",
    "    ########## Defining the Poisson Problem Object ##########\n",
    "    #########################################################\n",
    "    assert prob_type == 'poisson'\n",
    "\n",
    "    msg_ = f'chrg_dstr = {chrg_dstr} is not available/implemented.'\n",
    "    assert chrg_dstr in ('dmm',), msg_\n",
    "\n",
    "    trns_opts = dict(dim=dim, chrg_n=chrg_n, sqrt=np.sqrt)\n",
    "\n",
    "    # The poisson delta charge weights\n",
    "    if chrg_w_dstr == 'fixed':\n",
    "        chrg_w_0 = get_arr('chrg_w', '(chrg_n,)', \n",
    "            {**trns_opts, 'chrg_w': chrg_w_})\n",
    "        chrg_w = np.broadcast_to(chrg_w_0[None, ...],\n",
    "                                 (n_seeds, chrg_n)).copy()\n",
    "        assert chrg_w.shape == (n_seeds, chrg_n)\n",
    "    else:\n",
    "        raise ValueError(f'chrg_w_dstr={chrg_w_dstr} '\n",
    "                         'not implemented.')\n",
    "\n",
    "    # The poisson delta charge locations\n",
    "    if chrg_mu_dstr == 'fixed':\n",
    "        chrg_mu_0 = get_arr('chrg_mu', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'chrg_mu': chrg_mu_})\n",
    "        chrg_mu = np.broadcast_to(chrg_mu_0[None, ...],\n",
    "                                  (n_seeds, chrg_n, dim)).copy()\n",
    "        assert chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "    elif chrg_mu_dstr == 'uniform':\n",
    "        chrg_mu_low_0 =  get_arr('chrg_mu_low', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'chrg_mu_low': chrg_mu_low_})\n",
    "        chrg_mu_low = np.broadcast_to(chrg_mu_low_0[None, ...],\n",
    "            (n_seeds, chrg_n, dim)).copy()\n",
    "        assert chrg_mu_low.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        chrg_mu_high_0 =  get_arr('chrg_mu_high', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'chrg_mu_high': chrg_mu_high_})\n",
    "        chrg_mu_high = np.broadcast_to(chrg_mu_high_0[None, ...],\n",
    "            (n_seeds, chrg_n, dim)).copy()\n",
    "        assert chrg_mu_high.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        rnds = rng.uniform((n_seeds, chrg_n, dim)).detach().cpu().numpy()\n",
    "        chrg_mu = chrg_mu_low + rnds * (chrg_mu_high - chrg_mu_low)\n",
    "        assert chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "    elif chrg_mu_dstr == 'normal':\n",
    "        chrg_mu_loc_0 =  get_arr('chrg_mu_loc', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'chrg_mu_loc': chrg_mu_loc_})\n",
    "        chrg_mu_loc = np.broadcast_to(chrg_mu_loc_0[None, ...],\n",
    "            (n_seeds, chrg_n, dim)).copy()\n",
    "        assert chrg_mu_loc.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        chrg_mu_scale_0 =  get_arr('chrg_mu_scale', '(chrg_n,)', \n",
    "            {**trns_opts, 'chrg_mu_scale': chrg_mu_scale_})\n",
    "        chrg_mu_scale = np.broadcast_to(chrg_mu_scale_0[None, ...],\n",
    "            (n_seeds, chrg_n)).copy()\n",
    "        assert chrg_mu_scale.shape == (n_seeds, chrg_n)\n",
    "        \n",
    "        rnds = rng.normal((n_seeds, chrg_n, dim)).detach().cpu().numpy()\n",
    "        chrg_mu = chrg_mu_loc + rnds * chrg_mu_scale.reshape(n_seeds, chrg_n, 1)\n",
    "        assert chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "    elif chrg_mu_dstr == 'ball':\n",
    "        chrg_mu_c_0 =  get_arr('chrg_mu_c', '(chrg_n, dim)', \n",
    "            {**trns_opts, 'chrg_mu_c': chrg_mu_c_})\n",
    "        chrg_mu_c = np.broadcast_to(chrg_mu_c_0[None, ...],\n",
    "            (n_seeds, chrg_n, dim)).copy()\n",
    "        assert chrg_mu_c.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        chrg_mu_r_0 =  get_arr('chrg_mu_r', '(chrg_n,)', \n",
    "            {**trns_opts, 'chrg_mu_r': chrg_mu_r_})\n",
    "        chrg_mu_r = np.broadcast_to(chrg_mu_r_0[None, ...],\n",
    "            (n_seeds, chrg_n)).copy()\n",
    "        assert chrg_mu_r.shape == (n_seeds, chrg_n)\n",
    "        \n",
    "        rnds1 = rng.normal((n_seeds, chrg_n, dim)).detach().cpu().numpy()\n",
    "        rnds1_tilde = rnds1 / np.sqrt((rnds1**2).sum(axis=-1, keepdims=True))\n",
    "        assert rnds1_tilde.shape == (n_seeds, chrg_n, dim)\n",
    "        \n",
    "        rnds2 = rng.uniform((n_seeds, chrg_n)).detach().cpu().numpy()\n",
    "        rnds2_tilde = rnds2 ** (1.0 / dim)\n",
    "        assert rnds2_tilde.shape == (n_seeds, chrg_n)\n",
    "        \n",
    "        rnds3_tilde = (chrg_mu_r * rnds2_tilde).reshape(n_seeds, chrg_n, 1)\n",
    "        assert rnds3_tilde.shape == (n_seeds, chrg_n, 1)\n",
    "        \n",
    "        chrg_mu = chrg_mu_c + rnds1_tilde * rnds3_tilde\n",
    "        assert chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "    else:\n",
    "        raise ValueError(f'chrg_mu_dstr={chrg_mu_dstr} '\n",
    "                         'not implemented.')\n",
    "\n",
    "    # Defining the problem object\n",
    "    problem = DeltaProblem(weights=chrg_w, locations=chrg_mu,\n",
    "        tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "    \n",
    "    #########################################################\n",
    "    ####### Defining the Initial Condition Parameters #######\n",
    "    #########################################################\n",
    "    msg_ = f'ic/dstr = {ic_dstr} is not available/implemented.'\n",
    "    assert ic_dstr in ('sphere', 'trnsrf', None), msg_\n",
    "    msg_ = '\"ic/bpp\" must be either \"bias\" or \"all\".'\n",
    "    assert ic_bpp in ('bias', 'all', None), msg_\n",
    "\n",
    "    if ic_dstr == 'sphere':\n",
    "        if ic_c_dstr == 'fixed':\n",
    "            ic_c_0_np = get_arr('ic_c', '(dim,)', \n",
    "                {**trns_opts, 'ic_c': ic_c_})\n",
    "            ic_c_np = np.broadcast_to(ic_c_0_np[None, ...], \n",
    "                (n_seeds, dim)).copy()\n",
    "            assert ic_c_np.shape == (n_seeds, dim)\n",
    "        else:\n",
    "            raise ValueError(f'ic_c_dstr={ic_c_dstr} '\n",
    "                            'not implemented.')\n",
    "\n",
    "        if ic_r_dstr == 'fixed':\n",
    "            ic_r_0_np = get_arr('ic_r', '()', \n",
    "                {**trns_opts, 'ic_r': ic_r_})\n",
    "            ic_r_np = np.broadcast_to(ic_r_0_np[None, ...], \n",
    "                (n_seeds,)).copy()\n",
    "            assert ic_r_np.shape == (n_seeds,)\n",
    "        else:\n",
    "            raise ValueError(f'ic_r_dstr={ic_r_dstr} '\n",
    "                            'not implemented.')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ic_c = torch.from_numpy(ic_c_np).to(device=tch_device, \n",
    "                dtype=tch_dtype).reshape(n_seeds, 1, dim).expand(n_seeds, ic_n, dim)\n",
    "            assert ic_c.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "            ic_r = torch.from_numpy(ic_r_np).to(device=tch_device, \n",
    "                dtype=tch_dtype).reshape(n_seeds, 1, 1).expand(n_seeds, ic_n, 1)\n",
    "            assert ic_r.shape == (n_seeds, ic_n, 1)\n",
    "    elif ic_dstr in ('trnsrf', 'trnvol', None):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'ic_dstr={ic_dstr} not implemented')\n",
    "\n",
    "    #########################################################\n",
    "    ########## Defining the Volume Sampling Object ##########\n",
    "    #########################################################\n",
    "    msg_ = f'vol/dstr = {vol_dstr} is not available/implemented.'\n",
    "    assert vol_dstr in ('ball',), msg_\n",
    "\n",
    "    if vol_c_dstr == 'uniform':\n",
    "        vol_c_low_0 = get_arr('vol_c_low', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_low': vol_c_low_})\n",
    "        vol_c_low = np.broadcast_to(vol_c_low_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_low.shape == (n_seeds, dim)\n",
    "\n",
    "        vol_c_high_0 = get_arr('vol_c_high', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_high': vol_c_high_})\n",
    "        vol_c_high = np.broadcast_to(vol_c_high_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_high.shape == (n_seeds, dim)\n",
    "        \n",
    "        vol_c_params = dict(low=vol_c_low, high=vol_c_high)\n",
    "    elif vol_c_dstr == 'normal':\n",
    "        vol_c_loc_0 = get_arr('vol_c_loc', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_loc': vol_c_loc_})\n",
    "        vol_c_loc = np.broadcast_to(vol_c_loc_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_loc.shape == (n_seeds, dim)\n",
    "\n",
    "        vol_c_scale_0 = get_arr('vol_c_scale', '()', \n",
    "            {**trns_opts, 'vol_c_scale': vol_c_scale_})\n",
    "        vol_c_scale = np.broadcast_to(vol_c_scale_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_c_scale.shape == (n_seeds,)\n",
    "        \n",
    "        vol_c_params = dict(loc=vol_c_loc, scale=vol_c_scale)\n",
    "    elif vol_c_dstr == 'ball':\n",
    "        vol_c_c_0 = get_arr('vol_c_c', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_c': vol_c_c_})\n",
    "        vol_c_c = np.broadcast_to(vol_c_c_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_c.shape == (n_seeds, dim)\n",
    "\n",
    "        vol_c_r_0 = get_arr('vol_c_r', '()', \n",
    "            {**trns_opts, 'vol_c_r': vol_c_r_})\n",
    "        vol_c_r = np.broadcast_to(vol_c_r_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_c_r.shape == (n_seeds,)\n",
    "        \n",
    "        vol_c_params = dict(c=vol_c_c, r=vol_c_r)\n",
    "    else:\n",
    "        raise ValueError(f'vol_c_dstr={vol_c_dstr} not implemented.')\n",
    "\n",
    "    if vol_r_dstr in ('uniform', 'unifdpow'):\n",
    "        vol_r_low_0 = get_arr('vol_r_low', '()', \n",
    "            {**trns_opts, 'vol_r_low': vol_r_low_, \n",
    "             'vol_r_high': vol_r_high_})\n",
    "        vol_r_low = np.broadcast_to(vol_r_low_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_r_low.shape == (n_seeds,)\n",
    "\n",
    "        vol_r_high_0 = get_arr('vol_r_high', '()', \n",
    "            {**trns_opts, 'vol_r_low': vol_r_low_, \n",
    "             'vol_r_high': vol_r_high_})\n",
    "        vol_r_high = np.broadcast_to(vol_r_high_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_r_high.shape == (n_seeds,)\n",
    "        \n",
    "        vol_r_params = dict(low=vol_r_low, high=vol_r_high)\n",
    "    else:\n",
    "        raise ValueError(f'vol_r_dstr={vol_r_dstr} not implemented.')\n",
    "\n",
    "    volsampler = BallSampler(c_dstr=vol_c_dstr, c_params=vol_c_params,\n",
    "                             r_dstr=vol_r_dstr, r_params=vol_r_params,\n",
    "                             batch_rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    srfsampler = SphereSampler(batch_rng=rng)\n",
    "    \n",
    "    srftnsfrm_params = {'dstr': srf_tnsfrm}\n",
    "    if srf_tnsfrm == 'cube2sphr':\n",
    "        cube2sphr = Cube2Sphere(n_cdfint=n_cdfint, dim=dim, \n",
    "            tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "        srftnsfrm_params['cube2sphr'] = cube2sphr\n",
    "        rv_dim = dim - 1\n",
    "    elif srf_tnsfrm == 'normscale':\n",
    "        cube2sphr, rv_dim = None, dim\n",
    "    else:\n",
    "        msg_ = f'srf_tnsfrm=\"{srf_tnsfrm}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "        \n",
    "    if (srf_tnsfrm == 'cube2sphr') and (srf_samp in ['quad', 'qmc']):\n",
    "        cpy_dist = chaospy.Iid(chaospy.Uniform(0, 1), dim-1)\n",
    "    elif (srf_tnsfrm == 'normscale') and (srf_samp in ['quad', 'qmc']):\n",
    "        cpy_dist = chaospy.Iid(chaospy.Normal(0, 1), dim)\n",
    "    elif srf_samp in ['quad', 'qmc']:\n",
    "        msg_ = f'srf_tnsfrm=\"{srf_tnsfrm}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    if srf_samp == 'quad':\n",
    "        qx_np, quadw_np = chaospy.generate_quadrature(\n",
    "            order=quad_order, dist=cpy_dist, rule=quad_rule, \n",
    "            recurrence_algorithm=quad_rcuralg, sparse=quad_sprs)\n",
    "    elif srf_samp == 'qmc':\n",
    "        cpy_jac = chaospy.J(cpy_dist)\n",
    "        qx_np = cpy_jac.sample(n_points, rule=qmc_rule, \n",
    "            antithetic=qmc_antithetic, include_axis_dim=True)\n",
    "    elif srf_samp in ['grid', 'rng']:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError('not implemented yet')\n",
    "    \n",
    "    if (srf_tnsfrm == 'cube2sphr') and (srf_samp in ['quad', 'qmc']):\n",
    "        assert np.isclose(min(0.0, qx_np.min()), 0.0)\n",
    "        assert np.isclose(max(1.0, qx_np.max()), 1.0)\n",
    "        qx_np_ = np.clip(qx_np, 0.0, 1.0)\n",
    "    elif (srf_tnsfrm == 'normscale') and (srf_samp in ['quad', 'qmc']):\n",
    "        # Important to avoid all zeros scenario that cannot be normalized\n",
    "        qx_np_ = qx_np + 1e-12\n",
    "        assert (np.square(qx_np_).sum(axis=0) > 0.0).all()\n",
    "    elif srf_samp in ['quad', 'qmc']:\n",
    "        raise ValueError('not implemented yet')\n",
    "    \n",
    "    if srf_samp == 'quad':\n",
    "        n_quad = quadw_np.size\n",
    "        msg_ =  f'The number of quadrature points {n_quad} should be the '\n",
    "        msg_ += f'same as the sum of \"srfpts/n/mdl\"={n_srfpts_mdl} and '\n",
    "        msg_ += f'\"srfpts/n/trg\"={n_srfpts_trg}.'\n",
    "        assert n_quad == n_points, msg_\n",
    "        quad_x = torch.from_numpy(qx_np_.T).to(device=tch_device, dtype=tch_dtype)\n",
    "        assert quad_x.shape == (n_quad, rv_dim)\n",
    "        quad_w = torch.from_numpy(quadw_np).to(device=tch_device, dtype=tch_dtype) * n_quad\n",
    "        assert quad_w.shape == (n_quad,)\n",
    "        srfsamp_params = {'dstr': 'quad', 'x': quad_x, 'w': quad_w}\n",
    "    elif srf_samp == 'qmc':\n",
    "        qmc_x = torch.from_numpy(qx_np_.T).to(device=tch_device, dtype=tch_dtype)\n",
    "        assert qmc_x.shape == (n_points, rv_dim), f'{qmc_x.shape} != {(n_points, rv_dim)}'\n",
    "        srfsamp_params = {'dstr': 'qmc', 'x': qmc_x}\n",
    "    elif srf_samp in ['grid', 'rng']:\n",
    "        srfsamp_params = {'dstr': srf_samp}\n",
    "    else:\n",
    "        raise ValueError('not implemented yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    #### Evaluation Param Tensorization and Sanitization ####\n",
    "    #########################################################\n",
    "\n",
    "    # The following evaluates the 'eval/*' options and creates \n",
    "    # array parameters for evaluation.\n",
    "    # The input is mainly the `evalcfgs` dictionary, which holds \n",
    "    # some keys and list/string values. The output will be the \n",
    "    # `evalprms` dictionary which has the same keys but with \n",
    "    # np.array values.\n",
    "    # --------------\n",
    "    # Example input: \n",
    "    #   evalcfgs = {'ur': {'dstr': 'uniform', \n",
    "    #       'low': [0], \n",
    "    #       'high': '[sqrt(dim)]'}}\n",
    "    # --------------\n",
    "    # Example output\n",
    "    #   evalprms = {'ur': {'dstr': 'uniform',\n",
    "    #       'low' : torch.tensor([0]).expand(n_seeds, dim)\n",
    "    #       'high': torch.tensor(np.sqrt(dim)).expand(n_seeds, dim))}}\n",
    "\n",
    "    dstr2shapes = {'uniform': {'low':  '(dim,)', \n",
    "                               'high': '(dim,)'},\n",
    "                   'grid':    {'low':  '(dim,)', \n",
    "                               'high': '(dim,)'},\n",
    "                   'ball':    {'c':    '(dim,)', \n",
    "                               'r':    '()'    },\n",
    "                   'trnvol': {}}\n",
    "\n",
    "    assert all('dstr' in eopts for eopts in evalcfgs.values())\n",
    "    assert all('frq'  in eopts for eopts in evalcfgs.values())\n",
    "    assert all('n'    in eopts for eopts in evalcfgs.values())\n",
    "    evalprms = odict()\n",
    "    for eid, eopts_ in evalcfgs.items():\n",
    "        eopts = eopts_.copy()\n",
    "        eparam = odict()\n",
    "        for eopt in ('dstr', 'n', 'frq'):\n",
    "            eparam[eopt] = eopts.pop(eopt)\n",
    "\n",
    "        edstr = eparam['dstr']\n",
    "        msg_  = f'Unknown eval \"{eid}\" dstr -> \"{edstr}\". '\n",
    "        msg_ += f'dstr should be one of {dstr2shapes.keys()}.'\n",
    "        assert edstr in dstr2shapes, msg_\n",
    "\n",
    "        estore_dflt = (edstr == 'grid')\n",
    "        eparam['store'] = eopts.pop('store', estore_dflt)\n",
    "\n",
    "        opts2shape = dstr2shapes[edstr]\n",
    "        for eopt, eoptshpstr in opts2shape.items():\n",
    "            # Example: edstr = 'uniform'\n",
    "            #          eopt = 'low'\n",
    "            #          eoptshpstr = '(dim,)'\n",
    "            #          eoptshp = (dim,)\n",
    "            #          eoptval = \"[sqrt(dim)]\"\n",
    "            #          eopt_pnp0 = np.array([sqrt(dim)]*dim)\n",
    "            #          eopt_pnp0.shape = (dim,)\n",
    "            #          eopt_pnp = eopt_pnp0.expand(n_seeds, dim)\n",
    "            #          eopt_pnp.shape = (n_seeds, dim)\n",
    "            #          eopt_p = torch.from_numpy(eopt_pa0)\n",
    "            #          eopt_p.shape = (n_seeds, dim)\n",
    "            msg_  = f'\"eval/{eid}/{eopt}\" must be fixed and determined.'\n",
    "            msg_ += f' Hierarchical support not available yet.'\n",
    "            assert eopt in eopts, msg_\n",
    "\n",
    "            eoptval = eopts.pop(eopt)\n",
    "            etrns = {eopt: eoptval, 'dim': dim, \n",
    "                     'sqrt': np.sqrt}\n",
    "\n",
    "            eopt_pnp0 = get_arr(eopt, eoptshpstr, etrns)        \n",
    "            eoptshp = eval_formula(eoptshpstr, {'dim': dim})\n",
    "            eopt_pnp = np.broadcast_to(eopt_pnp0[None, ...],\n",
    "                                       (n_seeds, *eoptshp)).copy()\n",
    "            assert eopt_pnp.shape == (n_seeds, *eoptshp)\n",
    "            eopt_pc = torch.from_numpy(eopt_pnp)\n",
    "            eopt_p = eopt_pc.to(device=tch_device, dtype=tch_dtype)\n",
    "            assert eopt_p.shape == (n_seeds, *eoptshp)\n",
    "            eparam[eopt] = eopt_p\n",
    "        \n",
    "        if edstr in ('trnvol', 'ball'):\n",
    "            for eopt in ('rqnts/dstr', 'rqnts/n', 'use_crn',\n",
    "                         'rx/dstr', 'rx/static', 'rx/r/dstr', \n",
    "                         'rx/r/n', 'rx/r/static', 'rx/x/dstr', \n",
    "                         'rx/x/static'):\n",
    "                if eopt in eopts:\n",
    "                    eparam[eopt] = eopts.pop(eopt)\n",
    "        else:\n",
    "            for eopt in ('rx/dstr', 'rx/static'):\n",
    "                if eopt in eopts:\n",
    "                    eparam[eopt] = eopts.pop(eopt)\n",
    "\n",
    "        assert len(eopts) == 0, f'unused eval items left: {eopts}'\n",
    "        evalprms[eid] = eparam\n",
    "\n",
    "    #########################################################\n",
    "    ############# Evaluation Parameter Creation #############\n",
    "    #########################################################\n",
    "    for eid, eopts in evalprms.items():\n",
    "        edstr = eopts['dstr']\n",
    "        n_evlpnts = eopts['n']\n",
    "        if edstr == 'uniform':\n",
    "            e_low_ = eopts['low']\n",
    "            assert e_low_.shape == (n_seeds, dim)\n",
    "            e_low = e_low_.unsqueeze(dim=-2)\n",
    "            assert e_low.shape == (n_seeds, 1, dim)\n",
    "\n",
    "            e_high_ = eopts['high']\n",
    "            assert e_high_.shape == (n_seeds, dim)\n",
    "            e_high = e_high_.unsqueeze(dim=-2)\n",
    "            assert e_high.shape == (n_seeds, 1, dim)\n",
    "\n",
    "            e_slope = e_high - e_low\n",
    "            assert e_slope.shape == (n_seeds, 1, dim)\n",
    "\n",
    "            eopts['bias'] = e_low\n",
    "            eopts['slope'] = e_slope\n",
    "        elif edstr == 'ball':\n",
    "            e_c_ = eopts['c']\n",
    "            assert e_c_.shape == (n_seeds, dim)\n",
    "            e_c = e_c_.unsqueeze(dim=-2).expand(n_seeds, n_evlpnts, dim)\n",
    "            assert e_c.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "            e_r_ = eopts['r']\n",
    "            assert e_r_.shape == (n_seeds,)\n",
    "\n",
    "            e_r = e_r_.reshape(n_seeds, 1, 1).expand(n_seeds, n_evlpnts, 1)\n",
    "            assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "            eopts['c_xpnd'] = e_c\n",
    "            eopts['r_xpnd'] = e_r\n",
    "        elif edstr == 'trnvol':\n",
    "            pass\n",
    "        elif edstr == 'grid':\n",
    "            n_g = eopts['n']\n",
    "            n_gpd = int(np.ceil(n_g**(1./dim)))\n",
    "            assert n_g == (n_gpd ** dim)\n",
    "\n",
    "            elowt = eopts['low']\n",
    "            assert elowt.shape == (n_seeds, dim)\n",
    "\n",
    "            ehight = eopts['high']\n",
    "            assert ehight.shape == (n_seeds, dim)\n",
    "\n",
    "            assert (elowt[:1] == elowt).all()\n",
    "            assert (ehight[:1] == ehight).all()\n",
    "\n",
    "            elow = elowt.cpu().detach().numpy()[0].tolist()\n",
    "            ehigh = ehight.cpu().detach().numpy()[0].tolist()\n",
    "\n",
    "            gdict = make_grid(elow, ehigh, dim, n_gpd, 'torch')\n",
    "            e_pnts_ = gdict['x']\n",
    "            assert e_pnts_.shape == (n_g, dim)\n",
    "\n",
    "            e_pnts = e_pnts_.reshape(1, n_g, dim).expand(n_seeds, n_g, dim)\n",
    "            assert e_pnts.shape == (n_seeds, n_g, dim)\n",
    "\n",
    "            eopts['pnts'] = e_pnts.to(tch_device, tch_dtype)\n",
    "            eopts['xi_msh_np'] = gdict['xi_msh_np']\n",
    "        else:\n",
    "            raise ValueError(f'\"{edstr}\" not defined')\n",
    "        \n",
    "        if edstr in ('ball', 'trnvol'):\n",
    "            # `iscrstatic == True` means that the `e_pntcs` and `e_pntrs` must \n",
    "            # be sampled from the `(c_xpnd, r_xpnd)` ball using the RNG \n",
    "            # dynamically through the training.            \n",
    "            \n",
    "            # Legacy config conversion to the latest form\n",
    "            e_rqntsdstrold_ = eopts.get('rqnts/dstr', None)\n",
    "            e_usecrn_ = eopts.get('use_crn', None)\n",
    "            if e_rqntsdstrold_ is None:\n",
    "                pass\n",
    "            elif e_rqntsdstrold_ == 'det':\n",
    "                assert 'rx/dstr' not in eopts\n",
    "                eopts['rx/dstr'] = 'indep'\n",
    "                assert 'rx/r/dstr' not in eopts\n",
    "                eopts['rx/r/dstr'] = 'det'\n",
    "                eopts['rx/r/n'] = eopts['rqnts/n']\n",
    "                eopts['rx/x/dstr'] = 'iid'\n",
    "                eopts['rx/x/static'] = e_usecrn_ not in (False, None)\n",
    "            elif e_rqntsdstrold_ == 'iid':\n",
    "                eopts['rx/dstr'] = 'joint'\n",
    "                eopts['rx/static'] = e_usecrn_ not in (False, None)\n",
    "            else:\n",
    "                raise ValueError(f'rqnts/dstr={e_rqntsdstrold_} not implmntd')\n",
    "            \n",
    "            e_rxdstr = eopts.get('rx/dstr', 'joint')\n",
    "            if e_rxdstr == 'joint':\n",
    "                e_isrxstatic = eopts.get('rx/static', False)\n",
    "                \n",
    "                e_iscrstatic, e_isthstatic = False, False\n",
    "                # Don't worry! The static points will be generated and \n",
    "                # memorized in the first epoch. No need to reimplement \n",
    "                # all the routines here as well.\n",
    "            elif e_rxdstr == 'indep':\n",
    "                erqnt_type = eopts.get('rx/r/dstr')\n",
    "                if erqnt_type == 'det':\n",
    "                    e_iscrstatic = True\n",
    "                elif erqnt_type == 'iid':\n",
    "                    e_iscrstatic = eopts['rx/r/static']\n",
    "                else:\n",
    "                    raise ValueError(f'rx/r/dstr={erqnt_type} not implmntd')\n",
    "                \n",
    "                e_thdstr = eopts.get('rx/x/dstr')\n",
    "                if e_thdstr == 'iid':\n",
    "                    e_isthstatic = eopts.get('rx/x/static')\n",
    "                else:\n",
    "                    raise ValueError(f'rx/r/dstr={e_thdstr} not implmntd')\n",
    "                \n",
    "                n_r_ = eopts['rx/r/n']\n",
    "                n_th_ = n_evlpnts // n_r_\n",
    "                msg_ == 'to match the random effects, we must have (n_evlpnts mod rqnts/n == 0)'\n",
    "                assert n_evlpnts % n_r_ == 0, msg_\n",
    "            else:\n",
    "                raise ValueError(f'rx/dstr={e_rxdstr} is not implmntd')\n",
    "            \n",
    "            eopts['iscrstatic'] = e_iscrstatic\n",
    "            eopts['isthstatic'] = e_isthstatic\n",
    "            \n",
    "            if e_rxdstr == 'joint':\n",
    "                e_pntcs, e_pntrs, ethtilde = None, None, None\n",
    "                \n",
    "            if (e_rxdstr == 'indep') and e_iscrstatic and (edstr == 'trnvol'):\n",
    "                ####################################################\n",
    "                ### Step (1): Finding the mean ball-center point ###\n",
    "                ####################################################\n",
    "                vol_cdstr = volsampler.c_dstr\n",
    "                if vol_cdstr == 'ball':\n",
    "                    e_pntcs_ = volsampler.c_cntr\n",
    "                    assert e_pntcs_.shape == (n_seeds, 1, dim)\n",
    "                elif vol_cdstr == 'normal':\n",
    "                    e_pntcs_ = volsampler.c_loc\n",
    "                    assert e_pntcs_.shape == (n_seeds, 1, dim)\n",
    "                elif vol_cdstr == 'uniform':\n",
    "                    msg_  = f'uniform vol centers are mathematically '\n",
    "                    msg_ += f'incompatible with static radii evaluations.'\n",
    "                    raise ValueError(msg_)\n",
    "                else:\n",
    "                    msg_ = f'Unknown volume c_dstr={vol_cdstr}'\n",
    "                    raise ValueError(msg_)\n",
    "                e_pntcs = e_pntcs_.expand(n_seeds, n_evlpnts, dim)\n",
    "                assert e_pntcs.shape == (n_seeds, n_evlpnts, dim)\n",
    "                \n",
    "                ####################################################\n",
    "                ## Step (2): Simulating TV points from Volsampler ##\n",
    "                ####################################################\n",
    "                n_sim = 10000 if erqnt_type == 'det' else n_r_\n",
    "                    \n",
    "                evols_sim = volsampler(n=n_sim)\n",
    "                assert evols_sim['type'] == 'ball'\n",
    "                e_pntcssim = evols_sim['centers']\n",
    "                assert e_pntcssim.shape == (n_seeds, n_sim, dim)\n",
    "                e_rsim_ = evols_sim['radii']\n",
    "                assert e_rsim_.shape == (n_seeds, n_sim)\n",
    "                e_rsim = e_rsim_.unsqueeze(dim=-1)\n",
    "                assert e_rsim.shape == (n_seeds, n_sim, 1)\n",
    "                euntrdsim = erng.uniform((n_seeds, n_sim, 1))\n",
    "                assert euntrdsim.shape == (n_seeds, n_sim, 1)\n",
    "                untrsim = euntrdsim.pow(1.0 / dim)\n",
    "                assert untrsim.shape == (n_seeds, n_sim, 1)\n",
    "                e_pntrssim = untrsim * e_rsim\n",
    "                assert e_pntrssim.shape == (n_seeds, n_sim, 1)\n",
    "                \n",
    "                ethetasim = erng.normal((n_seeds, n_sim, dim))\n",
    "                assert ethetasim.shape == (n_seeds, n_sim, dim)\n",
    "                e_thtildesim = ethetasim / ethetasim.norm(dim=-1, keepdim=True)\n",
    "                assert e_thtildesim.shape == (n_seeds, n_sim, dim)\n",
    "                e_pntssim = e_pntcssim + e_thtildesim * e_pntrssim\n",
    "                assert e_pntssim.shape == (n_seeds, n_sim, dim)\n",
    "                e_pntsnomeansim = e_pntssim - e_pntcs_\n",
    "                assert e_pntsnomeansim.shape == (n_seeds, n_sim, dim)\n",
    "                e_realrsim = e_pntsnomeansim.norm(p=2, dim=-1)\n",
    "                assert e_realrsim.shape == (n_seeds, n_sim)\n",
    "\n",
    "                if erqnt_type == 'iid':\n",
    "                    e_pntrs_ = e_realrsim.reshape(n_seeds, n_r_, 1).expand(n_seeds, n_r_, n_th_)\n",
    "                    assert e_pntrs_.shape == (n_seeds, n_r_, n_th_)\n",
    "                    \n",
    "                    e_pntrs = e_pntrs_.reshape(n_seeds, n_evlpnts, 1)\n",
    "                    assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "                elif erqnt_type == 'det':\n",
    "                    ####################################################\n",
    "                    ####### Step (3): Computing Radius Quantiles #######\n",
    "                    ####################################################\n",
    "                    r_qs_ = torch.arange(n_r_, device=tch_device, dtype=tch_dtype) / n_r_ + 1.0 / (2 * n_r_)\n",
    "                    assert r_qs_.shape == (n_r_,)\n",
    "                    e_rqnts_ = torch.quantile(e_realrsim, r_qs_, dim=-1)\n",
    "                    assert e_rqnts_.shape == (n_r_, n_seeds)\n",
    "                    e_rqnts = e_rqnts_.T\n",
    "                    assert e_rqnts.shape == (n_seeds, n_r_)\n",
    "                    \n",
    "                    e_pntrs_ = e_rqnts.reshape(n_seeds, n_r_, 1).expand(n_seeds, n_r_, n_th_)\n",
    "                    assert e_pntrs_.shape == (n_seeds, n_r_, n_th_)\n",
    "                    e_pntrs = e_pntrs_.reshape(n_seeds, n_evlpnts, 1)\n",
    "                    assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "                else:\n",
    "                    raise ValueError(f'erqnt_type={erqnt_type} not implmntd')                \n",
    "            elif (e_rxdstr == 'indep') and e_iscrstatic and (edstr == 'ball'):\n",
    "                e_pntcs = eopts['c_xpnd']\n",
    "                assert e_pntcs.shape == (n_seeds, n_evlpnts, dim)\n",
    "                \n",
    "                e_r_ = eopts['r']\n",
    "                assert e_r_.shape == (n_seeds,)\n",
    "                if erqnt_type == 'iid':\n",
    "                    r_qs = erng.uniform((n_seeds, n_r_, 1)).expand(n_seeds, n_r_, n_th_)\n",
    "                    assert r_qs.shape == (n_seeds, n_r_, n_th_)\n",
    "                elif erqnt_type == 'det': \n",
    "                    r_qs_ = torch.arange(n_r_, device=tch_device, dtype=tch_dtype) / n_r_ + 1.0 / (2 * n_r_)\n",
    "                    assert r_qs_.shape == (n_r_,)\n",
    "                    r_qs = r_qs_.reshape(1, n_r_, 1).expand(n_seeds, n_r_, n_th_)\n",
    "                    assert r_qs.shape == (n_seeds, n_r_, n_th_)\n",
    "                else:\n",
    "                    raise ValueError(f'erqnt_type={erqnt_type} not implmntd')\n",
    "                \n",
    "                e_pntrs_ = (r_qs ** (1.0 / dim)) * e_r_.reshape(n_seeds, 1, 1)\n",
    "                assert e_pntrs_.shape == (n_seeds, n_r_, n_th_)\n",
    "                e_pntrs = e_pntrs_.reshape(n_seeds, n_evlpnts, 1)\n",
    "                assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "            elif (e_rxdstr == 'indep') and not(e_iscrstatic):\n",
    "                e_pntcs, e_pntrs = None, None\n",
    "            elif (e_rxdstr == 'indep'):\n",
    "                raise ValueError(f'not implemeted edstr={edstr}')\n",
    "            \n",
    "            if (e_rxdstr == 'indep') and e_isthstatic and e_iscrstatic:\n",
    "                etheta_ = erng.normal((n_seeds, 1, n_th_, dim))\n",
    "                assert etheta_.shape == (n_seeds, 1, n_th_, dim)\n",
    "                \n",
    "                etheta2_ = etheta_.expand(n_seeds, n_r_, n_th_, dim)\n",
    "                assert etheta2_.shape == (n_seeds, n_r_, n_th_, dim)\n",
    "                \n",
    "                etheta = etheta2_.reshape(n_seeds, n_evlpnts, dim)\n",
    "                assert etheta.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                ethtilde = etheta / etheta.norm(dim=-1, keepdim=True)\n",
    "                assert ethtilde.shape == (n_seeds, n_evlpnts, dim)\n",
    "            elif (e_rxdstr == 'indep') and e_isthstatic and not(e_iscrstatic):                \n",
    "                etheta = erng.normal((n_seeds, n_evlpnts, dim))\n",
    "                assert etheta.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                ethtilde = etheta / etheta.norm(dim=-1, keepdim=True)\n",
    "                assert ethtilde.shape == (n_seeds, n_evlpnts, dim)   \n",
    "            elif (e_rxdstr == 'indep'):\n",
    "                ethtilde = None\n",
    "                \n",
    "            eopts['pnt_c'] = e_pntcs\n",
    "            eopts['pnt_r'] = e_pntrs\n",
    "            eopts['thtilde'] = ethtilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e371f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    #### Collecting the Config Columns in the Dataframe #####\n",
    "    #########################################################\n",
    "    # Identifying the hyper-parameter from etc config columns\n",
    "    hppats = ['problem', 'dim', 'vol/n', 'srfpts/n/mdl', \n",
    "        'srfpts/n/trg',  'srfpts/detspc', 'srfpts/dblsmpl',\n",
    "        'trg/w', 'trg/btstrp', 'trg/tau', 'trg/reg/w', 'opt/lr', \n",
    "        'opt/dstr',  'nn/*', 'chrg/*', 'ic/*', 'vol/*', 'eval/*', \n",
    "        'srfpts/trnsfrm/*', 'srfpts/samp/*']\n",
    "    etcpats = ['desc', 'date', 'opt/epoch', 'rng_seed/list', 'io/*']\n",
    "\n",
    "    hpopts = [x for pat in hppats for x in \n",
    "               fnmatch.filter(cfg_dict_input.keys(), pat)]\n",
    "    etcopts = [x for pat in etcpats for x in \n",
    "               fnmatch.filter(cfg_dict_input.keys(), pat)]\n",
    "\n",
    "    err_list = []\n",
    "    for opt in cfg_dict_input:\n",
    "        if (opt in hpopts) and (opt in etcopts):\n",
    "            msg_ = f'\"{opt}\" should both be treated as hp and etc!'\n",
    "            err_list.append(msg_)\n",
    "        if (opt not in hpopts) and (opt not in etcopts):\n",
    "            msg_ = f'\"{opt}\" is neither hp nor etc!'\n",
    "            err_list.append(msg_)\n",
    "    if len(err_list) > 0:\n",
    "        raise RuntimeError(('\\n'+80*'*'+'\\n').join(err_list))\n",
    "\n",
    "    # Converting the list and tuples to strings\n",
    "    hp_dict_ = odict()\n",
    "    etc_dict_ = odict()\n",
    "    for opt, val in cfg_dict_input.items():\n",
    "        val = cfg_dict_input[opt]\n",
    "        if isinstance(val, (int, float, str, bool, type(None))):\n",
    "            srlval = val\n",
    "        elif isinstance(val, (list, tuple)):\n",
    "            srlval = repr(val)\n",
    "        else: \n",
    "            msg_  = f'Not sure how to log \"{opt}\" with '\n",
    "            msg_ += f'a value type of \"{type(val)}\"'\n",
    "            raise RuntimeError(msg_)\n",
    "\n",
    "        if opt in hpopts:\n",
    "            hp_dict_[opt] = srlval\n",
    "        elif opt in etcopts:\n",
    "            etc_dict_[opt] = srlval\n",
    "        else:\n",
    "            raise RuntimeError(f'Not sure how to log \"{opt}\"')\n",
    "\n",
    "    # Few exceptions for the etc directory\n",
    "    etc_dict_['hostname'] = hostname\n",
    "    etc_dict_['commit'] = commit_hash\n",
    "    etc_dict_['date/cfg'] = etc_dict_.pop('date')\n",
    "    etc_dict_['date/run'] = dtnow\n",
    "    etc_dict_['io/dvc_mdl'] = tch_dvcmdl\n",
    "    etc_dict_.pop('io/results_dir')\n",
    "    etc_dict_.pop('io/storage_dir')\n",
    "\n",
    "    # Repeating the values by n_seeds\n",
    "    hp_dict = odict()\n",
    "    for opt, val in hp_dict_.items():\n",
    "        hp_dict[opt] = [val] * n_seeds\n",
    "    etc_dict = odict()\n",
    "    for opt, val in etc_dict_.items():\n",
    "        etc_dict[opt] = [val] * n_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2f04f",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21829bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if results_dir is not None:\n",
    "        pathlib.Path(os.sep.join([results_dir, cfg_tree])\n",
    "                     ).mkdir(parents=True, exist_ok=True)\n",
    "    if storage_dir is not None:\n",
    "        cfgstrgpnt_dir = os.sep.join([storage_dir, cfg_tree, cfg_name])\n",
    "        pathlib.Path(cfgstrgpnt_dir).mkdir(parents=True, exist_ok=True)\n",
    "        strgidx = sum(isdir(f'{cfgstrgpnt_dir}/{x}') for x in os.listdir(cfgstrgpnt_dir))\n",
    "        dtnow_ = dtnow[2:].replace('-', '').replace(':', '').replace('.', '')\n",
    "        cfgstrg_dir = f'{cfgstrgpnt_dir}/{strgidx:02d}_{dtnow_}'\n",
    "        pathlib.Path(cfgstrg_dir).mkdir(parents=True, exist_ok=True)\n",
    "    if do_logtb:\n",
    "        if 'tbwriter' in locals():\n",
    "            tbwriter.close()\n",
    "        tbwriter = tensorboardX.SummaryWriter(cfgstrg_dir)\n",
    "    if do_profile:\n",
    "        profiler = Profiler()\n",
    "        profiler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914234e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initializing the model\n",
    "    model = bffnn(dim, nn_width, nn_hidden, nn_act, (n_seeds,), rng)\n",
    "    if do_bootstrap:\n",
    "        target = bffnn(dim, nn_width, nn_hidden, nn_act, (n_seeds,), rng)\n",
    "        target.load_state_dict(model.state_dict())\n",
    "    else:\n",
    "        target = model\n",
    "\n",
    "    # Set the optimizer\n",
    "    if opt_type == 'adam':\n",
    "        opt = torch.optim.Adam(model.parameters(), lr)\n",
    "    elif opt_type == 'sgd':\n",
    "        opt = torch.optim.SGD(model.parameters(), lr)\n",
    "    else:\n",
    "        raise NotImplementedError(f'opt/dstr=\"{opt_type}\" not implmntd')\n",
    "\n",
    "    # Evaluation tools\n",
    "    last_perfdict = dict()\n",
    "    ema = EMA(gamma=0.999, gamma_sq=0.998)\n",
    "    trn_sttime = time.time()\n",
    "\n",
    "    # Data writer construction\n",
    "    hdfpth = None\n",
    "    if results_dir is not None:\n",
    "        hdfpth = f'{results_dir}/{cfg_tree}/{cfg_name}.h5'\n",
    "    avg_history = odict()\n",
    "    dwriter = DataWriter(flush_period=ioflsh_period*n_seeds, \n",
    "                         compression_level=io_cmprssnlvl)\n",
    "\n",
    "    if storage_dir is not None:\n",
    "        with plt.style.context('default'):\n",
    "            figax_list = [plt.subplots(1, 1, figsize=(3.2, 2.5), dpi=100) for _ in range(3)]\n",
    "            (fig_mdl, ax_mdl), (fig_trg, ax_trg), (fig_gt, ax_gt) = figax_list\n",
    "            cax_list = [make_axes_locatable(ax).append_axes('right', size='5%', pad=0.05) \n",
    "                        for ax in (ax_mdl, ax_trg, ax_gt)]\n",
    "            cax_mdl, cax_trg, cax_gt = cax_list\n",
    "        stat_history = defaultdict(list)\n",
    "        model_history = odict()\n",
    "        target_history = odict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2721fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    for epoch in range(n_epochs+1):        \n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Sampling the volumes\n",
    "        volsamps = volsampler(n=n_srf)\n",
    "\n",
    "        # Sampling the points from the srferes\n",
    "        srfsamps = srfsampler(volsamps, n_points, trnsfrm_params=srftnsfrm_params, \n",
    "            samp_params=srfsamp_params, do_randrots=True, do_shflpts=srf_shflpts)\n",
    "        points = nn.Parameter(srfsamps['points'])\n",
    "        weights = srfsamps['weights']\n",
    "        surfacenorms = srfsamps['normals']\n",
    "        areas = srfsamps['areas']\n",
    "        assert points.shape == (n_seeds, n_srf, n_points, dim)\n",
    "        assert surfacenorms.shape == (n_seeds, n_srf, n_points, dim)\n",
    "        assert weights.shape == (n_seeds, n_srf, n_points)\n",
    "        assert areas.shape == (n_seeds, n_srf)\n",
    "\n",
    "        points_mdl = points[:, :, :n_srfpts_mdl, :]\n",
    "        assert points_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "        points_trg = points[:, :, n_srfpts_mdl:, :]\n",
    "        assert points_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "        surfacenorms_mdl = surfacenorms[:, :, :n_srfpts_mdl, :]\n",
    "        assert surfacenorms_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "        surfacenorms_trg = surfacenorms[:, :, n_srfpts_mdl:, :]\n",
    "        assert surfacenorms_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "        \n",
    "        weights_mdl = weights[:, :, :n_srfpts_mdl]\n",
    "        assert weights_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl)\n",
    "        weights_trg = weights[:, :, n_srfpts_mdl:]\n",
    "        assert weights_trg.shape == (n_seeds, n_srf, n_srfpts_trg)\n",
    "\n",
    "        # Making surface integral predictions using the reference model\n",
    "        u_mdl = model(points_mdl)\n",
    "        assert u_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, 1)\n",
    "        nabla_x_u_mdl, = torch.autograd.grad(u_mdl.sum(), [points_mdl],\n",
    "            grad_outputs=None, retain_graph=True, create_graph=True,\n",
    "            only_inputs=True, allow_unused=False)\n",
    "        assert nabla_x_u_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "        normprods_mdl = (nabla_x_u_mdl * surfacenorms_mdl).sum(dim=-1)\n",
    "        assert normprods_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl)\n",
    "        wnormprods_mdl = normprods_mdl * weights_mdl\n",
    "        assert wnormprods_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl)\n",
    "        if n_srfpts_mdl > 0:\n",
    "            mean_wnormprods_mdl = wnormprods_mdl.mean(dim=-1, keepdim=True)\n",
    "            assert mean_wnormprods_mdl.shape == (n_seeds, n_srf, 1)\n",
    "        else:\n",
    "            mean_wnormprods_mdl = 0.0\n",
    "\n",
    "        # Making surface integral predictions using the target model\n",
    "        u_trg = target(points_trg)\n",
    "        assert u_trg.shape == (n_seeds, n_srf, n_srfpts_trg, 1)\n",
    "        nabla_x_u_trg, = torch.autograd.grad(u_trg.sum(), [points_trg],\n",
    "            grad_outputs=None, retain_graph=True, create_graph=not(do_bootstrap),\n",
    "            only_inputs=True, allow_unused=False)\n",
    "        assert nabla_x_u_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "        normprods_trg = (nabla_x_u_trg * surfacenorms_trg).sum(dim=-1)\n",
    "        assert normprods_trg.shape == (n_seeds, n_srf, n_srfpts_trg)\n",
    "        wnormprods_trg = normprods_trg * weights_trg\n",
    "        assert wnormprods_trg.shape == (n_seeds, n_srf, n_srfpts_trg)\n",
    "        if do_dblsampling:\n",
    "            assert n_rsdls == 2\n",
    "\n",
    "            mean_wnormprods_trg1 = wnormprods_trg[..., 0::2].mean(\n",
    "                dim=-1, keepdim=True)\n",
    "            assert mean_wnormprods_trg1.shape == (n_seeds, n_srf, 1)\n",
    "\n",
    "            mean_wnormprods_trg2 = wnormprods_trg[..., 1::2].mean(\n",
    "                dim=-1, keepdim=True)\n",
    "            assert mean_wnormprods_trg2.shape == (n_seeds, n_srf, 1)\n",
    "\n",
    "            mean_wnormprods_trg = torch.cat(\n",
    "                [mean_wnormprods_trg1, mean_wnormprods_trg2], dim=-1)\n",
    "            assert mean_wnormprods_trg.shape == (n_seeds, n_srf, n_rsdls)\n",
    "        else:\n",
    "            assert n_rsdls == 1\n",
    "\n",
    "            mean_wnormprods_trg = wnormprods_trg.mean(dim=-1, keepdim=True)\n",
    "            assert mean_wnormprods_trg.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Linearly combining the reference and target predictions\n",
    "        mean_wnormprods = (       w_trg  * mean_wnormprods_trg +\n",
    "                           (1.0 - w_trg) * mean_wnormprods_mdl)\n",
    "        assert mean_wnormprods.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Considering the surface areas\n",
    "        pred_surfintegs = mean_wnormprods * areas.reshape(n_seeds, n_srf, 1)\n",
    "        assert pred_surfintegs.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Getting the reference volume integrals\n",
    "        ref_volintegs = problem.integrate_volumes(volsamps)\n",
    "        assert ref_volintegs.shape == (n_seeds, n_srf)\n",
    "\n",
    "        # Getting the residual terms\n",
    "        resterms = pred_surfintegs - ref_volintegs.reshape(n_seeds, n_srf, 1)\n",
    "        assert resterms.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Multiplying the residual terms\n",
    "        if do_dblsampling:\n",
    "            resterms_prod = resterms.prod(dim=-1)\n",
    "            assert resterms_prod.shape == (n_seeds, n_srf)\n",
    "        else:\n",
    "            resterms_prod = torch.square(resterms).squeeze(-1)\n",
    "            assert resterms_prod.shape == (n_seeds, n_srf)\n",
    "\n",
    "        # Computing the main loss\n",
    "        loss_main = resterms_prod.mean(-1)\n",
    "        assert loss_main.shape == (n_seeds,)\n",
    "\n",
    "        if do_bootstrap:\n",
    "            with torch.no_grad():\n",
    "                u_mdl_prime = target(points_mdl)\n",
    "            loss_trgreg = torch.square(u_mdl - u_mdl_prime).mean([-3, -2, -1])\n",
    "            assert loss_trgreg.shape == (n_seeds,)\n",
    "        else:\n",
    "            loss_trgreg = torch.zeros(n_seeds, device=tch_device, dtype=tch_dtype)\n",
    "            assert loss_trgreg.shape == (n_seeds,)\n",
    "\n",
    "        # The initial condition loss\n",
    "        renew_ic = (epoch == 0) if (ic_frq == 0) else (epoch % ic_frq == 0)\n",
    "        if ic_needsampling and renew_ic and (ic_dstr is not None):\n",
    "            with torch.no_grad():\n",
    "                if ic_dstr == 'sphere':\n",
    "                    ic_normsamps =rng.normal((n_seeds, ic_n, dim))\n",
    "                    assert ic_normsamps.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_ptstilde = ic_normsamps / ic_normsamps.norm()\n",
    "                    assert ic_ptstilde.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    assert ic_c.shape == (n_seeds, ic_n, dim)\n",
    "                    assert ic_r.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    ic_allpnts = ic_c + ic_ptstilde * ic_r\n",
    "                    assert ic_allpnts.shape == (n_seeds, ic_n, dim)\n",
    "                elif ic_dstr == 'trnvol':\n",
    "                    icvols = volsampler(n=ic_n)\n",
    "                    assert icvols['type'] == 'ball'\n",
    "\n",
    "                    ic_c = icvols['centers']\n",
    "                    assert ic_c.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_r_ = icvols['radii']\n",
    "                    assert ic_r_.shape == (n_seeds, ic_n)\n",
    "\n",
    "                    ic_r = ic_r_.unsqueeze(dim=-1)\n",
    "                    assert ic_r.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    untrd = rng.uniform((n_seeds, ic_n, 1))\n",
    "                    assert untrd.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    untr = untrd.pow(1.0 / dim)\n",
    "                    assert untr.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    ic_pntrs = untr * ic_r\n",
    "                    assert ic_pntrs.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    ic_theta = rng.normal((n_seeds, ic_n, dim))\n",
    "                    assert ic_theta.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_thtilde = ic_theta / ic_theta.norm(dim=-1, keepdim=True)\n",
    "                    assert ic_thtilde.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_allpnts = ic_c + ic_thtilde * ic_pntrs\n",
    "                    assert ic_allpnts.shape == (n_seeds, ic_n, dim)\n",
    "                else:\n",
    "                    raise ValueError(f'ic/dstr={ic_dstr} not defined')\n",
    "\n",
    "                ic_allgtvs = get_prob_sol(problem, ic_allpnts, eval_bs, \n",
    "                    get_field=False, out_lib='torch')['v']\n",
    "                assert ic_allgtvs.shape == (n_seeds, ic_n)\n",
    "\n",
    "        if ic_needsampling:\n",
    "            ic_idxs = ((np.arange(ic_bs) + epoch * ic_bs) % ic_n).tolist()\n",
    "\n",
    "            ic_pnts = ic_allpnts[:, ic_idxs, :]\n",
    "            assert ic_pnts.shape == (n_seeds, ic_bs, dim)\n",
    "\n",
    "            ic_vpreds = model(ic_pnts).squeeze(dim=-1)\n",
    "            assert ic_vpreds.shape == (n_seeds, ic_bs)\n",
    "\n",
    "            ic_gtvs = ic_allgtvs[:, ic_idxs]\n",
    "            assert ic_gtvs.shape == (n_seeds, ic_bs)\n",
    "        elif (ic_dstr is not None):\n",
    "            ic_pnts = points_mdl.reshape(n_seeds, n_srf * n_srfpts_mdl, dim)\n",
    "            assert ic_pnts.shape == (n_seeds, ic_bs, dim)\n",
    "\n",
    "            ic_vpreds = u_mdl.reshape(n_seeds, n_srf * n_srfpts_mdl)\n",
    "            assert ic_vpreds.shape == (n_seeds, ic_bs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ic_gtvs = get_prob_sol(problem, ic_pnts, eval_bs, \n",
    "                    get_field=False, out_lib='torch')['v']\n",
    "            assert ic_gtvs.shape == (n_seeds, ic_bs)\n",
    "\n",
    "        if ic_bpp == 'bias':\n",
    "            mdl_bias = model.layer_last[1].squeeze(dim=-1)\n",
    "            assert mdl_bias.shape == (n_seeds, 1)\n",
    "\n",
    "            ic_vpreds = ic_vpreds.detach() - mdl_bias.detach() + mdl_bias\n",
    "            assert ic_vpreds.shape == (n_seeds, ic_bs)\n",
    "        elif ic_bpp == 'all':\n",
    "            pass\n",
    "        else:\n",
    "            raise RuntimeError(f'ic/bpp={ic_bpp} not defined')\n",
    "\n",
    "        if ic_dstr is not None:\n",
    "            loss_ic = torch.square(ic_vpreds - ic_gtvs).mean(dim=-1)\n",
    "            assert loss_ic.shape == (n_seeds,)\n",
    "        else:\n",
    "            loss_ic = torch.zeros(n_seeds, dtype=tch_dtype, device=tch_device)\n",
    "            assert loss_ic.shape == (n_seeds,)\n",
    "\n",
    "        # The total loss\n",
    "        loss = loss_main + w_trgreg * loss_trgreg + w_ic * loss_ic\n",
    "        assert loss.shape == (n_seeds,)\n",
    "\n",
    "        loss_sum = loss.sum()\n",
    "        loss_sum.backward()\n",
    "\n",
    "        # We will not update in the first epoch so that we will \n",
    "        # record the initialization statistics as well. Instead, \n",
    "        # we will update an extra epoch at the end.\n",
    "        if (epoch > 0):\n",
    "            opt.step()\n",
    "\n",
    "        # Updating the target network\n",
    "        if do_bootstrap and (epoch > 0):\n",
    "            model_sd = model.state_dict()\n",
    "            target_sd = target.state_dict()\n",
    "            newtrg_sd = dict()\n",
    "            with torch.no_grad():\n",
    "                for key, param in model_sd.items():\n",
    "                    param_trg = target_sd[key]\n",
    "                    newtrg_sd[key] = tau * param_trg + (1-tau) * param\n",
    "            target.load_state_dict(newtrg_sd)\n",
    "\n",
    "        # computing the normal product variances\n",
    "        with torch.no_grad(): \n",
    "            normprods = torch.cat([normprods_mdl, normprods_trg], dim=-1)\n",
    "            npvm = (normprods.var(dim=-1)*areas.square()).mean(-1)\n",
    "\n",
    "        # evaluating the performance of the model and target    \n",
    "        perf_dict = dict()\n",
    "        eval_strg = dict()\n",
    "        for eid, eopts in evalprms.items():\n",
    "            edstr = eopts['dstr']\n",
    "            n_evlpnts = eopts['n']\n",
    "            e_frq = eopts['frq']\n",
    "            e_store = eopts['store']\n",
    "\n",
    "            if (epoch % e_frq) > 0:\n",
    "                assert eid in last_perfdict\n",
    "                perf_dict[eid] = last_perfdict[eid]\n",
    "                continue\n",
    "\n",
    "            # Sampling the evaluation points\n",
    "            with torch.no_grad():\n",
    "                is_jointstatic = (eopts.get('rx/dstr', 'joint') == 'joint') and eopts.get('rx/static', False)\n",
    "                if is_jointstatic and (epoch > 0):\n",
    "                    e_pnts = eopts['e_pnts']\n",
    "                    assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "                elif edstr == 'uniform':\n",
    "                    e_bias = eopts['bias']\n",
    "                    assert e_bias.shape == (n_seeds, 1, dim)\n",
    "\n",
    "                    e_slope = eopts['slope']\n",
    "                    assert e_slope.shape == (n_seeds, 1, dim)\n",
    "\n",
    "                    e_unfpnts = erng.uniform((n_seeds, n_evlpnts, dim))\n",
    "                    assert e_unfpnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                    e_pnts = e_bias + e_unfpnts * e_slope\n",
    "                    assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "                elif edstr in ('ball', 'trnvol'):\n",
    "                    # Whether the point centers and radii are static\n",
    "                    e_iscrstatic = eopts['iscrstatic']\n",
    "                    if e_iscrstatic:\n",
    "                        e_pntcs = eopts['pnt_c']\n",
    "                        assert e_pntcs.shape == (n_seeds, n_evlpnts, dim)\n",
    "                        \n",
    "                        e_pntrs = eopts['pnt_r']\n",
    "                        assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "                    elif (edstr == 'ball') and not(e_iscrstatic):\n",
    "                        e_pntcs = eopts['c_xpnd']\n",
    "                        assert e_pntcs.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                        e_r = eopts['r_xpnd']\n",
    "                        assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "                        \n",
    "                        euntrd = erng.uniform((n_seeds, n_evlpnts, 1))\n",
    "                        assert euntrd.shape == (n_seeds, n_evlpnts, 1)\n",
    "                    \n",
    "                        untr = euntrd.pow(1.0 / dim)\n",
    "                        assert untr.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                        e_pntrs = untr * e_r\n",
    "                        assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "                    elif (edstr == 'trnvol') and not(e_iscrstatic):\n",
    "                        evols = volsampler(n=n_evlpnts)\n",
    "                        assert evols['type'] == 'ball'\n",
    "\n",
    "                        e_pntcs = evols['centers']\n",
    "                        assert e_pntcs.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                        e_r_ = evols['radii']\n",
    "                        assert e_r_.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "                        e_r = e_r_.unsqueeze(dim=-1)\n",
    "                        assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "                        \n",
    "                        euntrd = erng.uniform((n_seeds, n_evlpnts, 1))\n",
    "                        assert euntrd.shape == (n_seeds, n_evlpnts, 1)\n",
    "                    \n",
    "                        untr = euntrd.pow(1.0 / dim)\n",
    "                        assert untr.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                        e_pntrs = untr * e_r\n",
    "                        assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "                    else:\n",
    "                        raise RuntimeError(f'case not defined')                    \n",
    "                    \n",
    "                    # Whether the point theta-tildes are static (i.e., determined \n",
    "                    # only once at the beginning of training)\n",
    "                    e_isthstatic = eopts['isthstatic']\n",
    "                    if e_isthstatic:\n",
    "                        e_thtilde = eopts['thtilde']\n",
    "                        assert e_thtilde.shape == (n_seeds, n_evlpnts, dim)\n",
    "                    else:\n",
    "                        etheta = erng.normal((n_seeds, n_evlpnts, dim))\n",
    "                        assert etheta.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                        e_thtilde = etheta / etheta.norm(dim=-1, keepdim=True)\n",
    "                        assert e_thtilde.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                    e_pnts = e_pntcs + e_thtilde * e_pntrs\n",
    "                    assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "                elif edstr in ('grid'):\n",
    "                    e_pnts = eopts['pnts']\n",
    "                    assert e_pnts.shape == (n_seeds, n_g, dim)\n",
    "                else:\n",
    "                    raise RuntimeError(f'eval dstr \"{edstr}\" not implmntd')\n",
    "                if is_jointstatic and (epoch == 0):\n",
    "                    eopts['e_pnts'] = e_pnts.detach().clone() \n",
    "\n",
    "            # Computing the model, target and ground truth solutions\n",
    "            prob_sol = get_prob_sol(problem, e_pnts, n_eval=eval_bs, \n",
    "                get_field=False, out_lib='torch')\n",
    "\n",
    "            e_prbsol = prob_sol['v']\n",
    "            assert e_prbsol.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "            # Computing the model solution\n",
    "            with torch.no_grad():\n",
    "                mdl_sol = get_nn_sol(model, e_pnts, n_eval=eval_bs,\n",
    "                    get_field=False, out_lib='torch')\n",
    "\n",
    "            e_mdlsol = mdl_sol['v']\n",
    "            assert e_mdlsol.shape == (n_seeds, n_evlpnts)\n",
    "            \n",
    "            # Computing the target solution\n",
    "            if do_bootstrap:\n",
    "                with torch.no_grad():\n",
    "                    trg_sol = get_nn_sol(target, e_pnts, n_eval=eval_bs, \n",
    "                        get_field=False, out_lib='torch')\n",
    "\n",
    "                e_trgsol = trg_sol['v']\n",
    "                assert e_trgsol.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "            eperfs = dict()\n",
    "            eperfs['mdl'] = get_perfdict(e_pnts, e_mdlsol, e_prbsol)\n",
    "            if do_bootstrap:\n",
    "                eperfs['trg'] = get_perfdict(e_pnts, e_trgsol, e_prbsol)\n",
    "            eperfs = deep2hie(eperfs, dictcls=dict)\n",
    "            # Example: eperfs = {'mdl/pln/mse': ...,\n",
    "            #                    'mdl/pln/mae': ...,\n",
    "            #                    'mdl/bc/mse': ...,\n",
    "            #                    'mdl/bc/mae': ...,\n",
    "            #                    'mdl/slc/mse': ...,\n",
    "            #                    'mdl/slc/mae': ...,\n",
    "            #                    'trg/pln/mse': ...,\n",
    "            #                    'trg/pln/mae': ...,\n",
    "            #                    'trg/bc/mse': ...,\n",
    "            #                    'trg/bc/mae': ...,\n",
    "            #                    'trg/slc/mse': ...,\n",
    "            #                    'trg/slc/mae': ...,\n",
    "            #                   }\n",
    "            perf_dict[eid] = eperfs\n",
    "            last_perfdict[eid] = eperfs\n",
    "            \n",
    "            if do_logtb:\n",
    "                for kk, vv in eperfs.items():\n",
    "                    tbwriter.add_scalar(f'perf/{eid}/{kk}', vv.mean(), epoch)\n",
    "            \n",
    "            # Storing the evaluation results\n",
    "            if e_store:\n",
    "                e_strg = dict()\n",
    "                e_strg['sol/mdl'] = e_mdlsol\n",
    "                if do_bootstrap:\n",
    "                    e_strg['sol/trg'] = e_trgsol\n",
    "                e_strg['sol/gt'] = e_prbsol\n",
    "                if edstr != 'grid':\n",
    "                    e_strg['pnts'] = e_pnts\n",
    "                e_strg = {kk: vv.detach().cpu().numpy().astype(np.float16) \n",
    "                          for kk, vv in e_strg.items()}\n",
    "                eval_strg[eid] = e_strg\n",
    "\n",
    "                if do_logtb and (edstr == 'grid') and (dim == 2):\n",
    "                    soltd_list = [('mdl', mdl_sol, fig_mdl, ax_mdl, cax_mdl)]\n",
    "                    if do_bootstrap:\n",
    "                        soltd_list += [('trg', trg_sol, fig_trg, ax_trg, cax_trg)]\n",
    "                    soltd_list += [('gt', prob_sol, fig_gt, ax_gt, cax_gt)]\n",
    "                    for sol_t, sol_dict, fig, ax, cax in soltd_list:\n",
    "                        x1_msh_np, x2_msh_np = eopts['xi_msh_np']\n",
    "                        plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=fig, ax=ax, cax=cax)\n",
    "                        fig.set_tight_layout(True)\n",
    "                        tbwriter.add_figure(f'viz/{eid}/{sol_t}', fig, epoch)\n",
    "                    tbwriter.flush()\n",
    "         \n",
    "        # monitoring the resource utilization \n",
    "        if epoch % iomon_period == 0:\n",
    "            s_rsrc = resource.getrusage(resource.RUSAGE_SELF)\n",
    "            c_rsrc = resource.getrusage(resource.RUSAGE_CHILDREN)\n",
    "            \n",
    "            psmem = psutil.virtual_memory()\n",
    "            pscpu = psutil.cpu_times()\n",
    "            pscpuload = psutil.getloadavg()\n",
    "            mon_dict = {'cpu/mem/tot': [psmem.total] * n_seeds, \n",
    "                'cpu/mem/avail': [psmem.available] * n_seeds, \n",
    "                'cpu/mem/used': [psmem.used] * n_seeds,\n",
    "                'cpu/mem/free': [psmem.free] * n_seeds,\n",
    "                'cpu/time/user/ps': [pscpu.user] * n_seeds,\n",
    "                'cpu/time/sys/ps': [pscpu.system] * n_seeds,\n",
    "                'cpu/time/idle/ps': [pscpu.idle] * n_seeds,\n",
    "                'cpu/load/1m': [pscpuload[0]] * n_seeds,\n",
    "                'cpu/load/5m': [pscpuload[1]] * n_seeds,\n",
    "                'cpu/load/15m': [pscpuload[2]] * n_seeds,\n",
    "                'cpu/time/train': [time.time()   - trn_sttime] * n_seeds,\n",
    "                'cpu/time/sys/py':   [s_rsrc.ru_stime  + c_rsrc.ru_stime] * n_seeds,\n",
    "                'cpu/time/user/py':  [s_rsrc.ru_utime  + c_rsrc.ru_utime] * n_seeds,\n",
    "                'n_seeds': [n_seeds] * n_seeds}\n",
    "            if 'cuda' in device_name:\n",
    "                t_gpumem = torch.cuda.get_device_properties(tch_device).total_memory\n",
    "                r_gpumem = torch.cuda.memory_reserved(tch_device)\n",
    "                a_gpumem = torch.cuda.memory_allocated(tch_device)\n",
    "                f_gpumem = r_gpumem - a_gpumem\n",
    "                mon_dict.update({'gpu/mem/tot':   [t_gpumem] * n_seeds,\n",
    "                                 'gpu/mem/res':   [r_gpumem] * n_seeds,\n",
    "                                 'gpu/mem/alloc': [a_gpumem] * n_seeds,\n",
    "                                 'gpu/mem/free':  [f_gpumem] * n_seeds})\n",
    "\n",
    "        # pushing the results to the data writer\n",
    "        psld = deep2hie({'perf': perf_dict}, odict)\n",
    "        slst = [('loss/total',  loss.tolist()),\n",
    "                ('loss/main',   loss_main.tolist()),\n",
    "                ('loss/trgreg', loss_trgreg.tolist()),\n",
    "                ('loss/ic',     loss_ic.tolist()),\n",
    "                ('npvm',        npvm.tolist()),\n",
    "                *list(psld.items())]\n",
    "        stat_dict = odict(slst)\n",
    "        for stat_name, stat_vals in stat_dict.items():\n",
    "            avg_history.setdefault(stat_name, [])\n",
    "            avg_history[stat_name].append(stat_vals)\n",
    "\n",
    "        dtups = []\n",
    "        if epoch % io_avgfrq == 0:\n",
    "            avg_statlst  = [('epoch',       [epoch] * n_seeds),\n",
    "                            ('rng_seed',    rng_seeds.tolist())]\n",
    "            avg_statlst += [(name, np.stack(svl, axis=0).mean(axis=0).tolist())\n",
    "                             for name, svl in avg_history.items()]\n",
    "            avg_statdict = odict(avg_statlst)\n",
    "\n",
    "            dtups += [('hp',    hp_dict,      'pd.cat'),\n",
    "                      ('stat',  avg_statdict, 'pd.qnt'),\n",
    "                      ('etc',   etc_dict,     'pd.cat')]\n",
    "            avg_history = odict()\n",
    "            \n",
    "        for eid, e_strg in eval_strg.items():\n",
    "            msg_ =  f'eval/{eid} requires storage, thus \"eval/{eid}/frq\" '\n",
    "            msg_ += f'% \"io/avg/frq\" == 0 should hold.'\n",
    "            assert epoch % io_avgfrq == 0, msg_\n",
    "            dtups += [(f'var/eval/{eid}', e_strg, 'np.arr')]\n",
    "        \n",
    "        if epoch % iomon_period == 0:\n",
    "            assert epoch % io_avgfrq == 0\n",
    "            dtups += [('mon', mon_dict, 'pd.qnt')]\n",
    "\n",
    "        if epoch % chkpnt_period == 0:\n",
    "            assert epoch % io_avgfrq == 0\n",
    "            mdl_sdnp = {k: v.detach().cpu().numpy() \n",
    "                for k, v in model.state_dict().items()}\n",
    "            dtups += [('mdl',   mdl_sdnp, 'np.arr')]\n",
    "            if do_bootstrap:\n",
    "                trg_sdnp = {k: v.detach().cpu().numpy() \n",
    "                    for k, v in target.state_dict().items()}\n",
    "                dtups += [('trg',   trg_sdnp, 'np.arr')]\n",
    "\n",
    "        dwriter.add(data_tups=dtups, file_path=hdfpth)\n",
    "\n",
    "        # Computing the loss moving averages\n",
    "        loss_ema_mean, loss_ema_std_mean = ema('loss', loss)\n",
    "        npvm_ema_mean, npvm_ema_std_mean = ema('npvm', npvm)\n",
    "        if (epoch % 1000 == 0) and (results_dir is not None):\n",
    "            print_str = f'Epoch {epoch}, EMA loss = {loss_ema_mean:.4f}'\n",
    "            print_str += f' +/- {2*loss_ema_std_mean:.4f}'\n",
    "            print_str += f', EMA Field-Norm Product Variance = {npvm_ema_mean:.4f}'\n",
    "            print_str += f' +/- {2*npvm_ema_std_mean:.4f} ({time.time()-trn_sttime:0.1f} s)'\n",
    "            print(print_str, flush=True)\n",
    "\n",
    "        if do_logtb:\n",
    "            import logging\n",
    "            logging.getLogger(\"tensorboardX.x2num\").setLevel(logging.CRITICAL)  \n",
    "            tbwriter.add_scalar('loss/total', loss.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/main', loss_main.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/trgreg', loss_trgreg.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/ic', loss_ic.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/npvm', npvm.mean(), epoch)\n",
    "\n",
    "        if do_tchsave and (epoch % chkpnt_period == 0):\n",
    "            model_history[epoch] = deepcopy({k: v.cpu() for k, v\n",
    "                in model.state_dict().items()})\n",
    "            target_history[epoch] = deepcopy({k: v.cpu() for k, v\n",
    "                in target.state_dict().items()})\n",
    "            \n",
    "\n",
    "    if results_dir is not None:\n",
    "        print(f'Training finished in {time.time() - trn_sttime:.1f} seconds.')\n",
    "    dwriter.close()\n",
    "    if do_logtb:\n",
    "        tbwriter.flush()\n",
    "    \n",
    "    outdict = dict()\n",
    "    tchmemusage = profmem()\n",
    "    assert str(tch_device) in tchmemusage\n",
    "    if 'cuda' in device_name:\n",
    "        tch_dvcmem = torch.cuda.get_device_properties(tch_device).total_memory\n",
    "    else:\n",
    "        tch_dvcmem = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
    "    outdict['dvc/mem/alloc'] = tchmemusage[str(tch_device)]\n",
    "    outdict['dvc/mem/total'] = tch_dvcmem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig = None\n",
    "    has_grid = any(eopts.get('dstr', '') == 'grid' \n",
    "                   for eid, eopts in evalprms.items())\n",
    "    if (storage_dir is not None) and has_grid:\n",
    "        soltd_list = [('mdl', mdl_sol, fig_mdl, ax_mdl, cax_mdl)]\n",
    "        eopts = list(eopts for eid, eopts in evalprms.items()\n",
    "                     if eopts.get('dstr', '') == 'grid')[0]\n",
    "        e_pnts = eopts['pnts']\n",
    "        x1_msh_np, x2_msh_np = eopts['xi_msh_np']\n",
    "\n",
    "        n_rows, n_cols = 1, 2 + do_bootstrap\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(\n",
    "            n_cols * 3.5, n_rows * 3), dpi=72)\n",
    "        cax = None\n",
    "\n",
    "        # Computing the model, target and ground truth solutions\n",
    "        prob_sol = get_prob_sol(problem, e_pnts, n_eval=eval_bs, get_field=False)\n",
    "        with torch.no_grad():\n",
    "            mdl_sol = get_nn_sol(model, e_pnts, n_eval=eval_bs, get_field=False) \n",
    "            if do_bootstrap:\n",
    "                trg_sol = get_nn_sol(target, e_pnts, n_eval=eval_bs, get_field=False)\n",
    "\n",
    "        soltd_list = [('gt', prob_sol, axes[0], 'Ground Truth'),\n",
    "                      ('mdl', mdl_sol, axes[1], 'Prediction')]\n",
    "        if do_bootstrap:\n",
    "            soltd_list += [('trg', trg_sol, axes[2], 'Target')]\n",
    "        for sol_t, sol_dict, ax, ttl in soltd_list:\n",
    "            plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=fig, ax=ax, cax=cax)\n",
    "            ax.set_title(ttl)\n",
    "    fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b71ad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    if do_tchsave:\n",
    "        torch.save(model_history, f'{cfgstrg_dir}/ckpt_mdl.pt')\n",
    "        if do_bootstrap:\n",
    "            torch.save(target_history, f'{cfgstrg_dir}/ckpt_trg.pt')\n",
    "    if storage_dir is not None:\n",
    "        shutil.copy2(hdfpth, f'{cfgstrg_dir}/progress.h5')\n",
    "        if fig is not None:\n",
    "            fig.savefig(f'{cfgstrg_dir}/finalpred.pdf', dpi=144, bbox_inches=\"tight\")   \n",
    "    if do_profile:\n",
    "        profiler.stop()\n",
    "        html = profiler.output_html()\n",
    "        htmlpath = f'{cfgstrg_dir}/profiler.html'\n",
    "        with open(htmlpath, 'w') as fp:\n",
    "            fp.write(html.encode('ascii', errors='ignore').decode('ascii'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87913cc3",
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "source": [
    "    return outdict\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    use_argparse = True\n",
    "    if use_argparse:\n",
    "        import argparse\n",
    "        my_parser = argparse.ArgumentParser()\n",
    "        my_parser.add_argument('-c', '--configid', action='store', type=str, required=True)\n",
    "        my_parser.add_argument('-d', '--device',   action='store', type=str, required=True)\n",
    "        my_parser.add_argument('-s', '--nodesize', action='store', type=int, default=1)\n",
    "        my_parser.add_argument('-r', '--noderank', action='store', type=int, default=0)\n",
    "        my_parser.add_argument('-i', '--rsmindex', action='store', type=str, default=\"0.0\")\n",
    "        my_parser.add_argument('--dry-run', action='store_true')\n",
    "        args = my_parser.parse_args()\n",
    "        args_configid = args.configid\n",
    "        args_device_name = args.device\n",
    "        args_nodesize = args.nodesize\n",
    "        args_noderank = args.noderank\n",
    "        arsg_rsmindex = args.rsmindex\n",
    "        args_dryrun = args.dry_run\n",
    "    else:\n",
    "        args_configid = 'lvl1/lvl2/poiss2d'\n",
    "        args_device_name = 'cuda:0'\n",
    "        args_nodesize = 1\n",
    "        args_noderank = 0\n",
    "        arsg_rsmindex = 0\n",
    "        args_dryrun = True\n",
    "\n",
    "    assert args_noderank < args_nodesize\n",
    "    cfgidsplit = args_configid.split('/')\n",
    "    # Example: args_configid == 'lvl1/lvl2/poiss2d'\n",
    "    config_id = cfgidsplit[-1]\n",
    "    # Example: config_id == 'poiss2d'\n",
    "    config_tree = '/'.join(cfgidsplit[:-1])\n",
    "    # Example: config_tree == 'lvl1/lvl2'\n",
    "\n",
    "    os.makedirs(configs_dir, exist_ok=True)\n",
    "    # Example: configs_dir == '.../code_bspinn/config'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    # Example: results_dir == '.../code_bspinn/result'\n",
    "    # os.makedirs(storage_dir, exist_ok=True)\n",
    "    # Example: storage_dir == '.../code_bspinn/storage'\n",
    "    \n",
    "    if args_dryrun:\n",
    "        print('>> Running in dry-run mode', flush=True)\n",
    "\n",
    "    cfg_path_ = f'{configs_dir}/{config_tree}/{config_id}'\n",
    "    cfg_exts = [cfg_ext for cfg_ext in ['json', 'yml', 'yaml'] if exists(f'{cfg_path_}.{cfg_ext}')]\n",
    "    assert len(cfg_exts) < 2, f'found multiple {cfg_exts} extensions for {cfg_path_}'\n",
    "    assert len(cfg_exts) > 0, f'found no json or yaml config at {cfg_path_}'\n",
    "    cfg_ext = cfg_exts[0]\n",
    "    cfg_path = f'{cfg_path_}.{cfg_ext}'\n",
    "    print(f'>> Reading configuration from {cfg_path}', flush=True)\n",
    "    \n",
    "    if cfg_ext.lower() == 'json':\n",
    "        with open(cfg_path, 'r') as fp:\n",
    "            json_cfgdict = json.load(fp, object_pairs_hook=odict)\n",
    "    elif cfg_ext.lower() in ('yml', 'yaml'):\n",
    "        with open(cfg_path, 'r') as fp:\n",
    "            json_cfgdict = odict(yaml.safe_load(fp))\n",
    "    else:\n",
    "        raise RuntimeError(f'unknown config extension: {cfg_ext}')\n",
    "    \n",
    "    if args_dryrun:\n",
    "        import tempfile\n",
    "        temp_resdir = tempfile.TemporaryDirectory()\n",
    "        temp_strdir = tempfile.TemporaryDirectory()\n",
    "        print(f'>> [dry-run] Temporary results dir placed at {temp_resdir.name}')\n",
    "        print(f'>> [dry-run] Temporary storage dir placed at {temp_strdir.name}')\n",
    "        results_dir = temp_resdir.name\n",
    "        storage_dir = temp_strdir.name\n",
    "        \n",
    "        dr_maxfrq = 10\n",
    "        dr_opts = {'opt/epoch': dr_maxfrq, 'io/cmprssn_lvl': 0}\n",
    "        for opt in dr_opts:\n",
    "            assert opt in json_cfgdict\n",
    "            json_cfgdict[opt] = dr_opts[opt]\n",
    "        for opt in fnmatch.filter(json_cfgdict.keys(), '*/frq'):\n",
    "            if json_cfgdict[opt] > dr_maxfrq:\n",
    "                json_cfgdict[opt] = dr_opts[opt] = dr_maxfrq\n",
    "        print(f'>> [dry-run] The following options were made overriden:', flush=True)\n",
    "        for opt, val in dr_opts.items():\n",
    "            print(f'>>           {opt}: {val}', flush=True)\n",
    "            \n",
    "        \n",
    "    nodepstfx = '' if args_nodesize == 1 else f'_{args_noderank:02d}'\n",
    "    # Example: nodepstfx in ('', '_01')\n",
    "    json_cfgdict['io/config_id'] = f'{config_id}{nodepstfx}'\n",
    "    # Example: ans in ('poiss2d', 'poiss2d_01')\n",
    "    json_cfgdict['io/results_dir'] = f'{results_dir}/{config_tree}'\n",
    "    # Example: ans == '.../code_bspinn/result/lvl1/lv2/poiss2d'\n",
    "    json_cfgdict['io/storage_dir'] = None # f'{storage_dir}/{config_tree}'\n",
    "    # Example: ans == '.../code_bspinn/storage/lvl1/lv2/poiss2d'\n",
    "    json_cfgdict['io/tch/device'] = args_device_name\n",
    "    # Example: args_device_name == 'cuda:0'\n",
    "\n",
    "    # Pre-processing and applying the looping processes\n",
    "    all_cfgdicts = preproc_cfgdict(json_cfgdict)\n",
    "    \n",
    "    # Selecting this node's config dict subset\n",
    "    node_cfgdicts = [cfg for i, cfg in enumerate(all_cfgdicts) \n",
    "                     if (i % args_nodesize == args_noderank)]\n",
    "    n_nodecfgs = len(node_cfgdicts)\n",
    "    \n",
    "    # Going over the config dicts one-by-one\n",
    "    rsmidx, rsmprt = tuple(int(x) for x in arsg_rsmindex.split('.'))\n",
    "    for cfgidx, config_dict in enumerate(node_cfgdicts):\n",
    "        if cfgidx < rsmidx:\n",
    "            continue\n",
    "        # Getting a single seed run to estimate the memory usage\n",
    "        tempcfg = config_dict.copy()\n",
    "        tempcfg['io/results_dir'] = None\n",
    "        tempcfg['io/storage_dir'] = None\n",
    "        tempcfg['rng_seed/list'] = [0]\n",
    "        tempcfg['opt/epoch'] = 0\n",
    "        tod = main(tempcfg)\n",
    "        allocmem, totmem = tod['dvc/mem/alloc'], tod['dvc/mem/total']\n",
    "        nsd_max = int(0.75 * totmem / allocmem)\n",
    "        \n",
    "        # Computing how many parts we must split the original config into\n",
    "        cfg_seeds = config_dict['rng_seed/list']\n",
    "        nprts = int(np.ceil(len(cfg_seeds) / nsd_max))\n",
    "        print(f'>> Config index {cfgidx} takes {allocmem/1e6:.1f} ' + \n",
    "              f'MB/seed (out of {totmem/1e9:.1f} GB)', flush=True)\n",
    "        print(f'>> Config index {cfgidx} must be ' + \n",
    "              f'devided into {nprts} parts.', flush=True)\n",
    "        \n",
    "        # Looping over each part of the config\n",
    "        for iprt in range(nprts):\n",
    "            if (cfgidx == rsmidx) and (iprt < rsmprt):\n",
    "                continue\n",
    "            print(f'>>> Started Working on config index {cfgidx}.{iprt}' + \n",
    "                  f' (out of {nprts} parts and {n_nodecfgs} configs).', flush=True)\n",
    "            iprtcfgseeds = cfg_seeds[(iprt*nsd_max):((iprt+1)*nsd_max)]\n",
    "            iprtcfgdict = config_dict.copy()\n",
    "            iprtcfgdict['rng_seed/list'] = iprtcfgseeds\n",
    "            main(iprtcfgdict)\n",
    "            print('-' * 40, flush=True)\n",
    "        print(f'>> Finished working on config index {cfgidx} ' + \n",
    "              f'(out of {n_nodecfgs} configs).', flush=True)\n",
    "        print('='*80, flush=True)\n",
    "        \n",
    "    if args_dryrun:\n",
    "        print(f'>> [dry-run] Cleaning up {temp_resdir.name}')\n",
    "        temp_resdir.cleanup()\n",
    "        print(f'>> [dry-run] Cleaning up {temp_strdir.name}')\n",
    "        temp_strdir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97556580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bspinn",
   "language": "python",
   "name": "bspinn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4aec7983bc6059d1b5d440a2253fd0eef7d09b7a26ee33cf7f8716a3ef03c04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
