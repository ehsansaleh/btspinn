{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58fbf2c3",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "This script plays with 3 ideas on the Poisson problem:\n",
    "\n",
    "1. Performing latent parameter search on the true problem oracle using the Gaussian average gradient idea.\n",
    "\n",
    "2. Performing some meta-learning to learn a quickly adaptable parameter set to many charge location configurations.\n",
    "\n",
    "3. Performing MCMC to look for the latent problem parameters using the true oracle.\n",
    "\n",
    "Only MCMC showed promising results, so you can find the MCMC implementation in the `16_poisson` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e827df9",
   "metadata": {},
   "source": [
    "## The Poisson Problem Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d52d6d",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "%matplotlib inline\n",
    "if importlib.util.find_spec(\"matplotlib_inline\") is not None:\n",
    "    import matplotlib_inline\n",
    "    matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
    "else:\n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    set_matplotlib_formats('retina')\n",
    "\n",
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import socket\n",
    "import random\n",
    "import pathlib\n",
    "import fnmatch\n",
    "import datetime\n",
    "import resource\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboardX\n",
    "import psutil\n",
    "import logging\n",
    "import torch.distributions\n",
    "from pyinstrument import Profiler\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "from scipy.special import gamma\n",
    "from os.path import exists, isdir\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict as odict\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a370bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bspinn.io_utils import DataWriter\n",
    "from bspinn.io_utils import get_git_commit\n",
    "from bspinn.io_utils import preproc_cfgdict\n",
    "from bspinn.io_utils import hie2deep, deep2hie\n",
    "\n",
    "from bspinn.tch_utils import isscalar\n",
    "from bspinn.tch_utils import EMA\n",
    "from bspinn.tch_utils import BatchRNG\n",
    "from bspinn.tch_utils import bffnn\n",
    "from bspinn.tch_utils import profmem\n",
    "\n",
    "from bspinn.io_cfg import configs_dir\n",
    "from bspinn.io_cfg import results_dir\n",
    "from bspinn.io_cfg import storage_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429fa7a",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Consider the $d$-dimensional space $\\mathbb{R}^{d}$, and the following charge:\n",
    "\n",
    "$$\\rho(x) = \\delta^d(x).$$\n",
    "\n",
    "For $d \\neq 2$, the analytical solution to the system\n",
    "\n",
    "$$\\nabla \\cdot \\vec{E} = \\rho$$\n",
    "\n",
    "$$\\nabla V = \\vec{E}$$\n",
    "\n",
    "can be defined as \n",
    "\n",
    "$$V_{\\vec{x}} = \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\|\\vec{x}\\|^{2-d}, $$\n",
    "\n",
    "$$\\vec{E}_{\\vec{x}} = \\frac{\\Gamma(d/2)}{2\\cdot \\pi^{d/2}\\cdot \\|\\vec{x}\\|^{d}} \\vec{x}.$$\n",
    "\n",
    "For $d=2$, $\\vec{E}_{\\vec{x}}$ is the same, but for $V_{\\vec{x}}$ we have\n",
    "\n",
    "$$V_{\\vec{x}} = \\frac{1}{2\\pi} \\ln(\\|\\vec{x}\\|).$$\n",
    "\n",
    "We want to solve this system using the divergence theorem:\n",
    "\n",
    "$$\\iint_{S_{d-1}(V)} \\vec{E}\\cdot \\hat{n}\\text{ d}S = \\iiint_{V_d} \\nabla.\\vec{E}\\text{ d}V.$$\n",
    "\n",
    "Keep in mind that the $d-1$-dimensional surface of a $d$-dimensional shpere with radius $r$ is \n",
    "$$\\iint_{S_{d-1}(V^{\\text{d-Ball}}_{r})} 1\\text{ d}S = \\frac{2\\cdot \\pi^{d/2}}{\\Gamma(d/2)}\\cdot r^{d-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697fb22",
   "metadata": {},
   "source": [
    "### Dimensionality Scaling\n",
    "\n",
    "We will assume that our domain of solution is a d-Ball centerred at zero with a radius of $r_b$.\n",
    "$$C_1 := \\int_{V_{r_b}^{d\\text{-Ball}}} 1 d\\vec{x} = \\frac{2\\pi^{d/2}}{d\\cdot\\Gamma(d/2)} r_b^d$$\n",
    "\n",
    "#### The Expectation of the Anlytical Solution\n",
    "\n",
    "$$E_v := \\int_{V_r^{d\\text{-Ball}}} V_{\\vec{x}} d\\vec{x} = \\int \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\|\\vec{x}\\|^{2-d} d\\vec{x}$$\n",
    "\n",
    "$$ = C_1 \\cdot \\int \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\|\\vec{x}\\|^{2-d} \\cdot \\frac{1}{C_1} d\\vec{x} $$\n",
    "\n",
    "$$ = C_1 \\cdot \\frac{\\Gamma(d/2)}{2\\cdot\\pi^{d/2}\\cdot (2-d)} \\int \\|\\vec{x}\\|^{2-d} \\cdot \\frac{1}{C_1} d\\vec{x} $$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\int \\|\\vec{x}\\|^{2-d} \\cdot \\frac{1}{C_1} d\\vec{x} $$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\mathbb{E}_{\\vec{x}} [\\|\\vec{x}\\|^{2-d}] $$\n",
    "\n",
    "By defining the radius of $\\vec{x}$ as $r=\\|\\vec{x}\\|$, the distribution of $r$ is\n",
    "\n",
    "$$Pr(\\|\\vec{x}\\|<r) = (\\frac{r}{r_b})^d$$\n",
    "\n",
    "$$P(\\|\\vec{x}\\|=r) = \\frac{(d-1) \\cdot r^d}{r_b^d}$$\n",
    "\n",
    "Therefore, we have\n",
    "\n",
    "$$E_v = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\mathbb{E}_{\\vec{x}} [r^{2-d}] $$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\int_{r=0}^{r_b} r^{2-d} \\frac{(d-1) \\cdot r^d}{r_b^d} dr$$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\frac{d}{r_b^d} \\int_{r=0}^{r_b} r dr$$\n",
    "\n",
    "$$ = \\frac{r_b^d}{d\\cdot(2-d)} \\cdot \\frac{d}{r_b^d} \\int_{r=0}^{r_b} r dr$$\n",
    "\n",
    "$$ = \\frac{r_b^2}{2\\cdot(2-d)}$$\n",
    "\n",
    "#### The Expectation of the Volume Ratio\n",
    "\n",
    "$$\\mathbb{E}_{r\\sim U[r_l, r_h]}[(\\frac{r}{r_b})^d] = \\frac{1}{r_h - r_l} \\int_{r_l}^{r_h} (\\frac{r}{r_b})^d dr$$\n",
    "\n",
    "$$=\\frac{1}{d+1} \\cdot \\frac{1}{r_b^d} \\frac{r_h^{d+1} - r_l^{d+1}}{r_h - r_l}.$$\n",
    "\n",
    "By setting $r_h=r_b$ and $r_l < r_h$, the above value closes in on $$\\frac{1}{d+1}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e4bd0",
   "metadata": {},
   "source": [
    "### Defining the Problem and the Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cfae0c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DeltaProblem:\n",
    "    def __init__(self, weights, locations, tch_device, tch_dtype):\n",
    "        # weights          -> np.array -> shape=(n_bch, n_chrg)\n",
    "        # locations.shape  -> np.array -> shape=(n_bch, n_chrg, d)\n",
    "        self.weights = weights\n",
    "        self.locations = locations\n",
    "        self.n_bch, self.n_chrg = self.weights.shape\n",
    "        self.d = self.locations.shape[-1]\n",
    "        assert self.weights.shape == (self.n_bch, self.n_chrg,)\n",
    "        assert self.locations.shape == (self.n_bch, self.n_chrg, self.d)\n",
    "        self.weights_tch = torch.from_numpy(\n",
    "            self.weights).to(tch_device, tch_dtype)\n",
    "        self.locations_tch = torch.from_numpy(\n",
    "            self.locations).to(tch_device, tch_dtype)\n",
    "        self.shape = (self.n_bch,)\n",
    "        self.tch_pi = torch.tensor(np.pi, device=tch_device, dtype=tch_dtype)\n",
    "        self.ndim = 1\n",
    "\n",
    "    def integrate_volumes(self, volumes):\n",
    "        # volumes -> dictionary\n",
    "        assert volumes['type'] == 'ball'\n",
    "        centers = volumes['centers']\n",
    "        radii = volumes['radii']\n",
    "        n_v = radii.shape[-1]\n",
    "        n_bch, n_chrg, d = self.n_bch, self.n_chrg, self.d\n",
    "        assert radii.shape == (n_bch, n_v,)\n",
    "        assert centers.shape == (n_bch, n_v, d)\n",
    "        lib = torch if torch.is_tensor(centers) else np\n",
    "        mu = self.locations_tch if torch.is_tensor(centers) else self.locations\n",
    "        w = self.weights_tch if torch.is_tensor(centers) else self.weights\n",
    "\n",
    "        c_diff_mu = centers.reshape(\n",
    "            n_bch, n_v, 1, d) - mu.reshape(n_bch, 1, n_chrg, d)\n",
    "        assert c_diff_mu.shape == (n_bch, n_v, n_chrg, d)\n",
    "        distl2 = lib.sqrt(lib.square(c_diff_mu).sum(-1))\n",
    "        assert distl2.shape == (n_bch, n_v, n_chrg)\n",
    "        integ = ((distl2 < radii.reshape(n_bch, n_v, 1))\n",
    "                 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "        assert integ.shape == (n_bch, n_v)\n",
    "        return integ\n",
    "\n",
    "    def potential(self, x):\n",
    "        lib = torch if torch.is_tensor(x) else np\n",
    "        lib_pi = self.tch_pi if torch.is_tensor(x) else np.pi\n",
    "        w = self.weights_tch if torch.is_tensor(x) else self.weights\n",
    "        mu = self.locations_tch if torch.is_tensor(x) else self.locations\n",
    "        n_bch, n_chrg, d = self.n_bch, self.n_chrg, self.d\n",
    "        n_x = x.shape[-2]\n",
    "        assert x.shape == (\n",
    "            n_bch, n_x, d), f'x.shape={x.shape}, (n_bch, n_x, d)={(n_bch, n_x, d)}'\n",
    "        x_diff_mu = x.reshape(n_bch, n_x, 1, d) - \\\n",
    "            mu.reshape(self.n_bch, 1, n_chrg, d)\n",
    "        assert x_diff_mu.shape == (n_bch, n_x, n_chrg, d)\n",
    "        x_dists = lib.sqrt(lib.square(x_diff_mu).sum(-1))\n",
    "        assert x_dists.shape == (n_bch, n_x, n_chrg)\n",
    "        if d != 2:\n",
    "            poten1 = (x_dists**(2-d))\n",
    "            assert poten1.shape == (n_bch, n_x, n_chrg)\n",
    "            poten2 = (poten1 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "            assert poten2.shape == (n_bch, n_x)\n",
    "            cst = gamma(d/2) / (2*(lib_pi**(d/2)))\n",
    "            cst = cst / (2-d)\n",
    "            assert isscalar(cst)\n",
    "            poten = cst * poten2\n",
    "            assert poten.shape == (n_bch, n_x)\n",
    "        else:\n",
    "            poten1 = lib.log(x_dists)\n",
    "            assert poten1.shape == (n_bch, n_x, n_chrg)\n",
    "            poten2 = (poten1 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "            assert poten2.shape == (n_bch, n_x)\n",
    "            poten = poten2 / (2*lib_pi)\n",
    "            assert poten.shape == (n_bch, n_x)\n",
    "        return poten\n",
    "\n",
    "    def field(self, x):\n",
    "        lib = torch if torch.is_tensor(x) else np\n",
    "        lib_pi = self.tch_pi if torch.is_tensor(x) else np.pi\n",
    "        w = self.weights_tch if torch.is_tensor(x) else self.weights\n",
    "        mu = self.locations_tch if torch.is_tensor(x) else self.locations\n",
    "        n_bch, n_chrg, d = self.n_bch, self.n_chrg, self.d\n",
    "        n_x = x.shape[-2]\n",
    "        assert x.shape == (n_bch, n_x, d)\n",
    "        x_diff_mu = x.reshape(n_bch, n_x, 1, d) - \\\n",
    "            mu.reshape(n_bch, 1, n_chrg, d)\n",
    "        assert x_diff_mu.shape == (n_bch, n_x, n_chrg, d)\n",
    "        x_dists = lib.sqrt(lib.square(x_diff_mu).sum(-1))\n",
    "        assert x_dists.shape == (n_bch, n_x, n_chrg)\n",
    "        poten1 = (x_dists**(-d))\n",
    "        assert poten1.shape == (n_bch, n_x, n_chrg)\n",
    "        poten2 = (poten1 * w.reshape(n_bch, 1, n_chrg)).sum(-1)\n",
    "        assert poten2.shape == (n_bch, n_x)\n",
    "        cst = gamma(d/2) / (2*(lib_pi**(d/2)))\n",
    "        assert isscalar(cst)\n",
    "        poten = cst * poten2\n",
    "        assert poten.shape == (n_bch, n_x)\n",
    "        field = poten.reshape(n_bch, n_x, 1) * x\n",
    "        assert field.shape == (n_bch, n_x, d)\n",
    "        return field\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return dict(weights=self.weights_tch, locations=self.locations_tch)\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        weights = state_dict['weights'].detach().cpu().numpy()\n",
    "        locations = state_dict['locations'].detach().cpu().numpy()\n",
    "        tch_device = self.weights_tch.device\n",
    "        tch_dtype = self.weights_tch.dtype\n",
    "        self.__init__(weights, locations, tch_device, tch_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3319be1",
   "metadata": {},
   "source": [
    "### Defining the Volume Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5270d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BallSampler:\n",
    "    def __init__(self, c_dstr, c_params, r_dstr, r_params, batch_rng):\n",
    "        assert isinstance(c_params, dict)\n",
    "        for name, param in c_params.items():\n",
    "            msg_ = f'center param {name} is not np.array'\n",
    "            assert isinstance(param, np.ndarray), msg_\n",
    "        \n",
    "        assert isinstance(r_params, dict)\n",
    "        for name, param in r_params.items():\n",
    "            msg_ = f'radius param {name} is not np.array'\n",
    "            assert isinstance(param, np.ndarray), msg_\n",
    "\n",
    "        self.batch_rng = batch_rng\n",
    "        self.lib = batch_rng.lib\n",
    "        \n",
    "        ##############################################################\n",
    "        ################# Center Sampling Parameters #################\n",
    "        ##############################################################\n",
    "        c_params_ = c_params.copy()\n",
    "        self.c_dstr = c_dstr\n",
    "        if c_dstr == 'uniform':\n",
    "            c_low = c_params_.pop('low')\n",
    "            c_high = c_params_.pop('high')\n",
    "            \n",
    "            n_bch, dim = c_low.shape\n",
    "            \n",
    "            self.c_low_np = c_low.reshape(n_bch, 1, dim)\n",
    "            self.c_high_np = c_high.reshape(n_bch, 1, dim)\n",
    "            self.c_size_np = (self.c_high_np - self.c_low_np)\n",
    "\n",
    "            if self.lib == 'torch':\n",
    "                self.c_low_tch = torch.from_numpy(self.c_low_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_high_tch = torch.from_numpy(self.c_high_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_size_tch = torch.from_numpy(self.c_size_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            \n",
    "            self.c_low = self.c_low_np if self.lib == 'numpy' else self.c_low_tch\n",
    "            self.c_size = self.c_size_np if self.lib == 'numpy' else self.c_size_tch\n",
    "        elif c_dstr == 'normal':\n",
    "            c_loc = c_params_.pop('loc')\n",
    "            c_scale = c_params_.pop('scale')\n",
    "            \n",
    "            n_bch, dim = c_loc.shape\n",
    "            self.c_loc_np = c_loc.reshape(n_bch, 1, dim)\n",
    "            self.c_scale_np = c_scale.reshape(n_bch, 1, 1)\n",
    "            \n",
    "            if self.lib == 'torch':\n",
    "                self.c_loc_tch = torch.from_numpy(self.c_loc_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_scale_tch = torch.from_numpy(self.c_scale_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                \n",
    "            self.c_loc = self.c_loc_np if self.lib == 'numpy' else self.c_loc_tch\n",
    "            self.c_scale = self.c_scale_np if self.lib == 'numpy' else self.c_scale_tch\n",
    "        elif c_dstr == 'ball':\n",
    "            c_cntr = c_params_.pop('c')\n",
    "            c_radi = c_params_.pop('r')\n",
    "            \n",
    "            n_bch, dim = c_cntr.shape\n",
    "            self.c_cntr_np = c_cntr.reshape(n_bch, 1, dim)\n",
    "            self.c_radi_np = c_radi.reshape(n_bch, 1, 1)\n",
    "            \n",
    "            if self.lib == 'torch':\n",
    "                self.c_cntr_tch = torch.from_numpy(self.c_cntr_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_radi_tch = torch.from_numpy(self.c_radi_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                \n",
    "            self.c_cntr = self.c_cntr_np if self.lib == 'numpy' else self.c_cntr_tch\n",
    "            self.c_radi = self.c_radi_np if self.lib == 'numpy' else self.c_radi_tch\n",
    "        else:\n",
    "            raise ValueError(f'c_dstr=\"{c_dstr}\" not implemented')\n",
    "        \n",
    "        msg_ = f'Some center parameters were left unused: {list(c_params_.keys())}'\n",
    "        assert len(c_params_) == 0, msg_\n",
    "            \n",
    "        self.n_bch, self.d = n_bch, dim\n",
    "        \n",
    "        ##############################################################\n",
    "        ################# Radius Sampling Parameters #################\n",
    "        ##############################################################\n",
    "        r_params_ = r_params.copy()\n",
    "        r_low = r_params_.pop('low')\n",
    "        r_high = r_params_.pop('high')\n",
    "        \n",
    "        if r_dstr == 'uniform':\n",
    "            self.r_upow = 1.0\n",
    "        elif r_dstr == 'unifdpow':\n",
    "            self.r_upow = 1.0 / self.d\n",
    "        else:\n",
    "            raise ValueError(f'r_dstr={r_dstr} not implemented')\n",
    "\n",
    "        r_low_rshp = r_low.reshape(self.n_bch, 1)\n",
    "        r_high_rshp = r_high.reshape(self.n_bch, 1)\n",
    "        assert (r_low >= 0.0).all()\n",
    "        assert (r_high >= r_low).all()\n",
    "        \n",
    "        self.r_dstr = r_dstr\n",
    "        self.r_low_np = np.power(r_low_rshp, 1.0/self.r_upow)\n",
    "        self.r_high_np = np.power(r_high_rshp, 1.0/self.r_upow)\n",
    "        self.r_size_np = (self.r_high_np - self.r_low_np)\n",
    "        \n",
    "        if self.lib == 'torch':\n",
    "            self.r_low_tch = torch.from_numpy(self.r_low_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            self.r_high_tch = torch.from_numpy(self.r_high_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            self.r_size_tch = torch.from_numpy(self.r_size_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            \n",
    "        self.r_low = self.r_low_np if self.lib == 'numpy' else self.r_low_tch\n",
    "        self.r_size = self.r_size_np if self.lib == 'numpy' else self.r_size_tch\n",
    "        \n",
    "        msg_ = f'Some center parameters were left unused: {list(r_params_.keys())}'\n",
    "        assert len(r_params_) == 0, msg_\n",
    "\n",
    "    def __call__(self, n=1):\n",
    "        radii = self.r_low + self.r_size * \\\n",
    "            self.batch_rng.uniform((self.n_bch, n))\n",
    "        radii = radii ** self.r_upow\n",
    "        \n",
    "        if self.c_dstr == 'uniform':\n",
    "            centers = self.batch_rng.uniform((self.n_bch, n, self.d))\n",
    "            centers = centers * self.c_size + self.c_low\n",
    "        elif self.c_dstr == 'normal':\n",
    "            centers = self.batch_rng.normal((self.n_bch, n, self.d))\n",
    "            centers = centers * self.c_scale + self.c_loc\n",
    "        elif self.c_dstr == 'ball':\n",
    "            rnd1 = self.batch_rng.normal((self.n_bch, n, self.d))\n",
    "            rnd1 = rnd1 / ((rnd1**2).sum(-1, keepdims=True)**0.5)\n",
    "            \n",
    "            rnd2 = self.batch_rng.uniform((self.n_bch, n, 1))\n",
    "            rnd2 = rnd2 ** (1./self.d)\n",
    "            \n",
    "            centers = self.c_radi * rnd2 * rnd1 + self.c_cntr\n",
    "        else:\n",
    "            raise ValueError(f'c_dstr=\"{self.c_dstr}\" not implemented')\n",
    "        \n",
    "        d = dict()\n",
    "        d['type'] = 'ball'\n",
    "        d['centers'] = centers\n",
    "        d['radii'] = radii\n",
    "        return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b6294",
   "metadata": {},
   "source": [
    "### Sruface Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d890c38",
   "metadata": {
    "code_folding": [
     0,
     1,
     6,
     12,
     20
    ]
   },
   "outputs": [],
   "source": [
    "class SphereSampler:\n",
    "    def __init__(self, batch_rng):\n",
    "        self.tch_dtype = batch_rng.dtype\n",
    "        self.tch_device = batch_rng.device\n",
    "        self.batch_rng = batch_rng\n",
    "\n",
    "    def np_exlinspace(self, start, end, n):\n",
    "        assert n >= 1\n",
    "        a = np.linspace(start, end, n, endpoint=False)\n",
    "        b = a + 0.5 * (end - a[-1])\n",
    "        return b\n",
    "\n",
    "    def tch_exlinspace(self, start, end, n):\n",
    "        assert n >= 1\n",
    "        a = torch.linspace(start, end, n+1,\n",
    "                           device=self.tch_device,\n",
    "                           dtype=self.tch_dtype)[:-1]\n",
    "        b = a + 0.5 * (end - a[-1])\n",
    "        return b\n",
    "\n",
    "    def __call__(self, volumes, n, do_detspacing=True):\n",
    "        # volumes -> dictionary\n",
    "        assert volumes['type'] == 'ball'\n",
    "        centers = volumes['centers']\n",
    "        radii = volumes['radii']\n",
    "        n_bch, n_v, d = centers.shape\n",
    "        use_np = not torch.is_tensor(centers)\n",
    "        assert centers.shape == (n_bch, n_v, d)\n",
    "        assert radii.shape == (n_bch, n_v)\n",
    "        assert not (use_np) or (self.batch_rng.lib == 'numpy')\n",
    "        assert use_np or (self.batch_rng.device == centers.device)\n",
    "        assert use_np or (self.batch_rng.dtype == centers.dtype)\n",
    "        assert self.batch_rng.shape == (n_bch,)\n",
    "        exlinspace = self.np_exlinspace if use_np else self.tch_exlinspace\n",
    "        meshgrid = np.meshgrid if use_np else torch.meshgrid\n",
    "        sin = np.sin if use_np else torch.sin\n",
    "        cos = np.cos if use_np else torch.cos\n",
    "        matmul = np.matmul if use_np else torch.matmul\n",
    "\n",
    "        if do_detspacing and (d == 2):\n",
    "            theta = exlinspace(0.0, 2*np.pi, n)\n",
    "            assert theta.shape == (n,)\n",
    "            theta_2d = theta.reshape(n, 1)\n",
    "            x_tilde_2d_list = [cos(theta_2d), sin(theta_2d)]\n",
    "            if use_np:\n",
    "                x_tilde_2d = np.concatenate(x_tilde_2d_list, axis=1)\n",
    "            else:\n",
    "                x_tilde_2d = torch.cat(x_tilde_2d_list, dim=1)\n",
    "            assert x_tilde_2d.shape == (n, d)\n",
    "            x_tilde_4d = x_tilde_2d.reshape(1, 1, n, d)\n",
    "            assert x_tilde_4d.shape == (1, 1, n, d)\n",
    "            x_tilde = x_tilde_4d.expand(n_bch, 1, n, d)\n",
    "            assert x_tilde.shape == (n_bch, 1, n, d)\n",
    "        elif do_detspacing and (d == 3):\n",
    "            n_sqrt = int(np.sqrt(n))\n",
    "            assert n == n_sqrt * n_sqrt, 'Need n to be int-square for now!'\n",
    "            theta_1d = exlinspace(0.0, 2*np.pi, n_sqrt)\n",
    "            unit_unif = exlinspace(0.0, 1.0, n_sqrt)\n",
    "            if use_np:\n",
    "                phi_1d = np.arccos(1-2*unit_unif)\n",
    "            else:\n",
    "                phi_1d = torch.arccos(1-2*unit_unif)\n",
    "            theta_msh, phi_msh = meshgrid(theta_1d, phi_1d)\n",
    "            assert theta_msh.shape == (n_sqrt, n_sqrt)\n",
    "            assert phi_msh.shape == (n_sqrt, n_sqrt)\n",
    "            theta_2d, phi_2d = theta_msh.reshape(n, 1), phi_msh.reshape(n, 1)\n",
    "            assert theta_2d.shape == (n, 1)\n",
    "            assert phi_2d.shape == (n, 1)\n",
    "            x_tilde_lst = [sin(phi_2d) * cos(theta),\n",
    "                           sin(phi_2d) * sin(theta), cos(phi_2d)]\n",
    "            if use_np:\n",
    "                x_tilde_2d = np.concatenate(x_tilde_lst, axis=1)\n",
    "            else:\n",
    "                x_tilde_2d = torch.cat(x_tilde_lst, dim=1)\n",
    "            assert x_tilde_2d.shape == (n, d)\n",
    "            x_tilde_4d = x_tilde_2d.reshape(1, 1, n, d)\n",
    "            assert x_tilde_4d.shape == (1, 1, n, d)\n",
    "            x_tilde = x_tilde_4d.expand(n_bch, 1, n, d)\n",
    "            assert x_tilde.shape == (n_bch, 1, n, d)\n",
    "        elif (not do_detspacing) and (not use_np):\n",
    "            x_tilde_unnorm = self.batch_rng.normal((n_bch, n_v, n, d))\n",
    "            x_tilde_l2 = torch.sqrt(torch.square(x_tilde_unnorm).sum(dim=-1))\n",
    "            x_tilde = x_tilde_unnorm / x_tilde_l2.reshape(n_bch, n_v, n, 1)\n",
    "            assert x_tilde.shape == (n_bch, n_v, n, d)\n",
    "        else:\n",
    "            raise RuntimeError('Not implemented yet!')\n",
    "\n",
    "        if do_detspacing:\n",
    "            rot_mats = self.batch_rng.so_n((n_bch, n_v, d, d))\n",
    "            assert rot_mats.shape == (n_bch, n_v, d, d)\n",
    "\n",
    "        if do_detspacing:\n",
    "            x_tilde_rot = matmul(x_tilde, rot_mats)\n",
    "        else:\n",
    "            x_tilde_rot = x_tilde\n",
    "        assert x_tilde_rot.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        points = x_tilde_rot * \\\n",
    "            radii.reshape(n_bch, n_v, 1, 1) + centers.reshape(n_bch, n_v, 1, d)\n",
    "        assert points.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        if use_np:\n",
    "            x_tilde_bc = np.broadcast_to(x_tilde, (n_bch, n_v, n, d))\n",
    "        else:\n",
    "            x_tilde_bc = x_tilde.expand(n_bch, n_v, n, d)\n",
    "\n",
    "        if do_detspacing:\n",
    "            rot_x_tilde = matmul(x_tilde_bc, rot_mats)\n",
    "        else:\n",
    "            rot_x_tilde = x_tilde_bc\n",
    "        assert rot_x_tilde.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        cst = (2*(np.pi**(d/2))) / gamma(d/2)\n",
    "        csts = cst * (radii**(d-1))\n",
    "        assert csts.shape == (n_bch, n_v)\n",
    "\n",
    "        ret_dict = dict(points=points, normals=rot_x_tilde, areas=csts)\n",
    "        return ret_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae26e3",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c393f4d9",
   "metadata": {
    "code_folding": [
     0,
     80,
     157,
     240,
     289,
     422
    ]
   },
   "outputs": [],
   "source": [
    "def get_nn_sol(model, x, n_eval=None, get_field=True, \n",
    "    out_lib='numpy'):\n",
    "    \"\"\"\n",
    "    Gets a model and evaluates it minibatch-wise on the tensor x. \n",
    "    The minibatch size is capped at n_eval. The output will have the \n",
    "    predicted potentials and the vector fields at them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: (nn.module) the batched neural network.\n",
    "\n",
    "    x: (torch.tensor) the evaluation points. This array should be \n",
    "        >2-dimensional and have a shape of `(..., x_rows, x_cols)`.\n",
    "\n",
    "    n_eval: (int or None) the maximum mini-batch size. If None is \n",
    "        given, `x_rows` will be used as `n_eval`.\n",
    "        \n",
    "    out_lib: (str) determines the output tensor type. Should be either \n",
    "        'numpy' or 'torch'.\n",
    "    \n",
    "    Output Dictionary\n",
    "    ----------\n",
    "    v: (np.array or torch.tensor) the evaluated potentials \n",
    "        with a shape of `(*model.shape, x_rows)` where\n",
    "        model.shape is the batch dimensions of the model. \n",
    "\n",
    "    e: (np.array or torch.tensor) the evaluated vector fields \n",
    "        with a shape of `(*model.shape, x_rows, x_cols)` where\n",
    "        model.shape is the batch dimensions of the model.\n",
    "    \"\"\"\n",
    "    x_rows, x_cols = tuple(x.shape)[-2:]\n",
    "    x_bd_ = tuple(x.shape)[:-2]\n",
    "    x_bd = (1,) if len(x_bd_) == 0 else x_bd_\n",
    "    msg_ = f'Cannot have {x.shape} fed to {model.shape}'\n",
    "    assert len(x_bd) <= model.ndim, msg_\n",
    "    if len(x_bd) < model.ndim:\n",
    "        x_bd = tuple([1] * (model.ndim-len(x_b)) + list(x_bd))\n",
    "    assert all((a == b) or (a == 1) or (b == 1) \n",
    "               for a, b in zip(x_bd, model.shape)), msg_\n",
    "    n_eval = x_rows if n_eval is None else n_eval\n",
    "    if out_lib == 'numpy':\n",
    "        to_lib = lambda a: a.detach().cpu().numpy()\n",
    "        lib_cat = lambda al: np.concatenate(al, axis=1)\n",
    "        lpf = '_np'\n",
    "    elif out_lib == 'torch':\n",
    "        to_lib = lambda a: a\n",
    "        lib_cat = lambda al: torch.cat(al, dim=1)\n",
    "        lpf = ''\n",
    "    else:\n",
    "        raise ValueError(f'outlib={outlib} not defined.')\n",
    "\n",
    "    n_batches = int(np.ceil(x_rows / n_eval))\n",
    "    v_pred_list = []\n",
    "    e_pred_list = []\n",
    "    for i in range(n_batches):\n",
    "        x_i = x[..., (i*n_eval):((i+1)*n_eval), :]\n",
    "        xi_rows = x_i.shape[-2]\n",
    "        x_ii = x_i.reshape(*x_bd, xi_rows, x_cols)\n",
    "        x_iii = x_ii.expand(*model.shape, xi_rows, x_cols)\n",
    "        x_iiii = nn.Parameter(x_iii)\n",
    "        v_pred_i = model(x_iiii).squeeze(-1)\n",
    "        v_pred_ii = to_lib(v_pred_i.detach())\n",
    "        v_pred_list.append(v_pred_ii)\n",
    "        if get_field:\n",
    "            e_pred_i, = torch.autograd.grad(v_pred_i.sum(), [x_iiii],\n",
    "                grad_outputs=None, retain_graph=False, create_graph=False,\n",
    "                only_inputs=True, allow_unused=False).squeeze(-1).detach()\n",
    "            e_pred_ii = to_lib(e_pred_i)\n",
    "            e_pred_list.append(e_pred_ii)\n",
    "\n",
    "    v_pred = lib_cat(v_pred_list)\n",
    "    if get_field:\n",
    "        e_pred = lib_cat(e_pred_list)\n",
    "    else:\n",
    "        e_pred = None\n",
    "\n",
    "    outdict = {f'v{lpf}': v_pred, f'e{lpf}': e_pred}\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def get_prob_sol(problem, x, n_eval=None, get_field=True, \n",
    "    out_lib='numpy'):\n",
    "    \"\"\"\n",
    "    Gets a problem and evaluates the analytical solution to its \n",
    "    potentials and vector fields minibatch-wise on the tensor x. \n",
    "    The minibatch size is capped at n_eval. The output will have the \n",
    "    predicted potentials and the vector fields at them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problem: (object) the problem with both the `potential` and \n",
    "        `field` methods for analytical solution evaluation.\n",
    "\n",
    "    x: (torch.tensor) the evaluation points. This array should be \n",
    "        >2-dimensional and have a shape of `(..., x_rows, x_cols)`.\n",
    "\n",
    "    n_eval: (int or None) the maximum mini-batch size. If None is \n",
    "        given, `x_rows` will be used as `n_eval`.\n",
    "\n",
    "    Output Dictionary\n",
    "    ----------\n",
    "    v_np: (np.array) the evaluated potentials with a shape of\n",
    "        `(..., x_rows)`. \n",
    "\n",
    "    e_np: (np.array) the evaluated vector fields with a shape of\n",
    "        `(..., x_rows, x_cols)`.\n",
    "    \"\"\"\n",
    "\n",
    "    assert hasattr(problem, 'potential')\n",
    "    assert callable(problem.potential)\n",
    "    assert hasattr(problem, 'field')\n",
    "    assert callable(problem.field)\n",
    "\n",
    "    x_rows, x_cols = tuple(x.shape)[-2:]\n",
    "    x_bd_ = tuple(x.shape)[:-2]\n",
    "    x_bd = (1,) if len(x_bd_) == 0 else x_bd_\n",
    "    msg_ = f'Cannot have {x.shape} fed to {problem.shape}'\n",
    "    assert len(x_bd) <= problem.ndim, msg_\n",
    "    if len(x_bd) < problem.ndim:\n",
    "        x_bd = tuple([1] * (problem.ndim-len(x_b)) + list(x_bd))\n",
    "    assert all((a == b) or (a == 1) or (b == 1) \n",
    "               for a, b in zip(x_bd, problem.shape)), msg_\n",
    "    n_eval = x_rows if n_eval is None else n_eval\n",
    "    if out_lib == 'numpy':\n",
    "        to_lib = lambda a: a.detach().cpu().numpy()\n",
    "        lib_cat = lambda al: np.concatenate(al, axis=1)\n",
    "        lpf = '_np'\n",
    "    elif out_lib == 'torch':\n",
    "        to_lib = lambda a: a\n",
    "        lib_cat = lambda al: torch.cat(al, dim=1)\n",
    "        lpf = ''\n",
    "    else:\n",
    "        raise ValueError(f'outlib={outlib} not defined.')\n",
    "\n",
    "    n_batches = int(np.ceil(x_rows / n_eval))\n",
    "    v_list = []\n",
    "    e_list = []\n",
    "    for i in range(n_batches):\n",
    "        x_i = x[..., (i*n_eval):((i+1)*n_eval), :]\n",
    "        xi_rows = x_i.shape[-2]\n",
    "        x_ii = x_i.reshape(*x_bd, xi_rows, x_cols)\n",
    "        x_iii = x_ii.expand(*problem.shape, xi_rows, x_cols)\n",
    "        v_i = problem.potential(x_iii)\n",
    "        v_list.append(to_lib(v_i))\n",
    "        if get_field:\n",
    "            e_i = problem.field(x_iii)\n",
    "            e_list.append(to_lib(e_i))\n",
    "\n",
    "    v = lib_cat(v_list)\n",
    "    if get_field:\n",
    "        e = lib_cat(e_list)\n",
    "    else:\n",
    "        e = None\n",
    "    outdict = {f'v{lpf}': v, f'e{lpf}': e}\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def make_grid(x_low, x_high, dim, n_gpd, lib):\n",
    "    \"\"\"\n",
    "    Creates a grid of points using the mesgrid functions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_low: (list) a list of length `dim` with floats \n",
    "        representing the lower limits of the grid.\n",
    "    \n",
    "    x_high: (list) a list of length `dim` with floats \n",
    "        representing the higher limits of the grid.\n",
    "    \n",
    "    dim: (int) the dimension of the grid space.\n",
    "    \n",
    "    n_gpd: (int) the number of points in each \n",
    "        grid dimension. This yields a total of \n",
    "        `n_gpd**dim` points in the total grid.\n",
    "        \n",
    "    lib: (str) either 'torch' or 'numpy'. This determines \n",
    "        the type of `x` output.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    x: (torch.tensor or np.array) a 2-d tensor or array \n",
    "        with the shape of `(n_gpd**dim, dim)`. \n",
    "    \n",
    "    xi_msh_np: (list of np.array) a list of length `dim` \n",
    "        with meshgrid tensors each with a shape of \n",
    "        `[n_gpd] * dim`.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert dim == 2, 'not implemented yet'\n",
    "    assert len(x_low) == dim\n",
    "    assert len(x_high) == dim\n",
    "    assert lib in ('torch', 'numpy')\n",
    "    library = torch if lib == 'torch' else np\n",
    "    tnper = lambda a: a.cpu().detach().numpy()\n",
    "    nper = tnper if lib == 'torch' else lambda a: a\n",
    "    \n",
    "    x1_low, x2_low = x_low\n",
    "    x1_high, x2_high = x_high\n",
    "    n_g_plt = n_gpd ** dim\n",
    "\n",
    "    x1_1d = library.linspace(x1_low, x1_high, n_gpd)\n",
    "    assert x1_1d.shape == (n_gpd,)\n",
    "\n",
    "    x2_1d = library.linspace(x2_low, x2_high, n_gpd)\n",
    "    assert x2_1d.shape == (n_gpd,)\n",
    "\n",
    "    x1_msh, x2_msh = library.meshgrid(x1_1d, x2_1d)\n",
    "    assert x1_msh.shape == (n_gpd, n_gpd)\n",
    "    assert x2_msh.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x1 = x1_msh.reshape(n_g_plt, 1)\n",
    "    assert x1.shape == (n_g_plt, 1)\n",
    "\n",
    "    x2 = x2_msh.reshape(n_g_plt, 1)\n",
    "    assert x2.shape == (n_g_plt, 1)\n",
    "\n",
    "    x1_1d_c = x1_1d.reshape(n_gpd, 1)\n",
    "    assert x1_1d_c.shape == (n_gpd, 1)\n",
    "\n",
    "    x2_1d_c = x2_1d.reshape(n_gpd, 1)\n",
    "    assert x2_1d_c.shape == (n_gpd, 1)\n",
    "\n",
    "    x1_msh_np = nper(x1_msh)\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x2_msh_np = nper(x2_msh)\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x = torch.cat([x1, x2], dim=1)\n",
    "    assert x.shape == (n_g_plt, dim)\n",
    "\n",
    "    x_np = nper(x)\n",
    "    assert x_np.shape == (n_g_plt, dim)\n",
    "    \n",
    "    xi_msh_np = [x1_msh_np, x2_msh_np]\n",
    "    outdict = dict(x=x, xi_msh_np=xi_msh_np)\n",
    "\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=None, ax=None, cax=None):\n",
    "    n_gpd, dim = x1_msh_np.shape[0], x1_msh_np.ndim\n",
    "    assert dim == 2, f'dim={dim}, x1_msh_np.shape={x1_msh_np.shape}'\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "    assert x2_msh_np.shape == (n_gpd, n_gpd)\n",
    "    n_g = (n_gpd ** dim)\n",
    "   \n",
    "    if fig is None:\n",
    "        assert ax is None\n",
    "        assert cax is None\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(3.0, 2.5), dpi=72)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    else:\n",
    "        assert ax is not None\n",
    "   \n",
    "    e_percentile_cap = 90\n",
    "    \n",
    "    v_np = sol_dict['v_np']\n",
    "    assert v_np.shape[-1] == n_g\n",
    "    \n",
    "    v_msh_np = v_np.reshape(-1, n_gpd, n_gpd).mean(axis=0)\n",
    "    im = ax.pcolormesh(x1_msh_np, x2_msh_np, v_msh_np,\n",
    "                        shading='auto', cmap='RdBu')\n",
    "    if cax is not None:\n",
    "        fig.colorbar(im, cax=cax)\n",
    "\n",
    "    e_msh_np = sol_dict['e_np']\n",
    "    if e_msh_np is not None:\n",
    "        assert e_msh_np.shape[-2:] == (n_g, dim)\n",
    "        e_msh_np = e_msh_np.reshape(-1, n_gpd,\n",
    "            n_gpd, dim).mean(axis=0)\n",
    "        if e_percentile_cap is not None:\n",
    "            e_size = np.sqrt((e_msh_np**2).sum(axis=-1))\n",
    "            e_size_cap = np.percentile(a=e_size, \n",
    "                q=e_percentile_cap, axis=None)\n",
    "            cap_coef = np.ones_like(e_size)\n",
    "            cap_coef[e_size > e_size_cap] = e_size_cap / \\\n",
    "                e_size[e_size > e_size_cap]\n",
    "            e_msh_capped = e_msh_np * \\\n",
    "                cap_coef.reshape(*e_msh_np.shape[:-1], 1)\n",
    "        else:\n",
    "            e_msh_capped = e_msh_np\n",
    "\n",
    "        ax.quiver(x1_msh_np, x2_msh_np,\n",
    "            e_msh_capped[:, :, 0], e_msh_capped[:, :, 1])\n",
    "    return fig, ax, cax\n",
    "\n",
    "\n",
    "def get_perfdict(e_pnts, e_mdlsol, e_prbsol):\n",
    "    \"\"\"\n",
    "    Computes the biased, bias-corrected, and slope-corrected error \n",
    "    metrics for the solutions of a Poisson problem.\n",
    "    \n",
    "    This function computes three types of MSE and MAE statistics:\n",
    "        \n",
    "        1. Plain: just take the model and ground truth solution\n",
    "            and subtract them to get the errors. No bias- or slope-correction \n",
    "            is applied to offset those degrees of freedom.\n",
    "            \n",
    "            shorthand: 'pln'\n",
    "            \n",
    "        2. Bias-corrected: subtracts the average value from both the model \n",
    "            and ground truth solutions, and then computes the errors.\n",
    "            \n",
    "            shorthand: 'bc'\n",
    "            \n",
    "        3. Slope-corrected: Since any linear function can be added to the\n",
    "            Poisson solutions without violating the poisson equation, this\n",
    "            function fits an ordinary least squares to both the model and\n",
    "            ground truth solutions, and then subtracts it from them. This\n",
    "            way, even the arbitrary-slope issue can be addressed.\n",
    "            \n",
    "            shorthand: 'slc'\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    e_pnts: (torch.tensor) The input points to the model and the ground truth.\n",
    "        This should have a shape of (n_seeds, n_evlpnts, dim).\n",
    "        \n",
    "    e_mdlsol: (torch.tensor) The model solution with a\n",
    "        (n_seeds, n_evlpnts) shape.\n",
    "    \n",
    "    e_prbsol: (torch.tensor) The ground truth solution with a\n",
    "        (n_seeds, n_evlpnts) shape.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    outdict: (dict) A mapping between the error keys and their numpy arrays.\n",
    "        The error keys are the cartesian product of ('pln', 'bc', 'slc') \n",
    "        and ('mse', 'mae').\n",
    "    \"\"\"\n",
    "    n_seeds, n_evlpnts, dim = e_pnts.shape\n",
    "    assert e_mdlsol.shape == (n_seeds, n_evlpnts)\n",
    "    assert e_prbsol.shape == (n_seeds, n_evlpnts)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # The plain non-processed error matrix\n",
    "        err_pln = e_mdlsol - e_prbsol\n",
    "        assert err_pln.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # The bias-corrected error matrix\n",
    "        e_mdlsol2 = e_mdlsol - e_mdlsol.mean(dim=1, keepdims=True)\n",
    "        assert e_mdlsol2.shape == (n_seeds, n_evlpnts)\n",
    "        e_prbsol2 = e_prbsol - e_prbsol.mean(dim=1, keepdims=True)\n",
    "        assert e_prbsol2.shape == (n_seeds, n_evlpnts)\n",
    "        err_bc = e_mdlsol2 - e_prbsol2\n",
    "        assert err_bc.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # The slope-corrected error matrix\n",
    "        e_pntstrans = e_pnts.transpose(-1, -2)\n",
    "        assert e_pntstrans.shape == (n_seeds, dim, n_evlpnts)\n",
    "        e_pntsig = e_pntstrans.matmul(e_pnts)\n",
    "        assert e_pntsig.shape == (n_seeds, dim, dim)\n",
    "        e_pntsiginv = torch.pinverse(e_pntsig)\n",
    "        assert e_pntsiginv.shape == (n_seeds, dim, dim)\n",
    "        e_pntpinv = e_pntsiginv.matmul(e_pntstrans)\n",
    "        assert e_pntpinv.shape == (n_seeds, dim, n_evlpnts)\n",
    "        \n",
    "        # e_pntpinv = torch.pinverse(e_pnts)\n",
    "        # assert e_pntpinv.shape == (n_seeds, dim, n_evlpnts)\n",
    "        \n",
    "        e_mdlbeta = e_pntpinv.matmul(e_mdlsol2.unsqueeze(-1))\n",
    "        assert e_mdlbeta.shape == (n_seeds, dim, 1)\n",
    "        e_mdlslpcrc = e_pnts.matmul(e_mdlbeta)\n",
    "        assert e_mdlslpcrc.shape == (n_seeds, n_evlpnts, 1)\n",
    "        e_mdlsol3 = e_mdlsol2 - e_mdlslpcrc.squeeze(-1)\n",
    "        assert e_mdlsol3.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        e_prbbeta = e_pntpinv.matmul(e_prbsol2.unsqueeze(-1))\n",
    "        assert e_prbbeta.shape == (n_seeds, dim, 1)\n",
    "        e_prbslpcrc = e_pnts.matmul(e_prbbeta)\n",
    "        assert e_prbslpcrc.shape == (n_seeds, n_evlpnts, 1)\n",
    "        e_prbsol3 = e_prbsol2 - e_prbslpcrc.squeeze(-1)\n",
    "        assert e_prbsol3.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        err_slc = e_mdlsol3 - e_prbsol3\n",
    "        assert err_slc.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # The normalized slope-corrected error matrix\n",
    "        e_mdlsol4 = e_mdlsol3 / e_mdlsol3.std(dim=1, keepdim=True)\n",
    "        assert e_mdlsol4.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        e_prbsol4 = e_prbsol3 / e_prbsol3.std(dim=1, keepdim=True)\n",
    "        assert e_prbsol4.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        err_scn = e_mdlsol4 - e_prbsol4\n",
    "        assert err_scn.shape == (n_seeds, n_evlpnts)\n",
    "        \n",
    "        # Computing the mse and mae values\n",
    "        e_plnmse = err_pln.square().mean(dim=-1)\n",
    "        assert e_plnmse.shape == (n_seeds,)\n",
    "        e_plnmae = err_pln.abs().mean(dim=-1)\n",
    "        assert e_plnmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_bcmse = err_bc.square().mean(dim=-1)\n",
    "        assert e_bcmse.shape == (n_seeds,)\n",
    "        e_bcmae = err_bc.abs().mean(dim=-1)\n",
    "        assert e_bcmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_slcmse = err_slc.square().mean(dim=-1)\n",
    "        assert e_slcmse.shape == (n_seeds,)\n",
    "        e_slcmae = err_slc.abs().mean(dim=-1)\n",
    "        assert e_slcmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_scnmse = err_scn.square().mean(dim=-1)\n",
    "        assert e_scnmse.shape == (n_seeds,)\n",
    "        e_scnmae = err_scn.abs().mean(dim=-1)\n",
    "        assert e_scnmse.shape == (n_seeds,)\n",
    "    \n",
    "        outdict = {'pln/mse': e_plnmse.detach().cpu().numpy(),\n",
    "                   'pln/mae': e_plnmae.detach().cpu().numpy(),\n",
    "                   'bc/mse': e_bcmse.detach().cpu().numpy(),\n",
    "                   'bc/mae': e_bcmae.detach().cpu().numpy(),\n",
    "                   'slc/mse': e_slcmse.detach().cpu().numpy(),\n",
    "                   'slc/mae': e_slcmae.detach().cpu().numpy(),\n",
    "                   'scn/mse': e_scnmse.detach().cpu().numpy(),\n",
    "                   'scn/mae': e_scnmae.detach().cpu().numpy()}\n",
    "    \n",
    "    return outdict\n",
    "\n",
    "\n",
    "def eval_pnts(problem, model, target, e_pnts, do_bootstrap,\n",
    "    n_seeds, n_evlpnts, dim, eval_bs):\n",
    "    assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "    # Computing the model, target and ground truth solutions\n",
    "    e_prbsol = get_prob_sol(problem, e_pnts, n_eval=eval_bs, \n",
    "        get_field=False, out_lib='torch')['v']\n",
    "    assert e_prbsol.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "    # Computing the model solution\n",
    "    with torch.no_grad():\n",
    "        e_mdlsol = get_nn_sol(model, e_pnts, n_eval=eval_bs,\n",
    "            get_field=False, out_lib='torch')['v']\n",
    "        assert e_mdlsol.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "    # Computing the target solution\n",
    "    if do_bootstrap:\n",
    "        with torch.no_grad():\n",
    "            e_trgsol = get_nn_sol(target, e_pnts, n_eval=eval_bs, \n",
    "                get_field=False, out_lib='torch')['v']\n",
    "        assert e_trgsol.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "    eperfs = dict()\n",
    "    eperfs['mdl'] = get_perfdict(e_pnts, e_mdlsol, e_prbsol)\n",
    "    if do_bootstrap:\n",
    "        eperfs['trg'] = get_perfdict(e_pnts, e_trgsol, e_prbsol)\n",
    "    eperfs = deep2hie(eperfs, dictcls=dict)\n",
    "    # Example: eperfs = {'mdl/pln/mse': ...,\n",
    "    #                    'mdl/pln/mae': ...,\n",
    "    #                    'mdl/bc/mse': ...,\n",
    "    #                    'mdl/bc/mae': ...,\n",
    "    #                    'mdl/slc/mse': ...,\n",
    "    #                    'mdl/slc/mae': ...,\n",
    "    #                    'trg/pln/mse': ...,\n",
    "    #                    'trg/pln/mae': ...,\n",
    "    #                    'trg/bc/mse': ...,\n",
    "    #                    'trg/bc/mae': ...,\n",
    "    #                    'trg/slc/mse': ...,\n",
    "    #                    'trg/slc/mae': ...,\n",
    "    #                   }\n",
    "    return eperfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa95d5",
   "metadata": {},
   "source": [
    "## Utility Functions for Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f525f",
   "metadata": {
    "code_folding": [
     8,
     60,
     85
    ]
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "########### Sanity Checking Utility Functions ###########\n",
    "#########################################################\n",
    "\n",
    "msg_bcast = '{} should be np broadcastable to {}={}. '\n",
    "msg_bcast += 'However, it has an inferred shape of {}.'\n",
    "\n",
    "\n",
    "def get_arr(name, trgshp_str, trns_opts):\n",
    "    \"\"\"\n",
    "    Gets a list of values, and checks if it is broadcastable to a \n",
    "    target shape. If the shape does not match, it will raise a proper\n",
    "    assertion error with a meaninful message. The output is a numpy \n",
    "    array that is guaranteed to be broadcastable to the target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: (str) name of the option / hyper-parameter.\n",
    "\n",
    "    trgshp_str: (str) the target shape elements representation. Must be a \n",
    "        valid python expression where the needed elements .\n",
    "\n",
    "    trns_opts: (dict) a dictionary containing the variables needed \n",
    "        for the string to list translation of val.\n",
    "\n",
    "    Key Variables\n",
    "    -------------\n",
    "    `val = trns_opts[name]`: (list or str) list of values read \n",
    "        from the config file. If a string is provided, python's \n",
    "        `eval` function will be used to translate it into a list.\n",
    "        \n",
    "    `trg_shape = eval_formula(trgshp_str, trns_opts)`: (tuple) \n",
    "        the target shape.\n",
    "    \n",
    "    Output\n",
    "    ----------\n",
    "    val_np: (np.array) the numpy array of val. \n",
    "    \"\"\"\n",
    "    msg_ =  f'\"{name}\" must be in trns_opts but it isnt: {trns_opts}'\n",
    "    assert name in trns_opts, msg_\n",
    "    val = trns_opts[name]\n",
    "    \n",
    "    if isinstance(val, str):\n",
    "        val_list = eval_formula(val, trns_opts)\n",
    "    else:\n",
    "        val_list = val\n",
    "    val_np = np.array(val_list)\n",
    "    src_shape = val_np.shape\n",
    "    trg_shape = eval_formula(trgshp_str, trns_opts)\n",
    "    msg_ = msg_bcast.format(name, trgshp_str, trg_shape, src_shape)\n",
    "\n",
    "    assert len(val_np.shape) == len(trg_shape), msg_\n",
    "\n",
    "    is_bcastble = all((x == y or x == 1 or y == 1) for x, y in\n",
    "                      zip(src_shape, trg_shape))\n",
    "    assert is_bcastble, msg_\n",
    "\n",
    "    return val_np\n",
    "\n",
    "\n",
    "def eval_formula(formula, variables):\n",
    "    \"\"\"\n",
    "    Gets a string formula and uses the `eval` function of python to  \n",
    "    translate it into a python variable. The necessary variables for \n",
    "    translation are provided through the `variables` argument.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    formula (str): a string that can be passed to `eval`.\n",
    "        Example: \"[np.sqrt(dim), 'a', None]\"\n",
    "\n",
    "    variables (dict): a dictionary of variables used in the formula.\n",
    "        Example: {\"dim\": 4}\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    pyobj (object): the translated formula into a python object\n",
    "        Example: [2.0, 'a', None]\n",
    "\n",
    "    \"\"\"\n",
    "    locals().update(variables)\n",
    "    pyobj = eval(formula)\n",
    "    return pyobj\n",
    "\n",
    "\n",
    "def chck_dstrargs(opt, cfgdict, dstr2args, opt2req, parnt_optdstr=None):\n",
    "    \"\"\"\n",
    "    Checks if the distribution arguments are provided correctly. Works \n",
    "    with hirarchical models through recursive applications. Proper error \n",
    "    messages are displayed if one of the checks fails.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    opt: (str) the option name.\n",
    "\n",
    "    cfgdict: (dict) the config dictionary.\n",
    "\n",
    "    dstr2args: (dict) a mapping between distribution and their \n",
    "        required arguments.\n",
    "        \n",
    "    opt2req: (dict) required arguments for an option itself, not \n",
    "        necessarily required by the option's distribution.\n",
    "    \"\"\"\n",
    "    opt_dstr = cfgdict.get(f'{opt}/dstr', 'fixed')\n",
    "\n",
    "    msg_ = f'Unknown {opt}_dstr: it should be one of {list(dstr2args.keys())}'\n",
    "    assert opt_dstr in dstr2args, msg_\n",
    "\n",
    "    opt2req = dict() if opt2req is None else opt2req\n",
    "    optreqs = opt2req.get(opt, tuple())\n",
    "    must_spec = list(dstr2args[opt_dstr]) + list(optreqs)\n",
    "    avid_spec = list(chain.from_iterable(\n",
    "        v for k, v in dstr2args.items() if k != opt_dstr))\n",
    "    avid_spec = [k for k in avid_spec if k not in must_spec]\n",
    "\n",
    "    if opt_dstr == 'fixed':\n",
    "        # To avoid infinite recursive calls, we should end this here.\n",
    "        msg_ = f'\"{opt}\" must be specified.'\n",
    "        if parnt_optdstr is not None:\n",
    "            parnt_opt, parnt_dstr = parnt_optdstr\n",
    "            msg_ += f'\"{parnt_opt}\" was specified as \"{parnt_dstr}\", and'\n",
    "        msg_ += f' \"{opt}\" was specified as \"{opt_dstr}\".'\n",
    "        if len(optreqs) > 0:\n",
    "            msg_ += f' Also, \"{opt}\" requires \"{optreqs}\" to be specified.'\n",
    "        opt_val = cfgdict.get(opt, None)\n",
    "        assert opt_val is not None, msg_\n",
    "    else:\n",
    "        for arg in must_spec:\n",
    "            opt_arg = f'{opt}{arg}'\n",
    "            chck_dstrargs(opt_arg, cfgdict, dstr2args, opt2req, (opt, opt_dstr))\n",
    "\n",
    "    for arg in avid_spec:\n",
    "        opt_arg = f'{opt}{arg}'\n",
    "        opt_arg_val = cfgdict.get(opt_arg, None)\n",
    "        msg_ = f'\"{opt_arg}\" should not be specified, since \"{opt}\" '\n",
    "        msg_ += f'appears to follow the \"{opt_dstr}\" distribution.'\n",
    "        assert opt_arg_val is None, msg_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa88d1",
   "metadata": {},
   "source": [
    "## Plain Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a05c3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "################### Mandatory Options ###################\n",
    "#########################################################\n",
    "rng_seed_list = list(range(0, 100_000, 1000))\n",
    "dim = 2\n",
    "\n",
    "n_srf = 400\n",
    "n_srfpts_mdl = 1\n",
    "n_srfpts_trg = 1\n",
    "do_detspacing = False\n",
    "do_dblsampling = False\n",
    "\n",
    "do_bootstrap = True\n",
    "\n",
    "# # The original values for 3-charges\n",
    "# tau = 0.999\n",
    "# w_trgreg = 1.0\n",
    "# w_trg = 0.99\n",
    "\n",
    "# Fast training values for 1-charge\n",
    "tau = 0.984\n",
    "w_trgreg = 2.0\n",
    "w_trg = 0.99\n",
    "\n",
    "opt_type = 'sgd'\n",
    "n_epochs = 5_000\n",
    "lr = 0.001\n",
    "\n",
    "nn_dstr = 'mlp'\n",
    "nn_width = 64\n",
    "nn_hidden = 2\n",
    "nn_act = 'tanh'\n",
    "\n",
    "eval_bs = 256\n",
    "n_evlpnts = 1000\n",
    "eval_frq = 100\n",
    "chkpnt_period = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfe471",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Derived options and assertions\n",
    "n_points = n_srfpts_mdl + n_srfpts_trg\n",
    "\n",
    "assert not (do_dblsampling) or (n_srfpts_trg > 1)\n",
    "if w_trg is None:\n",
    "    w_trg = n_srfpts_trg / n_points\n",
    "assert not (n_srfpts_mdl == 0) or (w_trg == 1.0)\n",
    "n_rsdls = 2 if do_dblsampling else 1\n",
    "\n",
    "if eval_bs is None:\n",
    "    eval_bs = max(n_srfpts_mdl, n_srfpts_trg) * n_srf\n",
    "\n",
    "#########################################################\n",
    "########### I/O-Related Options and Operations ##########\n",
    "#########################################################\n",
    "device_name = 'cuda:0'\n",
    "tch_device = torch.device(device_name)\n",
    "tch_dtype = torch.float32\n",
    "\n",
    "#########################################################\n",
    "########### Constructing the Batch RNG Object ###########\n",
    "#########################################################\n",
    "n_seeds = len(rng_seed_list)\n",
    "rng_seeds = np.array(rng_seed_list)\n",
    "rng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "               device=tch_device, dtype=tch_dtype,\n",
    "               unif_cache_cols=1_000_000,\n",
    "               norm_cache_cols=5_000_000)\n",
    "rng.seed(np.broadcast_to(rng_seeds, rng.shape))\n",
    "\n",
    "#########################################################\n",
    "########## Defining the Poisson Problem Object ##########\n",
    "#########################################################\n",
    "chrg_n = 1\n",
    "chrg_w = np.ones((n_seeds, chrg_n))\n",
    "chrg_mu = np.zeros((n_seeds, chrg_n, dim))\n",
    "problem = DeltaProblem(weights=chrg_w, locations=chrg_mu,\n",
    "    tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "\n",
    "vol_c_params = dict(c=np.zeros((n_seeds, dim)), r=np.ones(n_seeds))\n",
    "vol_r_params = dict(low=np.zeros(n_seeds), high=np.ones(n_seeds))\n",
    "volsampler = BallSampler(c_dstr='ball', c_params=vol_c_params,\n",
    "                         r_dstr='unifdpow', r_params=vol_r_params,\n",
    "                         batch_rng=rng)\n",
    "\n",
    "srfsampler = SphereSampler(batch_rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21829bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir = './15_search/01_plain/'\n",
    "pathlib.Path(storage_dir).mkdir(parents=True, exist_ok=True)\n",
    "strgidx = sum(isdir(f'{storage_dir}/{x}') for x in os.listdir(storage_dir))\n",
    "dtnow = datetime.datetime.now().isoformat(timespec='seconds')\n",
    "dtnow_ = dtnow[2:].replace('-', '').replace(':', '').replace('.', '')\n",
    "cfgstrg_dir = f'{storage_dir}/{strgidx:02d}_{dtnow_}'\n",
    "pathlib.Path(cfgstrg_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if 'tbwriter' in locals():\n",
    "    tbwriter.close()\n",
    "tbwriter = tensorboardX.SummaryWriter(cfgstrg_dir)\n",
    "logging.getLogger(\"tensorboardX.x2num\").setLevel(logging.CRITICAL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2721fd",
   "metadata": {
    "code_folding": [
     40,
     199
    ],
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "model = bffnn(dim, nn_width, nn_hidden, nn_act, (n_seeds,), rng)\n",
    "if do_bootstrap:\n",
    "    target = bffnn(dim, nn_width, nn_hidden, nn_act, (n_seeds,), rng)\n",
    "    target.load_state_dict(model.state_dict())\n",
    "else:\n",
    "    target = model\n",
    "\n",
    "# Set the optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "# Evaluation tools\n",
    "erng = rng\n",
    "last_perfdict = dict()\n",
    "ema = EMA(gamma=0.999, gamma_sq=0.998)\n",
    "trn_sttime = time.time()\n",
    "\n",
    "# Constructing the grid points\n",
    "with torch.no_grad():\n",
    "    n_gpd = 50\n",
    "    n_g = (n_gpd ** dim)\n",
    "    elow, ehigh = -np.ones(dim), np.ones(dim)\n",
    "    gdict = make_grid(elow, ehigh, dim, n_gpd, 'torch')\n",
    "    grid_x_ = gdict['x']\n",
    "    assert grid_x_.shape == (n_g, dim)\n",
    "    grid_x = grid_x_.reshape(1, n_g, dim).expand(n_seeds, n_g, dim)\n",
    "    grid_x = grid_x.to(tch_device, tch_dtype)\n",
    "    assert grid_x.shape == (n_seeds, n_g, dim)\n",
    "    \n",
    "    x1_msh_np, x2_msh_np = gdict['xi_msh_np']\n",
    "\n",
    "with plt.style.context('default'):\n",
    "    figax_list = [plt.subplots(1, 1, figsize=(3.2, 2.5), dpi=100) for _ in range(3)]\n",
    "    (fig_mdl, ax_mdl), (fig_trg, ax_trg), (fig_gt, ax_gt) = figax_list\n",
    "    cax_list = [make_axes_locatable(ax).append_axes('right', size='5%', pad=0.05) \n",
    "                for ax in (ax_mdl, ax_trg, ax_gt)]\n",
    "    cax_mdl, cax_trg, cax_gt = cax_list\n",
    "stat_history = defaultdict(list)\n",
    "train_history = odict()\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Sampling the volumes\n",
    "    volsamps = volsampler(n=n_srf)\n",
    "\n",
    "    # Sampling the points from the srferes\n",
    "    srfsamps = srfsampler(volsamps, n_points, do_detspacing=do_detspacing)\n",
    "    points = nn.Parameter(srfsamps['points'])\n",
    "    surfacenorms = srfsamps['normals']\n",
    "    areas = srfsamps['areas']\n",
    "    assert points.shape == (n_seeds, n_srf, n_points, dim)\n",
    "    assert surfacenorms.shape == (n_seeds, n_srf, n_points, dim)\n",
    "    assert areas.shape == (n_seeds, n_srf,)\n",
    "\n",
    "    points_mdl = points[:, :, :n_srfpts_mdl, :]\n",
    "    assert points_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "    points_trg = points[:, :, n_srfpts_mdl:, :]\n",
    "    assert points_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "    surfacenorms_mdl = surfacenorms[:, :, :n_srfpts_mdl, :]\n",
    "    assert surfacenorms_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "    surfacenorms_trg = surfacenorms[:, :, n_srfpts_mdl:, :]\n",
    "    assert surfacenorms_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "    # Making surface integral predictions using the reference model\n",
    "    u_mdl = model(points_mdl)\n",
    "    assert u_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, 1)\n",
    "    nabla_x_u_mdl, = torch.autograd.grad(u_mdl.sum(), [points_mdl],\n",
    "        grad_outputs=None, retain_graph=True, create_graph=True,\n",
    "        only_inputs=True, allow_unused=False)\n",
    "    assert nabla_x_u_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "    normprods_mdl = (nabla_x_u_mdl * surfacenorms_mdl).sum(dim=-1)\n",
    "    assert normprods_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl)\n",
    "    if n_srfpts_mdl > 0:\n",
    "        mean_normprods_mdl = normprods_mdl.mean(dim=-1, keepdim=True)\n",
    "        assert mean_normprods_mdl.shape == (n_seeds, n_srf, 1)\n",
    "    else:\n",
    "        mean_normprods_mdl = 0.0\n",
    "\n",
    "    # Making surface integral predictions using the target model\n",
    "    u_trg = target(points_trg)\n",
    "    assert u_trg.shape == (n_seeds, n_srf, n_srfpts_trg, 1)\n",
    "    nabla_x_u_trg, = torch.autograd.grad(u_trg.sum(), [points_trg],\n",
    "        grad_outputs=None, retain_graph=True, create_graph=not(do_bootstrap),\n",
    "        only_inputs=True, allow_unused=False)\n",
    "    assert nabla_x_u_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "    normprods_trg = (nabla_x_u_trg * surfacenorms_trg).sum(dim=-1)\n",
    "    assert normprods_trg.shape == (n_seeds, n_srf, n_srfpts_trg)\n",
    "    if do_dblsampling:\n",
    "        assert n_rsdls == 2\n",
    "\n",
    "        mean_normprods_trg1 = normprods_trg[..., 0::2].mean(\n",
    "            dim=-1, keepdim=True)\n",
    "        assert mean_normprods_trg1.shape == (n_seeds, n_srf, 1)\n",
    "\n",
    "        mean_normprods_trg2 = normprods_trg[..., 1::2].mean(\n",
    "            dim=-1, keepdim=True)\n",
    "        assert mean_normprods_trg2.shape == (n_seeds, n_srf, 1)\n",
    "\n",
    "        mean_normprods_trg = torch.cat(\n",
    "            [mean_normprods_trg1, mean_normprods_trg2], dim=-1)\n",
    "        assert mean_normprods_trg.shape == (n_seeds, n_srf, n_rsdls)\n",
    "    else:\n",
    "        assert n_rsdls == 1\n",
    "\n",
    "        mean_normprods_trg = normprods_trg.mean(dim=-1, keepdim=True)\n",
    "        assert mean_normprods_trg.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "    # Linearly combining the reference and target predictions\n",
    "    mean_normprods = (       w_trg  * mean_normprods_trg +\n",
    "                      (1.0 - w_trg) * mean_normprods_mdl)\n",
    "    assert mean_normprods.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "    # Considering the surface areas\n",
    "    pred_surfintegs = mean_normprods * areas.reshape(n_seeds, n_srf, 1)\n",
    "    assert pred_surfintegs.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "    # Getting the reference volume integrals\n",
    "    ref_volintegs = problem.integrate_volumes(volsamps)\n",
    "    assert ref_volintegs.shape == (n_seeds, n_srf)\n",
    "\n",
    "    # Getting the residual terms\n",
    "    resterms = pred_surfintegs - ref_volintegs.reshape(n_seeds, n_srf, 1)\n",
    "    assert resterms.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "    # Multiplying the residual terms\n",
    "    if do_dblsampling:\n",
    "        resterms_prod = resterms.prod(dim=-1)\n",
    "        assert resterms_prod.shape == (n_seeds, n_srf)\n",
    "    else:\n",
    "        resterms_prod = torch.square(resterms).squeeze(-1)\n",
    "        assert resterms_prod.shape == (n_seeds, n_srf)\n",
    "\n",
    "    # Computing the main loss\n",
    "    loss_main = resterms_prod.mean(-1)\n",
    "    assert loss_main.shape == (n_seeds,)\n",
    "\n",
    "    if do_bootstrap:\n",
    "        with torch.no_grad():\n",
    "            u_mdl_prime = target(points_mdl)\n",
    "        loss_trgreg = torch.square(u_mdl - u_mdl_prime).mean([-3, -2, -1])\n",
    "        assert loss_trgreg.shape == (n_seeds,)\n",
    "    else:\n",
    "        loss_trgreg = torch.zeros(n_seeds, device=tch_device, dtype=tch_dtype)\n",
    "        assert loss_trgreg.shape == (n_seeds,)\n",
    "\n",
    "    # The total loss\n",
    "    loss = loss_main + w_trgreg * loss_trgreg\n",
    "    assert loss.shape == (n_seeds,)\n",
    "\n",
    "    loss_sum = loss.sum()\n",
    "    loss_sum.backward()\n",
    "\n",
    "    # We will not update in the first epoch so that we will \n",
    "    # record the initialization statistics as well. Instead, \n",
    "    # we will update an extra epoch at the end.\n",
    "    if (epoch > 0):\n",
    "        opt.step()\n",
    "\n",
    "    # Updating the target network\n",
    "    if do_bootstrap and (epoch > 0):\n",
    "        model_sd = model.state_dict()\n",
    "        target_sd = target.state_dict()\n",
    "        newtrg_sd = dict()\n",
    "        with torch.no_grad():\n",
    "            for key, param in model_sd.items():\n",
    "                param_trg = target_sd[key]\n",
    "                newtrg_sd[key] = tau * param_trg + (1-tau) * param\n",
    "        target.load_state_dict(newtrg_sd)\n",
    "    \n",
    "    # Saving the grid solutions to tensorboard\n",
    "    if epoch % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            grid_prbsol = get_prob_sol(problem, grid_x, n_eval=eval_bs, \n",
    "                get_field=False, out_lib='numpy')\n",
    "            assert grid_prbsol['v_np'].shape == (n_seeds, n_g)\n",
    "\n",
    "            grid_mdlsol = get_nn_sol(model, grid_x, n_eval=eval_bs,\n",
    "                get_field=False, out_lib='numpy')\n",
    "            assert grid_mdlsol['v_np'].shape == (n_seeds, n_g)\n",
    "\n",
    "            if do_bootstrap:\n",
    "                grid_trgsol = get_nn_sol(target, grid_x, n_eval=eval_bs, \n",
    "                    get_field=False, out_lib='numpy')\n",
    "            assert grid_trgsol['v_np'].shape == (n_seeds, n_g)\n",
    "            \n",
    "            soltd_list = [('gt', grid_prbsol, fig_gt, ax_gt, cax_gt, 'Ground Truth'),\n",
    "                          ('mdl', grid_mdlsol, fig_mdl, ax_mdl, cax_mdl, 'Prediction')]\n",
    "            if do_bootstrap:\n",
    "                soltd_list += [('trg', grid_trgsol, fig_trg, ax_trg, cax_trg, 'Target')]\n",
    "            for sol_t, sol_dict, fig, ax, cax, ttl in soltd_list:\n",
    "                plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=fig, ax=ax, cax=cax)\n",
    "                ax.set_title(ttl)\n",
    "                fig.set_tight_layout(True)\n",
    "                tbwriter.add_figure(f'viz/{sol_t}', fig, epoch)\n",
    "            tbwriter.flush()\n",
    "            \n",
    "    if epoch % eval_frq == 0:\n",
    "        # Sampling the evaluation points\n",
    "        with torch.no_grad():\n",
    "            evols = volsampler(n=n_evlpnts)\n",
    "            assert evols['type'] == 'ball'\n",
    "\n",
    "            e_c = evols['centers']\n",
    "            assert e_c.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "            e_r_ = evols['radii']\n",
    "            assert e_r_.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "            e_r = e_r_.unsqueeze(dim=-1)\n",
    "            assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "            untrd = erng.uniform((n_seeds, n_evlpnts, 1))\n",
    "            assert untrd.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "            untr = untrd.pow(1.0 / dim)\n",
    "            assert untr.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "            e_pntrs = untr * e_r\n",
    "            assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "            etheta = erng.normal((n_seeds, n_evlpnts, dim))\n",
    "            assert etheta.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "            ethtilde = etheta / etheta.norm(dim=-1, keepdim=True)\n",
    "            assert ethtilde.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "            e_pnts = e_c + ethtilde * e_pntrs\n",
    "            assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "        \n",
    "        eperfs = eval_pnts(problem, model, target, e_pnts, do_bootstrap,\n",
    "            n_seeds, n_evlpnts, dim, eval_bs)\n",
    "        for kk, vv in eperfs.items():\n",
    "            if '/bc/' in kk:\n",
    "                tbwriter.add_scalar(f'perf/{kk}', vv.mean(), epoch)\n",
    "            \n",
    "    # computing the normal product variances\n",
    "    with torch.no_grad(): \n",
    "        normprods = torch.cat([normprods_mdl, normprods_trg], dim=-1)\n",
    "        npvm = (normprods.var(dim=-1)*areas.square()).mean(-1)\n",
    "\n",
    "    # Computing the loss moving averages\n",
    "    loss_ema_mean, loss_ema_std_mean = ema('loss', loss)\n",
    "    npvm_ema_mean, npvm_ema_std_mean = ema('npvm', npvm)\n",
    "    if epoch % 1000 == 0:\n",
    "        print_str = f'Epoch {epoch}, EMA loss = {loss_ema_mean:.4f}'\n",
    "        print_str += f' +/- {2*loss_ema_std_mean:.4f}'\n",
    "        print_str += f', EMA Field-Norm Product Variance = {npvm_ema_mean:.4f}'\n",
    "        print_str += f' +/- {2*npvm_ema_std_mean:.4f} ({time.time()-trn_sttime:0.1f} s)'\n",
    "        print(print_str, flush=True)\n",
    " \n",
    "    tbwriter.add_scalar('loss/total', loss.mean(), epoch)\n",
    "    tbwriter.add_scalar('loss/main', loss_main.mean(), epoch)\n",
    "    tbwriter.add_scalar('loss/trgreg', loss_trgreg.mean(), epoch)\n",
    "    tbwriter.add_scalar('loss/npvm', npvm.mean(), epoch)\n",
    "\n",
    "    if epoch % chkpnt_period == 0:\n",
    "        train_history[f'{epoch}/mdl'] = deepcopy({k: v.cpu() for k, v\n",
    "            in model.state_dict().items()})\n",
    "        train_history[f'{epoch}/trg'] = deepcopy({k: v.cpu() for k, v\n",
    "            in target.state_dict().items()})\n",
    "        train_history[f'{epoch}/prb'] = deepcopy({k: v.cpu() for k, v\n",
    "            in problem.state_dict().items()})\n",
    "        \n",
    "print(f'Training finished in {time.time() - trn_sttime:.1f} seconds.')\n",
    "tbwriter.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = 1, 2 + do_bootstrap\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(\n",
    "    n_cols * 3.5, n_rows * 3), dpi=72, sharex=True, sharey=True)\n",
    "cax = None\n",
    "\n",
    "# Computing the model, target and ground truth solutions\n",
    "prob_sol = get_prob_sol(problem, grid_x, n_eval=eval_bs, get_field=False)\n",
    "with torch.no_grad():\n",
    "    mdl_sol = get_nn_sol(model, grid_x, n_eval=eval_bs, get_field=False) \n",
    "    if do_bootstrap:\n",
    "        trg_sol = get_nn_sol(target, grid_x, n_eval=eval_bs, get_field=False)\n",
    "\n",
    "soltd_list = [('gt', prob_sol, axes[0], 'Ground Truth'),\n",
    "              ('mdl', mdl_sol, axes[1], 'Prediction')]\n",
    "if do_bootstrap:\n",
    "    soltd_list += [('trg', trg_sol, axes[2], 'Target')]\n",
    "for sol_t, sol_dict, ax, ttl in soltd_list:\n",
    "    plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=fig, ax=ax, cax=cax)\n",
    "    ax.set_title(ttl)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b71ad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "torch.save(train_history, f'{cfgstrg_dir}/train_history.pt')\n",
    "\n",
    "hp_dict = dict(dim=dim, n_srf=n_srf, n_srfpts_mdl=n_srfpts_mdl, \n",
    "               n_srfpts_trg=n_srfpts_trg, do_detspacing=do_detspacing, \n",
    "               do_dblsampling=do_dblsampling, do_bootstrap=do_bootstrap,\n",
    "               tau=tau, w_trgreg=w_trgreg, w_trg=w_trg, opt_type=opt_type, \n",
    "               n_epochs=n_epochs, lr=lr, nn_dstr=nn_dstr, nn_width=nn_width, \n",
    "               nn_hidden=nn_hidden, nn_act=nn_act)\n",
    "\n",
    "with open(f'{cfgstrg_dir}/config.json', \"w\") as outfile:\n",
    "    json.dump(hp_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0368c022",
   "metadata": {},
   "source": [
    "### Toy Example: Latent Parameter Identification\n",
    "\n",
    "Here, we implement a zero-order search method in the `GPMBO` class and use it to recover the charge locations in the following Poisson problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97fc3b6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GPMBO:\n",
    "    def __init__(self, dim, n_mdl, n_seeds, lr, init_mu, init_std, \n",
    "                 gamma, yb_gamma, rng, optim, tch_device, tch_dtype, \n",
    "                 opt_siglog=True):\n",
    "        mu = init_mu * torch.ones(n_seeds, 1, dim, device=tch_device, dtype=tch_dtype)\n",
    "        mu = torch.nn.Parameter(mu)\n",
    "        assert mu.shape == (n_seeds, 1, dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sig_rot = torch.eye(dim, device=tch_device, dtype=tch_dtype)\n",
    "            sig_rot = sig_rot.reshape(1, 1, dim, dim).expand(n_seeds, 1, dim, dim).clone()\n",
    "            sig_log = torch.full((n_seeds, 1, dim), np.log(init_std), device=tch_device, dtype=tch_dtype)\n",
    "        sig_rot = torch.nn.Parameter(sig_rot)\n",
    "        assert sig_rot.shape == (n_seeds, 1, dim, dim)\n",
    "        sig_log = torch.nn.Parameter(sig_log)\n",
    "        assert sig_log.shape == (n_seeds, 1, dim)\n",
    "        \n",
    "        assert not sig_rot.isnan().any()\n",
    "        \n",
    "        epsilon = rng.normal((n_seeds, n_mdl, dim, 1))\n",
    "        opt_vars = [mu, sig_rot, sig_log] if opt_siglog else [mu, sig_rot]\n",
    "        if optim == 'adam':\n",
    "            opt = torch.optim.Adam(opt_vars, lr=lr)\n",
    "        elif optim == 'sgd':\n",
    "            opt = torch.optim.SGD(opt_vars, lr=lr)\n",
    "        else:\n",
    "            raise ValueError(optim)\n",
    "        \n",
    "        self.mu, self.sig_rot, self.sig_log = mu, sig_rot, sig_log\n",
    "        self.opt = opt\n",
    "        self.rng = rng\n",
    "        self.epsilon = epsilon\n",
    "        self.dim = dim\n",
    "        self.n_mdl = n_mdl\n",
    "        self.n_seeds = n_seeds\n",
    "        self.pi = torch.tensor(np.pi).to(device=tch_device, dtype=tch_dtype)\n",
    "        self.y_base = torch.zeros(n_seeds, 1, device=tch_device, dtype=tch_dtype)\n",
    "        self.gamma = gamma\n",
    "        self.yb_gamma = yb_gamma\n",
    "         \n",
    "    def ask(self):\n",
    "        n_seeds, n_mdl, dim = self.n_seeds, self.n_mdl, self.dim\n",
    "        epsilon, mu = self.epsilon, self.mu\n",
    "        sig_log, sig_rot = self.sig_log, self.sig_rot\n",
    "        \n",
    "        # Computing the \"std\" matrix\n",
    "        sig_rot_ = sig_rot.tril()\n",
    "        assert sig_rot_.shape == (n_seeds, 1, dim, dim)\n",
    "        sig_r = torch.matrix_exp(sig_rot_ - sig_rot_.transpose(-1, -2))\n",
    "        assert sig_r.shape == (n_seeds, 1, dim, dim)\n",
    "        sig_sig = sig_log.exp().reshape(n_seeds, 1, 1, dim)\n",
    "        assert sig_sig.shape == (n_seeds, 1, 1, dim)\n",
    "        sig = sig_r * sig_sig\n",
    "        assert sig.shape == (n_seeds, 1, dim, dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x_query = sig.matmul(epsilon)\n",
    "            assert x_query.shape == (n_seeds, n_mdl, dim, 1)\n",
    "\n",
    "            x_query = x_query.squeeze(dim=-1)\n",
    "            assert x_query.shape == (n_seeds, n_mdl, dim)\n",
    "\n",
    "            x_query = x_query + self.mu\n",
    "            assert x_query.shape == (n_seeds, n_mdl, dim)\n",
    "            \n",
    "        return x_query\n",
    "    \n",
    "    def tell(self, y):\n",
    "        n_seeds, n_mdl, dim = self.n_seeds, self.n_mdl, self.dim\n",
    "        epsilon, mu = self.epsilon, self.mu\n",
    "        gamma, y_bias = self.gamma, self.y_base\n",
    "        sig_log, sig_rot = self.sig_log, self.sig_rot\n",
    "        yb_gamma = self.yb_gamma\n",
    "        \n",
    "        x = self.ask().detach()\n",
    "        assert x.shape == (n_seeds, n_mdl, dim)\n",
    "        assert y.shape == (n_seeds, n_mdl)\n",
    "        assert not(y.isnan().any())\n",
    "        \n",
    "        self.opt.zero_grad()\n",
    "        \n",
    "        e = (x - mu).unsqueeze(-2)\n",
    "        assert e.shape == (n_seeds, n_mdl, 1, dim)\n",
    "        \n",
    "        sig_rot_ = sig_rot.tril()\n",
    "        assert sig_rot_.shape == (n_seeds, 1, dim, dim)\n",
    "        sig_r = torch.matrix_exp(sig_rot_ - sig_rot_.transpose(-1, -2))\n",
    "        assert sig_r.shape == (n_seeds, 1, dim, dim)\n",
    "        \n",
    "        sig_lam = (-sig_log).exp().reshape(n_seeds, 1, 1, dim)\n",
    "        assert sig_lam.shape == (n_seeds, 1, 1, dim)\n",
    "        sig = sig_r * sig_lam\n",
    "        assert sig.shape == (n_seeds, 1, dim, dim)\n",
    "        eT_sig = e.matmul(sig)\n",
    "        assert eT_sig.shape == (n_seeds, n_mdl, 1, dim)\n",
    "        eT_siginv_e = eT_sig.square().sum(dim=-1).squeeze(-1)\n",
    "        assert eT_siginv_e.shape == (n_seeds, n_mdl)\n",
    "        \n",
    "        sigma_halflogdet = sig_log.sum(dim=-1)\n",
    "        assert sigma_halflogdet.shape == (n_seeds, 1)\n",
    "        logpdf = -0.5 * eT_siginv_e - sigma_halflogdet\n",
    "        assert logpdf.shape == (n_seeds, n_mdl)\n",
    "        \n",
    "        prob_ratio = (logpdf - logpdf.detach()).exp()\n",
    "        assert prob_ratio.shape == (n_seeds, n_mdl)\n",
    "        y_probratio = (y - y_bias) * prob_ratio\n",
    "        assert y_probratio.shape == (n_seeds, n_mdl)\n",
    "        max_obj = y_probratio.mean(dim=-1)\n",
    "        assert max_obj.shape == (n_seeds,)\n",
    "        \n",
    "        min_obj = -max_obj.sum()\n",
    "        min_obj.backward()\n",
    "        \n",
    "        self.opt.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Updating epsilon\n",
    "            deps = rng.normal((n_seeds, n_mdl, dim, 1))\n",
    "            self.epsilon = gamma * epsilon + (1. - self.gamma) * deps\n",
    "            assert self.epsilon.shape == (n_seeds, n_mdl, dim, 1)\n",
    "\n",
    "            # Updating y_bias\n",
    "            dyb = y_bias.mean(dim=-1, keepdims=True)\n",
    "            self.y_bias = yb_gamma * y_bias + (1. - yb_gamma) * dyb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4414d0e",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1: Fitting to the potentials without query noise\n",
    "chrg_n = 3\n",
    "n_mdl = 100\n",
    "query_noise = 0.0\n",
    "mbo_lr = 0.05\n",
    "n_mboiter = 400\n",
    "y_type = 'potential'\n",
    "\n",
    "# # Example 2: Fitting to the potentials with query noise\n",
    "# chrg_n = 3\n",
    "# n_mdl = 100\n",
    "# query_noise = 0.05\n",
    "# mbo_lr = 0.005\n",
    "# n_mboiter = 4000\n",
    "# y_type = 'potential'\n",
    "\n",
    "# # Example 3: Fitting to the fields with noise\n",
    "# chrg_n = 1\n",
    "# n_mdl = 1000\n",
    "# query_noise = 0.05\n",
    "# mbo_lr = 0.005\n",
    "# n_mboiter = 3500\n",
    "# y_type = 'field'\n",
    "\n",
    "# Common settings\n",
    "dim = 2\n",
    "n_seeds = 10\n",
    "n_true = 20\n",
    "opt_siglog = True\n",
    "init_std = 0.5\n",
    "\n",
    "tch_device = torch.device('cuda:0')\n",
    "tch_dtype = torch.float32\n",
    "\n",
    "# Creating the RNG\n",
    "seeds_arr = (np.arange(n_seeds) * 1000).tolist()\n",
    "rng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "       device=tch_device, dtype=tch_dtype,\n",
    "       unif_cache_cols=1_000_000,\n",
    "       norm_cache_cols=5_000_000)\n",
    "rng.seed(np.array(seeds_arr))\n",
    "\n",
    "# Creating the true problem\n",
    "true_chrg_w = np.ones((n_seeds, chrg_n))\n",
    "assert true_chrg_w.shape == (n_seeds, chrg_n)\n",
    "true_chrg_mu = np.array([[-0.5, -0.5],\n",
    "                         [ 0.5,  0.5],\n",
    "                         [ 0.0,  0.0]])[:chrg_n, :]\n",
    "true_chrg_mu = np.broadcast_to(true_chrg_mu[None, ...], \n",
    "    (n_seeds, chrg_n, dim)).copy()\n",
    "assert true_chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "true_problem = DeltaProblem(weights=true_chrg_w, \n",
    "    locations=true_chrg_mu,\n",
    "    tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "\n",
    "true_x = rng.uniform((n_seeds, n_true, dim)) * 2 - 1\n",
    "assert true_x.shape == (n_seeds, n_true, dim)\n",
    "if y_type == 'potential':\n",
    "    y_dim = 1\n",
    "    mse_mul = 100\n",
    "    mse_clip = np.inf\n",
    "    true_y = true_problem.potential(true_x).unsqueeze(-1)\n",
    "    assert true_y.shape == (n_seeds, n_true, 1)\n",
    "elif y_type == 'field':\n",
    "    y_dim = dim\n",
    "    mse_mul = 1\n",
    "    mse_clip = 10\n",
    "    true_y = true_problem.field(true_x)\n",
    "    assert true_y.shape == (n_seeds, n_true, y_dim)\n",
    "else:\n",
    "    raise ValueError(f'y_type={y_type} undefined')\n",
    "\n",
    "sdim = chrg_n * dim\n",
    "mbo = GPMBO(dim=sdim, n_mdl=n_mdl, n_seeds=n_seeds, \n",
    "            lr=mbo_lr, init_mu=0.0, init_std=init_std, gamma=0.0, \n",
    "            yb_gamma=0.9, rng=rng, optim='adam',\n",
    "            tch_device=tch_device, tch_dtype=tch_dtype, \n",
    "            opt_siglog=opt_siglog)\n",
    "\n",
    "all_mbo_mu = []\n",
    "for mbo_iter in range(n_mboiter):\n",
    "    php_query = mbo.ask()\n",
    "    assert php_query.shape == (n_seeds, n_mdl, sdim)\n",
    "    \n",
    "    # Adding noise to the query to make the problem more challenging\n",
    "    php_query = php_query + query_noise * rng.normal((n_seeds, n_mdl, sdim))\n",
    "    assert php_query.shape == (n_seeds, n_mdl, sdim)\n",
    "\n",
    "    # Running a fake query system\n",
    "    loc_query = php_query.reshape(n_seeds*n_mdl, chrg_n, dim).detach().cpu().numpy()\n",
    "    w_query = torch.ones(n_seeds*n_mdl, chrg_n).detach().cpu().numpy()\n",
    "\n",
    "    problem_query = DeltaProblem(weights=w_query, \n",
    "        locations=loc_query,\n",
    "        tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "\n",
    "    x_query = true_x.reshape(n_seeds, 1, n_true, dim)\n",
    "    x_query = x_query.expand(n_seeds, n_mdl, n_true, dim)\n",
    "    x_query = x_query.reshape(n_seeds*n_mdl, n_true, dim)\n",
    "    \n",
    "    if y_type == 'potential':\n",
    "        y_query_ = problem_query.potential(x_query).unsqueeze(-1)\n",
    "        assert y_query_.shape == (n_seeds*n_mdl, n_true, y_dim)\n",
    "    elif y_type == 'field':\n",
    "        y_query_ = problem_query.field(x_query)\n",
    "        assert y_query_.shape == (n_seeds*n_mdl, n_true, y_dim)\n",
    "    else:\n",
    "        raise ValueError(f'y_type={y_type} undefined')\n",
    "\n",
    "    y_query = y_query_.reshape(n_seeds, n_mdl, n_true, y_dim)\n",
    "    assert y_query.shape == (n_seeds, n_mdl, n_true, y_dim)\n",
    "\n",
    "    y_err = y_query - true_y.reshape(n_seeds, 1, n_true, y_dim)\n",
    "    assert y_err.shape == (n_seeds, n_mdl, n_true, y_dim)\n",
    "\n",
    "    y_mse_ = y_err.square().sum(dim=-1)\n",
    "    assert y_mse_.shape == (n_seeds, n_mdl, n_true)\n",
    "    \n",
    "    y_mse_ = torch.clip(y_mse_ , 0.0, mse_clip)\n",
    "    assert y_mse_.shape == (n_seeds, n_mdl, n_true)\n",
    "    \n",
    "    y_mse = mse_mul * y_mse_.mean(dim=-1)\n",
    "    assert y_mse.shape == (n_seeds, n_mdl)\n",
    "    \n",
    "    if mbo_iter % 100 == 0:\n",
    "        print(f'MSE: {y_mse.mean():.4f} +/- {y_mse.std()/np.sqrt(n_seeds*n_mdl):.4f}')\n",
    "\n",
    "    mbo.tell(-y_mse)\n",
    "    all_mbo_mu.append(mbo.mu)\n",
    "    \n",
    "mbo_mus_ = torch.cat(all_mbo_mu, dim=-2)\n",
    "assert mbo_mus_.shape == (n_seeds, n_mboiter, chrg_n*dim)\n",
    "mbo_mus = mbo_mus_.reshape(n_seeds, n_mboiter, chrg_n, dim)\n",
    "assert mbo_mus.shape == (n_seeds, n_mboiter, chrg_n, dim)\n",
    "\n",
    "print('The final Gaussian means:')\n",
    "a = mbo.mu.reshape(n_seeds, chrg_n*dim).detach().cpu().numpy()\n",
    "print(np.array_str(a, precision=3, suppress_small=True) + '\\n' + '-'*80)\n",
    "\n",
    "print('The final Gaussian stds:')\n",
    "a = mbo.sig_log.exp().reshape(n_seeds, chrg_n*dim).detach().cpu().numpy()\n",
    "print(np.array_str(a, precision=3, suppress_small=True) + '\\n' + '-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Fitting to the potentials without query noise\n",
    "chrg_n = 3\n",
    "n_mdl = 100\n",
    "query_noise = 0.0\n",
    "mbo_lr = 0.01\n",
    "n_mboiter = 4000\n",
    "y_type = 'potential'\n",
    "\n",
    "# # Example 2: Fitting to the potentials with query noise\n",
    "# chrg_n = 3\n",
    "# n_mdl = 100\n",
    "# query_noise = 0.05\n",
    "# mbo_lr = 0.005\n",
    "# n_mboiter = 4000\n",
    "# y_type = 'potential'\n",
    "\n",
    "# # Example 3: Fitting to the fields with noise\n",
    "# chrg_n = 1\n",
    "# n_mdl = 1000\n",
    "# query_noise = 0.05\n",
    "# mbo_lr = 0.005\n",
    "# n_mboiter = 3500\n",
    "# y_type = 'field'\n",
    "\n",
    "# Example 4: Fitting to the fields with query noise\n",
    "chrg_n = 3\n",
    "n_mdl = 100\n",
    "query_noise = 0.0\n",
    "mbo_lr = 0.005\n",
    "n_mboiter = 40000\n",
    "y_type = 'field'\n",
    "\n",
    "# Common settings\n",
    "dim = 2\n",
    "n_seeds = 100\n",
    "n_true = 50\n",
    "opt_siglog = False\n",
    "init_std = 0.01\n",
    "\n",
    "sdim = chrg_n * dim\n",
    "import torch.distributions\n",
    "tch_normal = torch.distributions.normal.Normal\n",
    "prior_dist = tch_normal(loc=torch.zeros(sdim, device=tch_device, dtype=tch_dtype),\n",
    "                        scale=torch.ones(sdim, device=tch_device, dtype=tch_dtype))\n",
    "\n",
    "tch_device = torch.device('cuda:0')\n",
    "tch_dtype = torch.float32\n",
    "\n",
    "# Creating the RNG\n",
    "seeds_arr = (np.arange(n_seeds) * 1000).tolist()\n",
    "rng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "       device=tch_device, dtype=tch_dtype,\n",
    "       unif_cache_cols=1_000,\n",
    "       norm_cache_cols=5_000)\n",
    "rng.seed(np.array(seeds_arr))\n",
    "\n",
    "# Creating the true problem\n",
    "true_chrg_w = np.ones((n_seeds, chrg_n))\n",
    "assert true_chrg_w.shape == (n_seeds, chrg_n)\n",
    "true_chrg_mu = np.array([[-0.5, -0.5],\n",
    "                         [ 0.5,  0.5],\n",
    "                         [ 0.0,  0.0]])[:chrg_n, :]\n",
    "true_chrg_mu = np.broadcast_to(true_chrg_mu[None, ...], \n",
    "    (n_seeds, chrg_n, dim)).copy()\n",
    "assert true_chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "true_problem = DeltaProblem(weights=true_chrg_w, \n",
    "    locations=true_chrg_mu,\n",
    "    tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "\n",
    "true_x = rng.uniform((n_seeds, n_true, dim)) * 2 - 1\n",
    "assert true_x.shape == (n_seeds, n_true, dim)\n",
    "if y_type == 'potential':\n",
    "    y_dim = 1\n",
    "    mse_mul = 100\n",
    "    mse_clip = np.inf\n",
    "    true_y = true_problem.potential(true_x).unsqueeze(-1)\n",
    "    assert true_y.shape == (n_seeds, n_true, 1)\n",
    "elif y_type == 'field':\n",
    "    y_dim = dim\n",
    "    mse_mul = 1\n",
    "    mse_clip = 10\n",
    "    true_y = true_problem.field(true_x)\n",
    "    assert true_y.shape == (n_seeds, n_true, y_dim)\n",
    "else:\n",
    "    raise ValueError(f'y_type={y_type} undefined')\n",
    "\n",
    "\n",
    "mbo = GPMBO(dim=sdim, n_mdl=n_mdl, n_seeds=n_seeds, \n",
    "            lr=mbo_lr, init_mu=0.0, init_std=init_std, gamma=0.0, \n",
    "            yb_gamma=0.9, rng=rng, optim='adam',\n",
    "            tch_device=tch_device, tch_dtype=tch_dtype, \n",
    "            opt_siglog=opt_siglog)\n",
    "\n",
    "all_mbo_mu = []\n",
    "for mbo_iter in range(n_mboiter):\n",
    "    php_query = mbo.ask()\n",
    "    assert php_query.shape == (n_seeds, n_mdl, sdim)\n",
    "    \n",
    "    # Adding noise to the query to make the problem more challenging\n",
    "    php_query = php_query + query_noise * rng.normal((n_seeds, n_mdl, sdim))\n",
    "    assert php_query.shape == (n_seeds, n_mdl, sdim)\n",
    "\n",
    "    # Running a fake query system\n",
    "    loc_query = php_query.reshape(n_seeds*n_mdl, chrg_n, dim).detach().cpu().numpy()\n",
    "    w_query = torch.ones(n_seeds*n_mdl, chrg_n).detach().cpu().numpy()\n",
    "\n",
    "    problem_query = DeltaProblem(weights=w_query, \n",
    "        locations=loc_query,\n",
    "        tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "\n",
    "    x_query = true_x.reshape(n_seeds, 1, n_true, dim)\n",
    "    x_query = x_query.expand(n_seeds, n_mdl, n_true, dim)\n",
    "    x_query = x_query.reshape(n_seeds*n_mdl, n_true, dim)\n",
    "    \n",
    "    if y_type == 'potential':\n",
    "        y_query_ = problem_query.potential(x_query).unsqueeze(-1)\n",
    "        assert y_query_.shape == (n_seeds*n_mdl, n_true, y_dim)\n",
    "    elif y_type == 'field':\n",
    "        y_query_ = problem_query.field(x_query)\n",
    "        assert y_query_.shape == (n_seeds*n_mdl, n_true, y_dim)\n",
    "    else:\n",
    "        raise ValueError(f'y_type={y_type} undefined')\n",
    "\n",
    "    y_query = y_query_.reshape(n_seeds, n_mdl, n_true, y_dim)\n",
    "    assert y_query.shape == (n_seeds, n_mdl, n_true, y_dim)\n",
    "\n",
    "    y_err = y_query - true_y.reshape(n_seeds, 1, n_true, y_dim)\n",
    "    assert y_err.shape == (n_seeds, n_mdl, n_true, y_dim)\n",
    "\n",
    "    y_mse_ = y_err.square().sum(dim=-1)\n",
    "    assert y_mse_.shape == (n_seeds, n_mdl, n_true)\n",
    "    \n",
    "    y_mse_ = torch.clip(y_mse_ , 0.0, mse_clip)\n",
    "    assert y_mse_.shape == (n_seeds, n_mdl, n_true)\n",
    "    \n",
    "    y_mse = mse_mul * y_mse_.mean(dim=-1)\n",
    "    assert y_mse.shape == (n_seeds, n_mdl)\n",
    "    \n",
    "    if mbo_iter % 100 == 0:\n",
    "        print(f'MSE: {y_mse.mean():.4f} +/- {y_mse.std()/np.sqrt(n_seeds*n_mdl):.4f}')\n",
    "\n",
    "    mbo_score = -y_mse + prior_dist.log_prob(php_query).sum(dim=-1).detach() * 1.0\n",
    "    assert mbo_score.shape == (n_seeds, n_mdl)\n",
    "    \n",
    "    mbo.tell(mbo_score)\n",
    "    all_mbo_mu.append(mbo.mu)\n",
    "    \n",
    "mbo_mus_ = torch.cat(all_mbo_mu, dim=-2)\n",
    "assert mbo_mus_.shape == (n_seeds, n_mboiter, chrg_n*dim)\n",
    "mbo_mus = mbo_mus_.reshape(n_seeds, n_mboiter, chrg_n, dim)\n",
    "assert mbo_mus.shape == (n_seeds, n_mboiter, chrg_n, dim)\n",
    "\n",
    "print('The final Gaussian means:')\n",
    "a = mbo.mu.reshape(n_seeds, chrg_n*dim).detach().cpu().numpy()\n",
    "print(np.array_str(a, precision=3, suppress_small=True) + '\\n' + '-'*80)\n",
    "\n",
    "if opt_siglog:\n",
    "    print('The final Gaussian stds:')\n",
    "    a = mbo.sig_log.exp().reshape(n_seeds, chrg_n*dim).detach().cpu().numpy()\n",
    "    print(np.array_str(a, precision=3, suppress_small=True) + '\\n' + '-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44ea75",
   "metadata": {},
   "source": [
    "## Meta-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0bde6",
   "metadata": {
    "code_folding": [
     0,
     64,
     77,
     84,
     93,
     108,
     118,
     149,
     260,
     363,
     402,
     418,
     426,
     436
    ]
   },
   "outputs": [],
   "source": [
    "def set_configs():\n",
    "    #########################################################\n",
    "    ################### Mandatory Options ###################\n",
    "    #########################################################\n",
    "    rng_seed_list = list(range(0, 100_000, 1000))\n",
    "    dim = 2\n",
    "\n",
    "    n_srf = 400\n",
    "    n_srfpts_mdl = 1\n",
    "    n_srfpts_trg = 1\n",
    "    do_detspacing = False\n",
    "    do_dblsampling = False\n",
    "\n",
    "    do_bootstrap = True\n",
    "\n",
    "    # # The original values for 3-charges\n",
    "    # tau = 0.999\n",
    "    # w_trgreg = 1.0\n",
    "    # w_trg = 0.99\n",
    "\n",
    "    # Fast training values for 1-charge\n",
    "    tau = 0.984\n",
    "    w_trgreg = 2.0\n",
    "    w_trg = 0.99\n",
    "    \n",
    "    n_srf = 8\n",
    "    n_srfpts_mdl = 1\n",
    "    n_srfpts_trg = 100\n",
    "    do_bootstrap = False\n",
    "\n",
    "    opt_type = 'sgd'\n",
    "    n_epochs = 1_000_000\n",
    "\n",
    "    nn_dstr = 'mlp'\n",
    "    nn_width = 64\n",
    "    nn_hidden = 2\n",
    "    nn_act = 'tanh'\n",
    "\n",
    "    eval_bs = 256\n",
    "    n_evlpnts = 1000\n",
    "    eval_frq = 100\n",
    "    chkpnt_period = 20000\n",
    "    \n",
    "    # Derived options and assertions\n",
    "    n_points = n_srfpts_mdl + n_srfpts_trg\n",
    "\n",
    "    assert not (do_dblsampling) or (n_srfpts_trg > 1)\n",
    "    if w_trg is None:\n",
    "        w_trg = n_srfpts_trg / n_points\n",
    "    assert not (n_srfpts_mdl == 0) or (w_trg == 1.0)\n",
    "    n_rsdls = 2 if do_dblsampling else 1\n",
    "\n",
    "    if eval_bs is None:\n",
    "        eval_bs = max(n_srfpts_mdl, n_srfpts_trg) * n_srf\n",
    "\n",
    "    #########################################################\n",
    "    ########### I/O-Related Options and Operations ##########\n",
    "    #########################################################\n",
    "    device_name = 'cuda:0'\n",
    "    tch_device = torch.device(device_name)\n",
    "    tch_dtype = torch.float32\n",
    "    \n",
    "    globals().update(locals())\n",
    "\n",
    "def make_rng():\n",
    "    #########################################################\n",
    "    ########### Constructing the Batch RNG Object ###########\n",
    "    #########################################################\n",
    "    n_seeds = len(rng_seed_list)\n",
    "    rng_seeds = np.array(rng_seed_list)\n",
    "    rng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "                   device=tch_device, dtype=tch_dtype,\n",
    "                   unif_cache_cols=1_000_000,\n",
    "                   norm_cache_cols=5_000_000)\n",
    "    rng.seed(np.broadcast_to(rng_seeds, rng.shape))\n",
    "    globals().update(locals())\n",
    "\n",
    "def make_problem():\n",
    "    chrg_w = np.ones((n_seeds, chrg_n))\n",
    "    chrg_mu = np.zeros((n_seeds, chrg_n, dim))\n",
    "    problem = DeltaProblem(weights=chrg_w, locations=chrg_mu,\n",
    "        tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "    globals().update(locals())    \n",
    "\n",
    "def make_vol_samplers():\n",
    "    vol_c_params = dict(c=np.zeros((n_seeds, dim)), r=np.ones(n_seeds))\n",
    "    vol_r_params = dict(low=np.zeros(n_seeds), high=np.ones(n_seeds))\n",
    "    volsampler = BallSampler(c_dstr='ball', c_params=vol_c_params,\n",
    "                             r_dstr='unifdpow', r_params=vol_r_params,\n",
    "                             batch_rng=rng)\n",
    "    srfsampler = SphereSampler(batch_rng=rng)\n",
    "    globals().update(locals())\n",
    "    \n",
    "def prep_tb():\n",
    "    storage_dir = './15_search/02_meta/'\n",
    "    pathlib.Path(storage_dir).mkdir(parents=True, exist_ok=True)\n",
    "    strgidx = sum(isdir(f'{storage_dir}/{x}') for x in os.listdir(storage_dir))\n",
    "    dtnow = datetime.datetime.now().isoformat(timespec='seconds')\n",
    "    dtnow_ = dtnow[2:].replace('-', '').replace(':', '').replace('.', '')\n",
    "    cfgstrg_dir = f'{storage_dir}/{strgidx:02d}_{dtnow_}'\n",
    "    pathlib.Path(cfgstrg_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #if 'tbwriter' in locals():\n",
    "    #    tbwriter.close()\n",
    "    tbwriter = tensorboardX.SummaryWriter(cfgstrg_dir)\n",
    "    logging.getLogger(\"tensorboardX.x2num\").setLevel(logging.CRITICAL)\n",
    "    globals().update(locals())\n",
    "    \n",
    "def make_model():\n",
    "    # Initializing the model\n",
    "    model = bffnn(dim, nn_width, nn_hidden, nn_act, (n_seeds,), rng)\n",
    "    if do_bootstrap:\n",
    "        target = bffnn(dim, nn_width, nn_hidden, nn_act, (n_seeds,), rng)\n",
    "        target.load_state_dict(model.state_dict())\n",
    "    else:\n",
    "        target = model\n",
    "    globals().update(locals())\n",
    "\n",
    "def pre_train_prep():\n",
    "    # Evaluation tools\n",
    "    erng = rng\n",
    "    last_perfdict = dict()\n",
    "    ema = EMA(gamma=0.999, gamma_sq=0.998)\n",
    "    trn_sttime = time.time()\n",
    "\n",
    "    # Constructing the grid points\n",
    "    with torch.no_grad():\n",
    "        n_gpd = 50\n",
    "        n_g = (n_gpd ** dim)\n",
    "        elow, ehigh = -np.ones(dim), np.ones(dim)\n",
    "        gdict = make_grid(elow, ehigh, dim, n_gpd, 'torch')\n",
    "        grid_x_ = gdict['x']\n",
    "        assert grid_x_.shape == (n_g, dim)\n",
    "        grid_x = grid_x_.reshape(1, n_g, dim).expand(n_seeds, n_g, dim)\n",
    "        grid_x = grid_x.to(tch_device, tch_dtype)\n",
    "        assert grid_x.shape == (n_seeds, n_g, dim)\n",
    "\n",
    "        x1_msh_np, x2_msh_np = gdict['xi_msh_np']\n",
    "\n",
    "    with plt.style.context('default'):\n",
    "        figax_list = [plt.subplots(1, 1, figsize=(3.2, 2.5), dpi=100) for _ in range(3)]\n",
    "        (fig_mdl, ax_mdl), (fig_trg, ax_trg), (fig_gt, ax_gt) = figax_list\n",
    "        cax_list = [make_axes_locatable(ax).append_axes('right', size='5%', pad=0.05) \n",
    "                    for ax in (ax_mdl, ax_trg, ax_gt)]\n",
    "        cax_mdl, cax_trg, cax_gt = cax_list\n",
    "    stat_history = defaultdict(list)\n",
    "    train_history = odict()\n",
    "    globals().update(locals())\n",
    "\n",
    "def calc_grad():\n",
    "    # Sampling the points from the srferes\n",
    "    srfsamps = srfsampler(volsamps, n_points, do_detspacing=do_detspacing)\n",
    "    points = nn.Parameter(srfsamps['points'])\n",
    "    surfacenorms = srfsamps['normals']\n",
    "    areas = srfsamps['areas']\n",
    "    assert points.shape == (n_seeds, n_srf, n_points, dim)\n",
    "    assert surfacenorms.shape == (n_seeds, n_srf, n_points, dim)\n",
    "    assert areas.shape == (n_seeds, n_srf,)\n",
    "\n",
    "    points_mdl = points[:, :, :n_srfpts_mdl, :]\n",
    "    assert points_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "    points_trg = points[:, :, n_srfpts_mdl:, :]\n",
    "    assert points_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "    surfacenorms_mdl = surfacenorms[:, :, :n_srfpts_mdl, :]\n",
    "    assert surfacenorms_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "    surfacenorms_trg = surfacenorms[:, :, n_srfpts_mdl:, :]\n",
    "    assert surfacenorms_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "    # Making surface integral predictions using the reference model\n",
    "    u_mdl = model(points_mdl)\n",
    "    assert u_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, 1)\n",
    "    nabla_x_u_mdl, = torch.autograd.grad(u_mdl.sum(), [points_mdl],\n",
    "        grad_outputs=None, retain_graph=True, create_graph=True,\n",
    "        only_inputs=True, allow_unused=False)\n",
    "    assert nabla_x_u_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "    normprods_mdl = (nabla_x_u_mdl * surfacenorms_mdl).sum(dim=-1)\n",
    "    assert normprods_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl)\n",
    "    if n_srfpts_mdl > 0:\n",
    "        mean_normprods_mdl = normprods_mdl.mean(dim=-1, keepdim=True)\n",
    "        assert mean_normprods_mdl.shape == (n_seeds, n_srf, 1)\n",
    "    else:\n",
    "        mean_normprods_mdl = 0.0\n",
    "\n",
    "    # Making surface integral predictions using the target model\n",
    "    u_trg = target(points_trg)\n",
    "    assert u_trg.shape == (n_seeds, n_srf, n_srfpts_trg, 1)\n",
    "    nabla_x_u_trg, = torch.autograd.grad(u_trg.sum(), [points_trg],\n",
    "        grad_outputs=None, retain_graph=True, create_graph=not(do_bootstrap),\n",
    "        only_inputs=True, allow_unused=False)\n",
    "    assert nabla_x_u_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "    normprods_trg = (nabla_x_u_trg * surfacenorms_trg).sum(dim=-1)\n",
    "    assert normprods_trg.shape == (n_seeds, n_srf, n_srfpts_trg)\n",
    "    if do_dblsampling:\n",
    "        assert n_rsdls == 2\n",
    "\n",
    "        mean_normprods_trg1 = normprods_trg[..., 0::2].mean(\n",
    "            dim=-1, keepdim=True)\n",
    "        assert mean_normprods_trg1.shape == (n_seeds, n_srf, 1)\n",
    "\n",
    "        mean_normprods_trg2 = normprods_trg[..., 1::2].mean(\n",
    "            dim=-1, keepdim=True)\n",
    "        assert mean_normprods_trg2.shape == (n_seeds, n_srf, 1)\n",
    "\n",
    "        mean_normprods_trg = torch.cat(\n",
    "            [mean_normprods_trg1, mean_normprods_trg2], dim=-1)\n",
    "        assert mean_normprods_trg.shape == (n_seeds, n_srf, n_rsdls)\n",
    "    else:\n",
    "        assert n_rsdls == 1\n",
    "\n",
    "        mean_normprods_trg = normprods_trg.mean(dim=-1, keepdim=True)\n",
    "        assert mean_normprods_trg.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "    # Linearly combining the reference and target predictions\n",
    "    mean_normprods = (       w_trg  * mean_normprods_trg +\n",
    "                      (1.0 - w_trg) * mean_normprods_mdl)\n",
    "    assert mean_normprods.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "    # Considering the surface areas\n",
    "    pred_surfintegs = mean_normprods * areas.reshape(n_seeds, n_srf, 1)\n",
    "    assert pred_surfintegs.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "    # Getting the reference volume integrals\n",
    "    ref_volintegs = problem.integrate_volumes(volsamps)\n",
    "    assert ref_volintegs.shape == (n_seeds, n_srf)\n",
    "\n",
    "    # Getting the residual terms\n",
    "    resterms = pred_surfintegs - ref_volintegs.reshape(n_seeds, n_srf, 1)\n",
    "    assert resterms.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "    # Multiplying the residual terms\n",
    "    if do_dblsampling:\n",
    "        resterms_prod = resterms.prod(dim=-1)\n",
    "        assert resterms_prod.shape == (n_seeds, n_srf)\n",
    "    else:\n",
    "        resterms_prod = torch.square(resterms).squeeze(-1)\n",
    "        assert resterms_prod.shape == (n_seeds, n_srf)\n",
    "\n",
    "    # Computing the main loss\n",
    "    loss_main = resterms_prod.mean(-1)\n",
    "    assert loss_main.shape == (n_seeds,)\n",
    "\n",
    "    if do_bootstrap:\n",
    "        with torch.no_grad():\n",
    "            u_mdl_prime = target(points_mdl)\n",
    "        loss_trgreg = torch.square(u_mdl - u_mdl_prime).mean([-3, -2, -1])\n",
    "        assert loss_trgreg.shape == (n_seeds,)\n",
    "    else:\n",
    "        loss_trgreg = torch.zeros(n_seeds, device=tch_device, dtype=tch_dtype)\n",
    "        assert loss_trgreg.shape == (n_seeds,)\n",
    "\n",
    "    # The total loss\n",
    "    loss = loss_main + w_trgreg * loss_trgreg\n",
    "    assert loss.shape == (n_seeds,)\n",
    "\n",
    "    loss_sum = loss.sum()\n",
    "    loss_sum.backward()\n",
    "    globals().update(locals())\n",
    "\n",
    "def print_tb_fig():\n",
    "    with torch.no_grad():\n",
    "        grid_prbsol = get_prob_sol(problem, grid_x, n_eval=eval_bs, \n",
    "            get_field=False, out_lib='numpy')\n",
    "        assert grid_prbsol['v_np'].shape == (n_seeds, n_g)\n",
    "\n",
    "        grid_mdlsol = get_nn_sol(model, grid_x, n_eval=eval_bs,\n",
    "            get_field=False, out_lib='numpy')\n",
    "        assert grid_mdlsol['v_np'].shape == (n_seeds, n_g)\n",
    "\n",
    "        if do_bootstrap:\n",
    "            grid_trgsol = get_nn_sol(target, grid_x, n_eval=eval_bs, \n",
    "                get_field=False, out_lib='numpy')\n",
    "            assert grid_trgsol['v_np'].shape == (n_seeds, n_g)\n",
    "\n",
    "        soltd_list = [('gt', grid_prbsol, fig_gt, ax_gt, cax_gt, 'Ground Truth'),\n",
    "                      ('mdl', grid_mdlsol, fig_mdl, ax_mdl, cax_mdl, 'Prediction')]\n",
    "        if do_bootstrap:\n",
    "            soltd_list += [('trg', grid_trgsol, fig_trg, ax_trg, cax_trg, 'Target')]\n",
    "        for sol_t, sol_dict, fig, ax, cax, ttl in soltd_list:\n",
    "            plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=fig, ax=ax, cax=cax)\n",
    "            ax.set_title(ttl)\n",
    "            fig.set_tight_layout(True)\n",
    "            tbwriter.add_figure(f'viz/{sol_t}', fig, epoch)\n",
    "        tbwriter.flush()\n",
    "    globals().update(locals())\n",
    "    \n",
    "    if epoch % eval_frq == 0:\n",
    "        # Sampling the evaluation points\n",
    "        def print_tb_perf():\n",
    "            with torch.no_grad():\n",
    "                evols = volsampler(n=n_evlpnts)\n",
    "                assert evols['type'] == 'ball'\n",
    "\n",
    "                e_c = evols['centers']\n",
    "                assert e_c.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                e_r_ = evols['radii']\n",
    "                assert e_r_.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "                e_r = e_r_.unsqueeze(dim=-1)\n",
    "                assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                untrd = erng.uniform((n_seeds, n_evlpnts, 1))\n",
    "                assert untrd.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                untr = untrd.pow(1.0 / dim)\n",
    "                assert untr.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                e_pntrs = untr * e_r\n",
    "                assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                etheta = erng.normal((n_seeds, n_evlpnts, dim))\n",
    "                assert etheta.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                ethtilde = etheta / etheta.norm(dim=-1, keepdim=True)\n",
    "                assert ethtilde.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                e_pnts = e_c + ethtilde * e_pntrs\n",
    "                assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "            eperfs = eval_pnts(problem, model, target, e_pnts, do_bootstrap,\n",
    "                n_seeds, n_evlpnts, dim, eval_bs)\n",
    "            for kk, vv in eperfs.items():\n",
    "                if '/bc/' in kk:\n",
    "                    tbwriter.add_scalar(f'perf/{kk}', vv.mean(), epoch)\n",
    "            globals().update(locals())\n",
    "    \n",
    "    def print_tb_scalars():\n",
    "        # computing the normal product variances\n",
    "        with torch.no_grad(): \n",
    "            normprods = torch.cat([normprods_mdl, normprods_trg], dim=-1)\n",
    "            npvm = (normprods.var(dim=-1)*areas.square()).mean(-1)\n",
    "\n",
    "        # Computing the loss moving averages\n",
    "        loss_ema_mean, loss_ema_std_mean = ema('loss', loss)\n",
    "        npvm_ema_mean, npvm_ema_std_mean = ema('npvm', npvm)\n",
    "\n",
    "        tbwriter.add_scalar('loss/total', loss.mean(), epoch)\n",
    "        tbwriter.add_scalar('loss/main', loss_main.mean(), epoch)\n",
    "        tbwriter.add_scalar('loss/trgreg', loss_trgreg.mean(), epoch)\n",
    "        tbwriter.add_scalar('loss/npvm', npvm.mean(), epoch)\n",
    "        globals().update(locals())\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        def print_stdout():\n",
    "            print_str = f'Epoch {epoch}, EMA loss = {loss_ema_mean:.4f}'\n",
    "            print_str += f' +/- {2*loss_ema_std_mean:.4f}'\n",
    "            print_str += f', EMA Field-Norm Product Variance = {npvm_ema_mean:.4f}'\n",
    "            print_str += f' +/- {2*npvm_ema_std_mean:.4f} ({time.time()-trn_sttime:0.1f} s)'\n",
    "            print(print_str, flush=True)\n",
    "            globals().update(locals())\n",
    "    \n",
    "    if epoch % chkpnt_period == 0:\n",
    "        def take_checkpoint():\n",
    "            train_history[f'{epoch}/mdl'] = deepcopy({k: v.cpu() for k, v\n",
    "                in model.state_dict().items()})\n",
    "            train_history[f'{epoch}/trg'] = deepcopy({k: v.cpu() for k, v\n",
    "                in target.state_dict().items()})\n",
    "            train_history[f'{epoch}/prb'] = deepcopy({k: v.cpu() for k, v\n",
    "                in problem.state_dict().items()})\n",
    "            globals().update(locals())    \n",
    "\n",
    "def print_tb_perf():\n",
    "    with torch.no_grad():\n",
    "        evols = volsampler(n=n_evlpnts)\n",
    "        assert evols['type'] == 'ball'\n",
    "\n",
    "        e_c = evols['centers']\n",
    "        assert e_c.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "        e_r_ = evols['radii']\n",
    "        assert e_r_.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "        e_r = e_r_.unsqueeze(dim=-1)\n",
    "        assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "        untrd = erng.uniform((n_seeds, n_evlpnts, 1))\n",
    "        assert untrd.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "        untr = untrd.pow(1.0 / dim)\n",
    "        assert untr.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "        e_pntrs = untr * e_r\n",
    "        assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "        etheta = erng.normal((n_seeds, n_evlpnts, dim))\n",
    "        assert etheta.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "        ethtilde = etheta / etheta.norm(dim=-1, keepdim=True)\n",
    "        assert ethtilde.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "        e_pnts = e_c + ethtilde * e_pntrs\n",
    "        assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "    eperfs = eval_pnts(problem, model, target, e_pnts, do_bootstrap,\n",
    "        n_seeds, n_evlpnts, dim, eval_bs)\n",
    "    for kk, vv in eperfs.items():\n",
    "        if '/bc/' in kk:\n",
    "            tbwriter.add_scalar(f'perf/{kk}', vv.mean(), epoch)\n",
    "    globals().update(locals())\n",
    "\n",
    "def print_tb_scalars():\n",
    "    # computing the normal product variances\n",
    "    with torch.no_grad(): \n",
    "        normprods = torch.cat([normprods_mdl, normprods_trg], dim=-1)\n",
    "        npvm = (normprods.var(dim=-1)*areas.square()).mean(-1)\n",
    "\n",
    "    # Computing the loss moving averages\n",
    "    loss_ema_mean, loss_ema_std_mean = ema('loss', loss)\n",
    "    npvm_ema_mean, npvm_ema_std_mean = ema('npvm', npvm)\n",
    "\n",
    "    tbwriter.add_scalar('loss/total', loss.mean(), epoch)\n",
    "    tbwriter.add_scalar('loss/main', loss_main.mean(), epoch)\n",
    "    tbwriter.add_scalar('loss/trgreg', loss_trgreg.mean(), epoch)\n",
    "    tbwriter.add_scalar('loss/npvm', npvm.mean(), epoch)\n",
    "    globals().update(locals())\n",
    "\n",
    "def print_stdout():\n",
    "    print_str = f'Epoch {epoch}, EMA loss = {loss_ema_mean:.4f}'\n",
    "    print_str += f' +/- {2*loss_ema_std_mean:.4f}'\n",
    "    print_str += f', EMA Field-Norm Product Variance = {npvm_ema_mean:.4f}'\n",
    "    print_str += f' +/- {2*npvm_ema_std_mean:.4f} ({time.time()-trn_sttime:0.1f} s)'\n",
    "    print(print_str, flush=True)\n",
    "    globals().update(locals())\n",
    "\n",
    "def take_checkpoint():\n",
    "    train_history[f'{epoch}/mdl'] = deepcopy({k: v.cpu() for k, v\n",
    "        in model.state_dict().items()})\n",
    "    train_history[f'{epoch}/trg'] = deepcopy({k: v.cpu() for k, v\n",
    "        in target.state_dict().items()})\n",
    "    train_history[f'{epoch}/prb'] = deepcopy({k: v.cpu() for k, v\n",
    "        in problem.state_dict().items()})\n",
    "    globals().update(locals())\n",
    "    \n",
    "### Utility functions\n",
    "def sample_unit_ball(n_seeds, chrg_n, dim, rng):\n",
    "    rnd1 = rng.normal((n_seeds, chrg_n, dim))\n",
    "    rnd1 = rnd1 / ((rnd1**2).sum(-1, keepdims=True)**0.5)\n",
    "\n",
    "    rnd2 = rng.uniform((n_seeds, chrg_n, 1))\n",
    "    rnd2 = rnd2 ** (1./dim)\n",
    "\n",
    "    return rnd2 * rnd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ff38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_configs()\n",
    "\n",
    "make_rng()\n",
    "chrg_n = 1\n",
    "make_problem()\n",
    "make_vol_samplers()\n",
    "prep_tb()\n",
    "make_model()\n",
    "\n",
    "# Set the optimizer\n",
    "lr = 0.001\n",
    "opt = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "# Setting up the meta-parameters\n",
    "n_mseeds = 1\n",
    "n_mtasks = n_seeds // n_mseeds\n",
    "assert (n_mseeds * n_mtasks) == n_seeds\n",
    "\n",
    "meta_model_sd = dict()\n",
    "if do_bootstrap:\n",
    "    meta_target_sd = dict()\n",
    "for name, param in model.state_dict().items():\n",
    "    pshape = param.shape[1:]\n",
    "    param1 = param.reshape(n_mseeds, n_mtasks, *pshape)\n",
    "    assert param1.shape == (n_mseeds, n_mtasks, *pshape)\n",
    "    param2 = param1.mean(dim=1)\n",
    "    assert param2.shape == (n_mseeds, *pshape)\n",
    "    meta_model_sd[name] = torch.nn.Parameter(param2.detach())      \n",
    "    if do_bootstrap:\n",
    "        meta_target_sd[name] = param2.clone().detach()\n",
    "meta_opt = torch.optim.SGD(list(meta_model_sd.values()), lr=0.1)\n",
    "\n",
    "pre_train_prep()\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "    with torch.no_grad():\n",
    "        mesd = {name: param.reshape(n_mseeds,        1, *param.shape[1:]\n",
    "                           ).expand(n_mseeds, n_mtasks, *param.shape[1:]\n",
    "                          ).reshape(n_seeds,            *param.shape[1:])\n",
    "                for name, param in meta_model_sd.items()}\n",
    "    model.load_state_dict(mesd)\n",
    "    if do_bootstrap:\n",
    "        with torch.no_grad():\n",
    "            tesd = {name: param.reshape(n_mseeds,        1, *param.shape[1:]\n",
    "                               ).expand(n_mseeds, n_mtasks, *param.shape[1:]\n",
    "                              ).reshape(n_seeds,            *param.shape[1:])\n",
    "                for name, param in meta_target_sd.items()}\n",
    "        target.load_state_dict(tesd)\n",
    "    \n",
    "    # Sampling new tasks\n",
    "    problem.locations = None\n",
    "    problem.locations_tch = sample_unit_ball(n_seeds, chrg_n, dim, rng)\n",
    "    \n",
    "    # Meta-train\n",
    "    opt.zero_grad()\n",
    "    volsamps = volsampler(n=n_srf)\n",
    "    calc_grad()\n",
    "    if (epoch > 0):\n",
    "        opt.step()\n",
    "\n",
    "    model_sd = model.state_dict()\n",
    "    if do_bootstrap and (epoch > 0):\n",
    "        target_sd = target.state_dict()\n",
    "        newtrg_sd = dict()\n",
    "        with torch.no_grad():\n",
    "            for key, param in model_sd.items():\n",
    "                param_trg = target_sd[key]\n",
    "                newtrg_sd[key] = tau * param_trg + (1-tau) * param\n",
    "        target.load_state_dict(newtrg_sd)\n",
    "    \n",
    "    # Meta-validation\n",
    "    volsamps = volsampler(n=n_srf)\n",
    "    calc_grad()\n",
    "    \n",
    "    meta_opt.zero_grad()\n",
    "    \n",
    "    mnp = dict(model.named_parameters())\n",
    "    for name, mparam in meta_model_sd.items():\n",
    "        param = mnp[name]\n",
    "        pshape = param.shape[1:]\n",
    "        assert param.shape == (n_seeds, *pshape)\n",
    "        \n",
    "        pgrad = param.grad\n",
    "        if pgrad is None:\n",
    "            pgrad = torch.zeros_like(param)\n",
    "        assert pgrad.shape == (n_seeds, *pshape)\n",
    "        \n",
    "        mpgrad_ = pgrad.reshape(n_mseeds, n_mtasks, *pshape)\n",
    "        assert mpgrad_.shape == (n_mseeds, n_mtasks, *pshape)\n",
    "        \n",
    "        mpgrad = mpgrad_.mean(dim=1)\n",
    "        assert mpgrad.shape == (n_mseeds, *pshape)\n",
    "        assert mparam.shape == (n_mseeds, *pshape)\n",
    "        \n",
    "        mparam.grad = mpgrad\n",
    "    \n",
    "    if epoch > 0:\n",
    "        meta_opt.step()\n",
    "    \n",
    "    if do_bootstrap and (epoch > 0):\n",
    "        for name, mparam in meta_model_sd.items():\n",
    "            mtparam = meta_target_sd[name]\n",
    "            meta_target_sd[name] = tau * mtparam + (1-tau) * mparam\n",
    "        \n",
    "    if epoch % 1000 == 0:\n",
    "        print_tb_fig()\n",
    "    if epoch % eval_frq == 0:\n",
    "        print_tb_perf()\n",
    "    print_tb_scalars()\n",
    "    if epoch % 100 == 0:\n",
    "        print_stdout()\n",
    "    if epoch % chkpnt_period == 0:\n",
    "        take_checkpoint()\n",
    "        \n",
    "print(f'Training finished in {time.time() - trn_sttime:.1f} seconds.')\n",
    "tbwriter.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = 1, 2 + do_bootstrap\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(\n",
    "    n_cols * 3.5, n_rows * 3), dpi=72, sharex=True, sharey=True)\n",
    "cax = None\n",
    "\n",
    "# Computing the model, target and ground truth solutions\n",
    "prob_sol = get_prob_sol(problem, grid_x, n_eval=eval_bs, get_field=False)\n",
    "with torch.no_grad():\n",
    "    mdl_sol = get_nn_sol(model, grid_x, n_eval=eval_bs, get_field=False) \n",
    "    if do_bootstrap:\n",
    "        trg_sol = get_nn_sol(target, grid_x, n_eval=eval_bs, get_field=False)\n",
    "\n",
    "soltd_list = [('gt', prob_sol, axes[0], 'Ground Truth'),\n",
    "              ('mdl', mdl_sol, axes[1], 'Prediction')]\n",
    "if do_bootstrap:\n",
    "    soltd_list += [('trg', trg_sol, axes[2], 'Target')]\n",
    "for sol_t, sol_dict, ax, ttl in soltd_list:\n",
    "    plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=fig, ax=ax, cax=cax)\n",
    "    ax.set_title(ttl)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298e1d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "torch.save(train_history, f'{cfgstrg_dir}/train_history.pt')\n",
    "\n",
    "hp_dict = dict(dim=dim, n_srf=n_srf, n_srfpts_mdl=n_srfpts_mdl, \n",
    "               n_srfpts_trg=n_srfpts_trg, do_detspacing=do_detspacing, \n",
    "               do_dblsampling=do_dblsampling, do_bootstrap=do_bootstrap,\n",
    "               tau=tau, w_trgreg=w_trgreg, w_trg=w_trg, opt_type=opt_type, \n",
    "               n_epochs=n_epochs, lr=lr, nn_dstr=nn_dstr, nn_width=nn_width, \n",
    "               nn_hidden=nn_hidden, nn_act=nn_act)\n",
    "\n",
    "with open(f'{cfgstrg_dir}/config.json', \"w\") as outfile:\n",
    "    json.dump(hp_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1243c2d",
   "metadata": {},
   "source": [
    "# MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58303d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Fitting to the potentials without query noise\n",
    "chrg_n = 3\n",
    "query_noise = 0.00\n",
    "n_mcmciter = 4000\n",
    "y_type = 'field'\n",
    "\n",
    "# Common settings\n",
    "dim = 2\n",
    "n_seeds = 10000\n",
    "n_true = 50\n",
    "\n",
    "tch_device = torch.device('cuda:0')\n",
    "tch_dtype = torch.float32\n",
    "\n",
    "# Creating the RNG\n",
    "seeds_arr = (np.arange(n_seeds) * 1000).tolist()\n",
    "rng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "       device=tch_device, dtype=tch_dtype,\n",
    "       unif_cache_cols=1_000,\n",
    "       norm_cache_cols=5_000)\n",
    "rng.seed(np.array(seeds_arr))\n",
    "\n",
    "# Creating the true problem\n",
    "true_chrg_w = np.ones((n_seeds, chrg_n))\n",
    "assert true_chrg_w.shape == (n_seeds, chrg_n)\n",
    "true_chrg_mu = np.array([[-0.5, -0.5],\n",
    "                         [ 0.5,  0.5],\n",
    "                         [ 0.0,  0.0]])[:chrg_n, :]\n",
    "true_chrg_mu = np.broadcast_to(true_chrg_mu[None, ...], \n",
    "    (n_seeds, chrg_n, dim)).copy()\n",
    "assert true_chrg_mu.shape == (n_seeds, chrg_n, dim)\n",
    "true_problem = DeltaProblem(weights=true_chrg_w, \n",
    "    locations=true_chrg_mu,\n",
    "    tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "sdim = chrg_n * dim\n",
    "\n",
    "# Tensorboard\n",
    "storage_dir = './15_search/03_mcmc'\n",
    "pathlib.Path(storage_dir).mkdir(parents=True, exist_ok=True)\n",
    "strgidx = sum(isdir(f'{storage_dir}/{x}') for x in os.listdir(storage_dir))\n",
    "dtnow = datetime.datetime.now().isoformat(timespec='seconds')\n",
    "dtnow_ = dtnow[2:].replace('-', '').replace(':', '').replace('.', '')\n",
    "cfgstrg_dir = f'{storage_dir}/{strgidx:02d}_{dtnow_}'\n",
    "pathlib.Path(cfgstrg_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if 'tbwriter' in locals():\n",
    "    tbwriter.close()\n",
    "tbwriter = tensorboardX.SummaryWriter(cfgstrg_dir)\n",
    "logging.getLogger(\"tensorboardX.x2num\").setLevel(logging.CRITICAL) \n",
    "\n",
    "# MCMC Parameters\n",
    "import torch.distributions\n",
    "mu_mcmc = rng.uniform((n_seeds, sdim)) * 2 - 1.0\n",
    "sig_mcmc = 0.01\n",
    "logl_mcmc = None\n",
    "temp_mcmc = 100.0\n",
    "n_cpg = 100\n",
    "tch_normal = torch.distributions.normal.Normal\n",
    "prior_mcmc = tch_normal(loc=torch.zeros(sdim, device=tch_device, dtype=tch_dtype),\n",
    "                        scale=torch.ones(sdim, device=tch_device, dtype=tch_dtype))\n",
    "\n",
    "reset_period_mcmc = n_mcmciter // 2\n",
    "reset_topk_mcmc = n_cpg // 2\n",
    "\n",
    "n_grps = n_seeds // n_cpg\n",
    "assert (n_grps * n_cpg) == n_seeds\n",
    "\n",
    "# Creating the data\n",
    "true_x1 = rng.uniform((n_seeds, n_true, dim)) * 2 - 1\n",
    "assert true_x1.shape == (n_seeds, n_true, dim)\n",
    "true_x2 = true_x1.reshape(n_grps, n_cpg, n_true, dim)\n",
    "assert true_x2.shape == (n_grps, n_cpg, n_true, dim)\n",
    "true_x3 = true_x2[:, :1, ...]\n",
    "assert true_x3.shape == (n_grps, 1, n_true, dim)\n",
    "true_x4 = true_x3.expand(n_grps, n_cpg, n_true, dim)\n",
    "assert true_x4.shape == (n_grps, n_cpg, n_true, dim)\n",
    "true_x = true_x4.reshape(n_seeds, n_true, dim)\n",
    "assert true_x.shape == (n_seeds, n_true, dim)\n",
    "\n",
    "if y_type == 'potential':\n",
    "    y_dim = 1\n",
    "    mse_mul = 100\n",
    "    mse_clip = np.inf\n",
    "    true_y = true_problem.potential(true_x).unsqueeze(-1)\n",
    "    assert true_y.shape == (n_seeds, n_true, 1)\n",
    "elif y_type == 'field':\n",
    "    y_dim = dim\n",
    "    mse_mul = 1\n",
    "    mse_clip = 10\n",
    "    true_y = true_problem.field(true_x)\n",
    "    assert true_y.shape == (n_seeds, n_true, y_dim)\n",
    "else:\n",
    "    raise ValueError(f'y_type={y_type} undefined')\n",
    "\n",
    "\n",
    "trace_mcmc = defaultdict(list)\n",
    "for mcmc_iter in range(n_mcmciter):\n",
    "    with torch.no_grad():\n",
    "        mu_proposed = mu_mcmc\n",
    "        if mcmc_iter > 0:\n",
    "            mu_proposed = mu_proposed + sig_mcmc * rng.normal((n_seeds, sdim))\n",
    "        assert mu_proposed.shape == (n_seeds, sdim)\n",
    "        \n",
    "        # Adding noise to the query to make the problem more challenging\n",
    "        mu_proposed = mu_proposed + query_noise * rng.normal((n_seeds, sdim))\n",
    "        assert mu_proposed.shape == (n_seeds, sdim)\n",
    "\n",
    "        # Running a fake query system\n",
    "        loc_proposed = mu_proposed.reshape(n_seeds, chrg_n, dim).detach().cpu().numpy()\n",
    "        w_proposed = torch.ones(n_seeds, chrg_n).detach().cpu().numpy()\n",
    "\n",
    "        problem_proposed = DeltaProblem(weights=w_proposed, \n",
    "            locations=loc_proposed,\n",
    "            tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "    \n",
    "    if y_type == 'potential':\n",
    "        y_proposed_ = problem_proposed.potential(true_x).unsqueeze(-1)\n",
    "        assert y_proposed_.shape == (n_seeds, n_true, y_dim)\n",
    "    elif y_type == 'field':\n",
    "        y_proposed_ = problem_proposed.field(true_x)\n",
    "        assert y_proposed_.shape == (n_seeds, n_true, y_dim)\n",
    "    else:\n",
    "        raise ValueError(f'y_type={y_type} undefined')\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        y_proposed = y_proposed_.reshape(n_seeds, n_true, y_dim)\n",
    "        assert y_proposed.shape == (n_seeds, n_true, y_dim)\n",
    "\n",
    "        y_err = y_proposed - true_y.reshape(n_seeds, n_true, y_dim)\n",
    "        assert y_err.shape == (n_seeds, n_true, y_dim)\n",
    "\n",
    "        y_mse_ = y_err.square().sum(dim=-1)\n",
    "        assert y_mse_.shape == (n_seeds, n_true)\n",
    "        \n",
    "        y_mse_ = torch.clip(y_mse_ , 0.0, mse_clip)\n",
    "        assert y_mse_.shape == (n_seeds, n_true)\n",
    "        \n",
    "        y_mse = mse_mul * y_mse_.mean(dim=-1)\n",
    "        assert y_mse.shape == (n_seeds,)\n",
    "\n",
    "        \n",
    "        logl_proposed = -y_mse\n",
    "        assert logl_proposed.shape == (n_seeds,)\n",
    "        logpri_proposed = prior_mcmc.log_prob(mu_proposed).sum(-1)\n",
    "        assert logpri_proposed.shape == (n_seeds,)\n",
    "        logpost_proposed = logpri_proposed + logl_proposed\n",
    "        assert logpost_proposed.shape == (n_seeds,)\n",
    "        \n",
    "        logpri_mcmc = prior_mcmc.log_prob(mu_mcmc).sum(-1)\n",
    "        assert logpri_mcmc.shape == (n_seeds,)\n",
    "        if mcmc_iter == 0:\n",
    "            logl_mcmc = logl_proposed\n",
    "            assert logl_mcmc.shape == (n_seeds,)\n",
    "            best_logl_mcmc = logl_mcmc\n",
    "            assert best_logl_mcmc.shape == (n_seeds,)\n",
    "        logpost_mcmc = logpri_mcmc + logl_mcmc\n",
    "        assert logpost_mcmc.shape == (n_seeds,)\n",
    "        if mcmc_iter == 0:\n",
    "            best_logpost_mcmc = logpost_mcmc\n",
    "            assert best_logpost_mcmc.shape == (n_seeds,)\n",
    "        \n",
    "        logpost_diff = logpost_proposed - logpost_mcmc\n",
    "        assert logpost_diff.shape == (n_seeds,)\n",
    "        # logl_diff = logl_proposed - logl_mcmc\n",
    "        # assert logl_diff.shape == (n_seeds,)\n",
    "        # logpri_diff = prior_mcmc.log_prob(mu_proposed).sum(-1) - prior_mcmc.log_prob(mu_mcmc).sum(-1)\n",
    "        # assert logpri_diff.shape == (n_seeds,)\n",
    "        # logpost_diff = logpri_diff + logl_diff\n",
    "        # assert logpost_diff.shape == (n_seeds,)\n",
    "        acceptance_mcmc = (temp_mcmc * logpost_diff.unsqueeze(-1)) > rng.uniform((n_seeds,1)).log()\n",
    "        assert acceptance_mcmc.shape == (n_seeds, 1)\n",
    "        mu_mcmc = torch.where(acceptance_mcmc, mu_proposed, mu_mcmc)\n",
    "        assert mu_mcmc.shape == (n_seeds, sdim)\n",
    "        logl_mcmc = torch.where(acceptance_mcmc.squeeze(-1), logl_proposed, logl_mcmc)\n",
    "        assert logl_mcmc.shape == (n_seeds,)\n",
    "        \n",
    "        best_logl_mcmc = torch.where(logl_mcmc > best_logl_mcmc, logl_mcmc, best_logl_mcmc)\n",
    "        assert best_logl_mcmc.shape == (n_seeds,)\n",
    "        best_logpost_mcmc = torch.where(logpost_mcmc > best_logpost_mcmc, logpost_mcmc, best_logpost_mcmc)\n",
    "        assert best_logpost_mcmc.shape == (n_seeds,)\n",
    "            \n",
    "        trace_mcmc['mu'].append(mu_mcmc)\n",
    "        trace_mcmc['acceptance'].append(acceptance_mcmc)\n",
    "        trace_mcmc['logl'].append(logl_mcmc)\n",
    "        \n",
    "        blm = best_logl_mcmc.reshape(n_grps, n_cpg)\n",
    "        tbwriter.add_scalar('acceptance', acceptance_mcmc.float().mean(), mcmc_iter)\n",
    "        for q in np.linspace(0, 1, 5):\n",
    "            tbwriter.add_scalar(f'best_logl/q{int(100*q):02d}', \n",
    "                                blm.quantile(q, dim=-1).median(), mcmc_iter)\n",
    "        tbwriter.add_scalar('logl', logl_mcmc.mean(), mcmc_iter)\n",
    "        \n",
    "        if mcmc_iter % 100 == 0:\n",
    "            print(f'MCMC Iteration {mcmc_iter:04d}: Best MSE: {-best_logl_mcmc.max():.4f}')\n",
    "            tbwriter.flush()\n",
    "        \n",
    "        if (mcmc_iter % reset_period_mcmc == 0) and (mcmc_iter > 0):\n",
    "            mu_mcmc_ = mu_mcmc.reshape(n_grps, n_cpg, sdim)\n",
    "            assert mu_mcmc_.shape == (n_grps, n_cpg, sdim)\n",
    "            \n",
    "            logpost_mcmc_ = logpost_mcmc.reshape(n_grps, n_cpg)\n",
    "            assert logpost_mcmc_.shape == (n_grps, n_cpg)\n",
    "            \n",
    "            top_idx = torch.topk(logpost_mcmc_, reset_topk_mcmc, dim=1, largest=True, sorted=True).indices\n",
    "            assert top_idx.shape == (n_grps, reset_topk_mcmc)\n",
    "            \n",
    "            top_chains = torch.take_along_dim(mu_mcmc_, top_idx.unsqueeze(-1), dim=-2)\n",
    "            assert top_chains.shape == (n_grps, reset_topk_mcmc, sdim)\n",
    "            \n",
    "            aa = n_cpg // reset_topk_mcmc\n",
    "            mu_mcmc = top_chains.reshape(n_grps, 1, reset_topk_mcmc, sdim)\n",
    "            mu_mcmc = mu_mcmc.expand(n_grps, aa, reset_topk_mcmc, sdim)\n",
    "            mu_mcmc = mu_mcmc.reshape(n_seeds, sdim)\n",
    "            assert mu_mcmc.shape == (n_seeds, sdim)\n",
    "\n",
    "tbwriter.flush()\n",
    "with torch.no_grad():\n",
    "    trace_mcmc_ = dict()\n",
    "    for key, tnesor_list in trace_mcmc.items():\n",
    "        trace_mcmc_[key] = torch.stack(tnesor_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a12910",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_mu = trace_mcmc_['mu']\n",
    "n_srchdraws, n_seeds, sdim = trace_mu.shape\n",
    "\n",
    "trace_mu = trace_mu.reshape(n_srchdraws, n_seeds, chrg_n, dim)\n",
    "assert trace_mu.shape == (n_srchdraws, n_seeds, chrg_n, dim)\n",
    "\n",
    "trace_mu = trace_mu.detach().cpu().numpy()\n",
    "\n",
    "kdedata = trace_mu[-1].reshape(n_seeds*chrg_n, dim)\n",
    "assert kdedata.shape == (n_seeds*chrg_n, dim)\n",
    "\n",
    "kdedata = kdedata.reshape(n_grps, n_cpg, chrg_n, dim)[1].reshape(n_cpg*chrg_n, dim)\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "fig_srch, ax_srch = plt.subplots(1, 1, dpi=100)\n",
    "\n",
    "# sns.kdeplot(\n",
    "#     x=kdedata[:, 0], y=kdedata[:, 1],\n",
    "#     fill=True, thresh=0, levels=100, cmap=\"RdBu\",\n",
    "# )\n",
    "\n",
    "ax_srch.scatter(kdedata[:, 0], kdedata[:, 1], s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01165ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_mu = trace_mcmc_['mu']\n",
    "n_srchdraws, n_seeds, sdim = trace_mu.shape\n",
    "\n",
    "ii = trace_mcmc_['logl'][700].argmax().item()\n",
    "bst = trace_mu[:, ii, ...].reshape(n_srchdraws, chrg_n, dim).transpose(0, 1)\n",
    "assert bst.shape == (chrg_n, n_srchdraws, dim)\n",
    "\n",
    "bst = bst.detach().cpu().numpy()\n",
    "bst = bst[:, 0:700, :]\n",
    "\n",
    "%matplotlib inline\n",
    "fig_srch, ax_srch = plt.subplots(1, 1, dpi=100)\n",
    "\n",
    "for i_chrg, chrg_traj in enumerate(bst):\n",
    "    ax_srch.plot(chrg_traj[:, 0], chrg_traj[:, 1])\n",
    "    \n",
    "ax_srch.set_xlim(-1, 1)\n",
    "ax_srch.set_ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e1c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "jupytext": {
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8 (default, Feb 24 2021, 21:46:12) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4aec7983bc6059d1b5d440a2253fd0eef7d09b7a26ee33cf7f8716a3ef03c04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
