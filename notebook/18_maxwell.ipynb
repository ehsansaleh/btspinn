{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6d9dd",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "%matplotlib inline\n",
    "if importlib.util.find_spec(\"matplotlib_inline\") is not None:\n",
    "    import matplotlib_inline\n",
    "    matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
    "else:\n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    set_matplotlib_formats('retina')\n",
    "\n",
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956912bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import socket\n",
    "import random\n",
    "import pathlib\n",
    "import fnmatch\n",
    "import datetime\n",
    "import resource\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import psutil\n",
    "from pyinstrument import Profiler\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "from scipy.special import gamma\n",
    "from os.path import exists, isdir\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict as odict\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff95ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bspinn.io_utils import DataWriter\n",
    "from bspinn.io_utils import get_git_commit\n",
    "from bspinn.io_utils import preproc_cfgdict\n",
    "from bspinn.io_utils import hie2deep, deep2hie\n",
    "\n",
    "from bspinn.tch_utils import isscalar\n",
    "from bspinn.tch_utils import EMA\n",
    "from bspinn.tch_utils import BatchRNG\n",
    "from bspinn.tch_utils import bffnn\n",
    "from bspinn.tch_utils import profmem\n",
    "\n",
    "from bspinn.io_cfg import configs_dir\n",
    "from bspinn.io_cfg import results_dir\n",
    "from bspinn.io_cfg import storage_dir\n",
    "\n",
    "#from bspinn.poisson import make_grid\n",
    "# Disabling pytorch deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9396e6b",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Consider the $3$-dimensional space $\\mathbb{R}^{3}$, and the following current along the z-axis:\n",
    "\n",
    "$$\\vec{J}(x) = I \\cdot \\delta^2(x_1=0, x_2=0, x_3\\in [z_1, z_2]).$$\n",
    "\n",
    "The analytical solution to the system\n",
    "\n",
    "$$\\nabla \\times \\vec{A} = \\vec{B}$$\n",
    "\n",
    "$$\\nabla \\times \\vec{B} = \\vec{J}$$\n",
    "\n",
    "can be expressed as\n",
    "\n",
    "$$\\vec{A} = \\frac{-I}{4\\pi} \\begin{bmatrix}0\\\\0\\\\ \n",
    "\\log\\bigg(\\frac{(z_2-x_3) + \n",
    "\\sqrt{x_1^2 + x_2^2 + (z_2-x_3)^2}}{(z_1-x_3) + \n",
    "\\sqrt{x_1^2 + x_2^2 + (z_1 - x_3)^2}}\\bigg)\\end{bmatrix},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\vec{B} = \\frac{-I}{4\\pi\\cdot \\sqrt{x_1^2+x_2^2}} \\cdot \n",
    "\\bigg(\\frac{z_2-x_3}{\\sqrt{x_1^2 + x_2^2 + (z_2-x_3)^2}} - \n",
    "\\frac{z_1-x_3}{\\sqrt{x_1^2 + x_2^2 + (z_1-x_3)^2}}\\bigg) \\cdot \n",
    "\\begin{bmatrix}\\frac{-x_2}{\\sqrt{x_1^2 + x_2^2}}\\\\\n",
    "\\frac{x_1}{\\sqrt{x_1^2 + x_2^2}}\\\\0\\end{bmatrix}.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982dc9b",
   "metadata": {},
   "source": [
    "### Defining the Problem and the Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2d759",
   "metadata": {
    "code_folding": [
     1,
     17,
     80,
     138
    ]
   },
   "outputs": [],
   "source": [
    "class DeltaLineProblem:\n",
    "    def __init__(self, source, sink, current, tch_device, tch_dtype):\n",
    "        n_seeds, n_wires, dim = source.shape\n",
    "        assert source.shape   == (n_seeds, n_wires, dim)\n",
    "        assert sink.shape     == (n_seeds, n_wires, dim)\n",
    "        assert current.shape  == (n_seeds, n_wires)\n",
    "        assert dim == 3\n",
    "        \n",
    "        self.n_seeds = n_seeds\n",
    "        self.n_wires = n_wires\n",
    "        self.source = source\n",
    "        self.sink = sink\n",
    "        self.current = current\n",
    "        self.source_tch = torch.from_numpy(source).to(tch_device, tch_dtype)\n",
    "        self.sink_tch = torch.from_numpy(sink).to(tch_device, tch_dtype)\n",
    "        self.current_tch = torch.from_numpy(current).to(tch_device, tch_dtype)\n",
    "        \n",
    "        self.shape = (self.n_seeds,)\n",
    "        self.ndim = 1\n",
    "        \n",
    "    def integrate_volumes(self, volumes):\n",
    "        n_seeds, n_wires = self.n_seeds, self.n_wires\n",
    "        source, sink, current = self.source_tch, self.sink_tch, self.current_tch\n",
    "        dim = 3\n",
    "        \n",
    "        assert volumes['type'] == 'disk'\n",
    "        centers = volumes['centers']\n",
    "        radii = volumes['radii']\n",
    "        normals = volumes['normals'].squeeze(-1)\n",
    "        \n",
    "        n_vol = radii.shape[-1]\n",
    "        \n",
    "        assert centers.shape == (n_seeds, n_vol, dim), (centers.shape, (n_seeds, n_vol, dim))\n",
    "        assert radii.shape   == (n_seeds, n_vol)\n",
    "        assert normals.shape == (n_seeds, n_vol, dim)\n",
    "        \n",
    "        assert source.shape  == (n_seeds, n_wires, dim)\n",
    "        assert sink.shape    == (n_seeds, n_wires, dim)\n",
    "        assert current.shape == (n_seeds, n_wires)\n",
    "        \n",
    "        sink2cntr = centers.reshape(n_seeds, n_vol, 1, dim) - sink.reshape(n_seeds, 1, n_wires, dim)\n",
    "        assert sink2cntr.shape == (n_seeds, n_vol, n_wires, dim)\n",
    "        \n",
    "        src2snk = source - sink\n",
    "        assert src2snk.shape == (n_seeds, n_wires, dim)\n",
    "        \n",
    "        lam_numer = (sink2cntr * normals.reshape(n_seeds, n_vol, 1, dim)).sum(dim=-1)\n",
    "        assert lam_numer.shape == (n_seeds, n_vol, n_wires)\n",
    "        \n",
    "        lam_denom = (src2snk.reshape(n_seeds, 1, n_wires, dim) * \n",
    "                     normals.reshape(n_seeds, n_vol,   1, dim)).sum(dim=-1)\n",
    "        assert lam_denom.shape == (n_seeds, n_vol, n_wires)\n",
    "        \n",
    "        # The interpolation coefficient between source and sink that \n",
    "        # lies on the disk plane: $lambda * source  + (1 - lambda) sink$\n",
    "        lam = lam_numer / lam_denom\n",
    "        assert lam.shape == (n_seeds, n_vol, n_wires)\n",
    "        \n",
    "        lam_ss = lam.reshape(n_seeds, n_vol, n_wires, 1) * src2snk.reshape(n_seeds, 1, n_wires, dim)\n",
    "        assert lam_ss.shape == (n_seeds, n_vol, n_wires, dim)\n",
    "        \n",
    "        # The difference between the wire projection and the volum center\n",
    "        p2c = sink2cntr - lam_ss\n",
    "        assert p2c.shape == (n_seeds, n_vol, n_wires, dim)\n",
    "        \n",
    "        # The projection to center radius\n",
    "        p2c_r = p2c.square().sum(dim=-1).sqrt() \n",
    "        assert p2c_r.shape == (n_seeds, n_vol, n_wires)\n",
    "        \n",
    "        is_within_r = p2c_r < radii.reshape(n_seeds, n_vol, 1)\n",
    "        assert is_within_r.shape == (n_seeds, n_vol, n_wires)\n",
    "        \n",
    "        is_within_ss = (lam <= 1) * (lam >= 0)\n",
    "        assert is_within_ss.shape == (n_seeds, n_vol, n_wires)\n",
    "        \n",
    "        valid_cross = is_within_r * is_within_ss\n",
    "        assert valid_cross.shape == (n_seeds, n_vol, n_wires)\n",
    "        \n",
    "        sign = (normals.reshape(n_seeds, n_vol, 1, dim) * \n",
    "                src2snk.reshape(n_seeds, 1, n_wires, dim)).sum(dim=-1).sign()\n",
    "        assert sign.shape == (n_seeds, n_vol, n_wires)\n",
    "        \n",
    "        current_enc = (valid_cross * sign * current.reshape(n_seeds, 1, n_wires)).sum(dim=-1)\n",
    "        assert current_enc.shape == (n_seeds, n_vol)\n",
    "        \n",
    "        return current_enc\n",
    "    \n",
    "    def potential(self, x, eps=1e-10):\n",
    "        n_seeds, n_wires = self.n_seeds, self.n_wires\n",
    "        source, sink, current = self.source_tch, self.sink_tch, self.current_tch\n",
    "        \n",
    "        n_seeds, n_x, dim = x.shape \n",
    "        assert x.shape == (n_seeds, n_x, dim)\n",
    "        \n",
    "        src2snk = sink - source\n",
    "        assert src2snk.shape == (n_seeds, n_wires, dim)\n",
    "        \n",
    "        src2snk_dist = src2snk.norm(dim=-1, keepdim=True)\n",
    "        assert src2snk_dist.shape == (n_seeds, n_wires, 1)\n",
    "        \n",
    "        src2snk_norm = src2snk / src2snk_dist\n",
    "        assert src2snk_norm.shape == (n_seeds, n_wires, dim)\n",
    "        \n",
    "        x2src = source.reshape(n_seeds, 1, n_wires, dim) - x.reshape(n_seeds, n_x, 1, dim)\n",
    "        assert x2src.shape == (n_seeds, n_x, n_wires, dim)\n",
    "        \n",
    "        z1 = (x2src * src2snk_norm.reshape(n_seeds, 1, n_wires, dim)).sum(dim=-1)\n",
    "        assert z1.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        x2snk = sink.reshape(n_seeds, 1, n_wires, dim) - x.reshape(n_seeds, n_x, 1, dim)\n",
    "        assert x2snk.shape == (n_seeds, n_x, n_wires, dim)\n",
    "        \n",
    "        z2 = (x2snk * src2snk_norm.reshape(n_seeds, 1, n_wires, dim)).sum(dim=-1)\n",
    "        assert z2.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        numer = (z2 + x2snk.norm(dim=-1))\n",
    "        assert numer.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        numer = torch.clip(numer, eps, None)\n",
    "        assert numer.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        assert (numer > 0).all(), numer[numer <= 0]\n",
    "        \n",
    "        log_numer = numer.log()\n",
    "        assert log_numer.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        denom = (z1 + x2src.norm(dim=-1))\n",
    "        assert denom.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        denom = torch.clip(denom, eps, None)\n",
    "        assert denom.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        assert (denom > 0).all(), denom[denom <= 0]\n",
    "        \n",
    "        log_denom = denom.log()\n",
    "        assert log_denom.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        cst = src2snk_dist.reshape(n_seeds, 1, n_wires).log() * 2.0\n",
    "        assert cst.shape == (n_seeds, 1, n_wires)\n",
    "        \n",
    "        A_z = (log_numer - log_denom - cst) * current.reshape(n_seeds, 1, n_wires) / (-4 * np.pi)\n",
    "        assert A_z.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        A_wires = A_z.reshape(n_seeds, n_x, n_wires, 1) * src2snk_norm.reshape(n_seeds, 1, n_wires, dim)\n",
    "        assert A_wires.shape == (n_seeds, n_x, n_wires, dim)\n",
    "        \n",
    "        A = A_wires.sum(dim=-2)\n",
    "        assert A.shape == (n_seeds, n_x, dim)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def field(self, x):\n",
    "        n_seeds, n_wires = self.n_seeds, self.n_wires\n",
    "        source, sink, current = self.source_tch, self.sink_tch, self.current_tch\n",
    "        \n",
    "        n_seeds, n_x, dim = x.shape \n",
    "        assert x.shape == (n_seeds, n_x, dim)\n",
    "        \n",
    "        src2snk = sink - source\n",
    "        assert src2snk.shape == (n_seeds, n_wires, dim)\n",
    "        \n",
    "        src2snk_dist = src2snk.norm(dim=-1, keepdim=True)\n",
    "        assert src2snk_dist.shape == (n_seeds, n_wires, 1)\n",
    "        \n",
    "        src2snk_norm = src2snk / src2snk_dist\n",
    "        assert src2snk_norm.shape == (n_seeds, n_wires, dim)\n",
    "        \n",
    "        x2src = source.reshape(n_seeds, 1, n_wires, dim) - x.reshape(n_seeds, n_x, 1, dim)\n",
    "        assert x2src.shape == (n_seeds, n_x, n_wires, dim)\n",
    "        \n",
    "        z1 = (x2src * src2snk_norm.reshape(n_seeds, 1, n_wires, dim)).sum(dim=-1)\n",
    "        assert z1.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        x2snk = sink.reshape(n_seeds, 1, n_wires, dim) - x.reshape(n_seeds, n_x, 1, dim)\n",
    "        assert x2snk.shape == (n_seeds, n_x, n_wires, dim)\n",
    "        \n",
    "        z2 = (x2snk * src2snk_norm.reshape(n_seeds, 1, n_wires, dim)).sum(dim=-1)\n",
    "        assert z2.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        x2snk_dist = x2snk.norm(dim=-1)\n",
    "        assert x2snk_dist.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        x2src_dist = x2src.norm(dim=-1)\n",
    "        assert x2src_dist.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        x12 = x2src - z1.reshape(n_seeds, n_x, n_wires, 1) * src2snk_norm.reshape(n_seeds, 1, n_wires, dim)\n",
    "        assert x12.shape == (n_seeds, n_x, n_wires, dim)\n",
    "        \n",
    "        c1 = (z2 / x2snk_dist - z1 / x2src_dist) / x12.norm(dim=-1)\n",
    "        assert c1.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        c2 = c1 * current.reshape(n_seeds, 1, n_wires) / (-4 * np.pi)\n",
    "        assert c2.shape == (n_seeds, n_x, n_wires)\n",
    "        \n",
    "        xss_cross = torch.cross(x2src, src2snk.reshape(n_seeds, 1, n_wires, dim), dim=-1)\n",
    "        assert xss_cross.shape == (n_seeds, n_x, n_wires, dim)\n",
    "        \n",
    "        xss_cross_nrom = xss_cross / xss_cross.norm(dim=-1, keepdim=True)\n",
    "        assert xss_cross_nrom.shape == (n_seeds, n_x, n_wires, dim)\n",
    "        \n",
    "        field_wires = xss_cross_nrom * c2.reshape(n_seeds, n_x, n_wires, 1)\n",
    "        assert field_wires.shape == (n_seeds, n_x, n_wires, dim)\n",
    "        \n",
    "        field = field_wires.sum(dim=-2)\n",
    "        assert field.shape == (n_seeds, n_x, dim)\n",
    "        \n",
    "        return field\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741652bb",
   "metadata": {},
   "source": [
    "### Defining the Volume Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe99ee",
   "metadata": {
    "code_folding": [
     1,
     150
    ]
   },
   "outputs": [],
   "source": [
    "class DiskSampler:\n",
    "    def __init__(self, c_dstr, c_params, r_dstr, r_params, n_dstr, n_params, batch_rng):\n",
    "        assert isinstance(c_params, dict)\n",
    "        for name, param in c_params.items():\n",
    "            msg_ = f'center param {name} is not np.array'\n",
    "            assert isinstance(param, np.ndarray), msg_\n",
    "        \n",
    "        assert isinstance(r_params, dict)\n",
    "        for name, param in r_params.items():\n",
    "            msg_ = f'radius param {name} is not np.array'\n",
    "            assert isinstance(param, np.ndarray), msg_\n",
    "            \n",
    "        assert isinstance(n_params, dict)\n",
    "        for name, param in n_params.items():\n",
    "            msg_ = f'radius param {name} is not np.array'\n",
    "            assert isinstance(param, np.ndarray), msg_\n",
    "\n",
    "        self.batch_rng = batch_rng\n",
    "        self.lib = batch_rng.lib\n",
    "        \n",
    "        ##############################################################\n",
    "        ################# Center Sampling Parameters #################\n",
    "        ##############################################################\n",
    "        c_params_ = c_params.copy()\n",
    "        self.c_dstr = c_dstr\n",
    "        if c_dstr == 'uniform':\n",
    "            c_low = c_params_.pop('low')\n",
    "            c_high = c_params_.pop('high')\n",
    "            \n",
    "            n_bch, dim = c_low.shape\n",
    "            \n",
    "            self.c_low_np = c_low.reshape(n_bch, 1, dim)\n",
    "            self.c_high_np = c_high.reshape(n_bch, 1, dim)\n",
    "            self.c_size_np = (self.c_high_np - self.c_low_np)\n",
    "\n",
    "            if self.lib == 'torch':\n",
    "                self.c_low_tch = torch.from_numpy(self.c_low_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_high_tch = torch.from_numpy(self.c_high_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_size_tch = torch.from_numpy(self.c_size_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            \n",
    "            self.c_low = self.c_low_np if self.lib == 'numpy' else self.c_low_tch\n",
    "            self.c_size = self.c_size_np if self.lib == 'numpy' else self.c_size_tch\n",
    "        elif c_dstr == 'normal':\n",
    "            c_loc = c_params_.pop('loc')\n",
    "            c_scale = c_params_.pop('scale')\n",
    "            \n",
    "            n_bch, dim = c_loc.shape\n",
    "            self.c_loc_np = c_loc.reshape(n_bch, 1, dim)\n",
    "            self.c_scale_np = c_scale.reshape(n_bch, 1, 1)\n",
    "            \n",
    "            if self.lib == 'torch':\n",
    "                self.c_loc_tch = torch.from_numpy(self.c_loc_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_scale_tch = torch.from_numpy(self.c_scale_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                \n",
    "            self.c_loc = self.c_loc_np if self.lib == 'numpy' else self.c_loc_tch\n",
    "            self.c_scale = self.c_scale_np if self.lib == 'numpy' else self.c_scale_tch\n",
    "        elif c_dstr == 'ball':\n",
    "            c_cntr = c_params_.pop('c')\n",
    "            c_radi = c_params_.pop('r')\n",
    "            \n",
    "            n_bch, dim = c_cntr.shape\n",
    "            self.c_cntr_np = c_cntr.reshape(n_bch, 1, dim)\n",
    "            self.c_radi_np = c_radi.reshape(n_bch, 1, 1)\n",
    "            \n",
    "            if self.lib == 'torch':\n",
    "                self.c_cntr_tch = torch.from_numpy(self.c_cntr_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                self.c_radi_tch = torch.from_numpy(self.c_radi_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                \n",
    "            self.c_cntr = self.c_cntr_np if self.lib == 'numpy' else self.c_cntr_tch\n",
    "            self.c_radi = self.c_radi_np if self.lib == 'numpy' else self.c_radi_tch\n",
    "        else:\n",
    "            raise ValueError(f'c_dstr=\"{c_dstr}\" not implemented')\n",
    "        \n",
    "        msg_ = f'Some center parameters were left unused: {list(c_params_.keys())}'\n",
    "        assert len(c_params_) == 0, msg_\n",
    "            \n",
    "        self.n_bch, self.d = n_bch, dim\n",
    "        \n",
    "        ##############################################################\n",
    "        ################# Radius Sampling Parameters #################\n",
    "        ##############################################################\n",
    "        r_params_ = r_params.copy()\n",
    "        r_low = r_params_.pop('low')\n",
    "        r_high = r_params_.pop('high')\n",
    "        \n",
    "        if r_dstr == 'uniform':\n",
    "            self.r_upow = 1.0\n",
    "        elif r_dstr == 'unifdpow':\n",
    "            self.r_upow = 1.0 / self.d\n",
    "        else:\n",
    "            raise ValueError(f'r_dstr={r_dstr} not implemented')\n",
    "\n",
    "        r_low_rshp = r_low.reshape(self.n_bch, 1)\n",
    "        r_high_rshp = r_high.reshape(self.n_bch, 1)\n",
    "        assert (r_low >= 0.0).all()\n",
    "        assert (r_high >= r_low).all()\n",
    "        \n",
    "        self.r_dstr = r_dstr\n",
    "        self.r_low_np = np.power(r_low_rshp, 1.0/self.r_upow)\n",
    "        self.r_high_np = np.power(r_high_rshp, 1.0/self.r_upow)\n",
    "        self.r_size_np = (self.r_high_np - self.r_low_np)\n",
    "        \n",
    "        if self.lib == 'torch':\n",
    "            self.r_low_tch = torch.from_numpy(self.r_low_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            self.r_high_tch = torch.from_numpy(self.r_high_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            self.r_size_tch = torch.from_numpy(self.r_size_np).to(\n",
    "                device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "            \n",
    "        self.r_low = self.r_low_np if self.lib == 'numpy' else self.r_low_tch\n",
    "        self.r_size = self.r_size_np if self.lib == 'numpy' else self.r_size_tch\n",
    "        \n",
    "        msg_ = f'Some center parameters were left unused: {list(r_params_.keys())}'\n",
    "        assert len(r_params_) == 0, msg_\n",
    "        \n",
    "        ##############################################################\n",
    "        ############## Plane Normal Sampling Parameters ##############\n",
    "        ##############################################################\n",
    "        n_params_ = n_params.copy()\n",
    "\n",
    "        self.n_dstr = n_dstr\n",
    "        assert n_dstr in ('uball', 'fixed')\n",
    "        if n_dstr == 'fixed':\n",
    "            n_fixed_np = n_params_.pop('value')\n",
    "            assert n_fixed_np.shape == (n_bch, dim)\n",
    "            \n",
    "            n_fixed_size = np.sqrt(np.square(n_fixed_np).sum(axis=-1, keepdims=True))\n",
    "            self.n_fixed_np = n_fixed_np / n_fixed_size\n",
    "            \n",
    "            if self.lib == 'torch':\n",
    "                self.n_fixed_tch = torch.from_numpy(self.n_fixed_np).to(\n",
    "                    device=self.batch_rng.device, dtype=self.batch_rng.dtype)\n",
    "                \n",
    "            self.n_fixed = self.n_fixed_np if self.lib == 'numpy' else self.n_fixed_tch\n",
    "        elif n_dstr == 'uball':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f'n_dstr={n_dstr} undefined')\n",
    "            \n",
    "        msg_ = f'Some normal parameters were left unused: {list(n_params_.keys())}'\n",
    "        assert len(n_params_) == 0, msg_\n",
    "\n",
    "    def __call__(self, n=1):\n",
    "        radii = self.r_low + self.r_size * \\\n",
    "            self.batch_rng.uniform((self.n_bch, n))\n",
    "        radii = radii ** self.r_upow\n",
    "        \n",
    "        if self.c_dstr == 'uniform':\n",
    "            centers = self.batch_rng.uniform((self.n_bch, n, self.d))\n",
    "            centers = centers * self.c_size + self.c_low\n",
    "        elif self.c_dstr == 'normal':\n",
    "            centers = self.batch_rng.normal((self.n_bch, n, self.d))\n",
    "            centers = centers * self.c_scale + self.c_loc\n",
    "        elif self.c_dstr == 'ball':\n",
    "            rnd1 = self.batch_rng.normal((self.n_bch, n, self.d))\n",
    "            rnd1 = rnd1 / ((rnd1**2).sum(-1, keepdims=True)**0.5)\n",
    "            \n",
    "            rnd2 = self.batch_rng.uniform((self.n_bch, n, 1))\n",
    "            rnd2 = rnd2 ** (1./self.d)\n",
    "            \n",
    "            centers = self.c_radi * rnd2 * rnd1 + self.c_cntr\n",
    "        else:\n",
    "            raise ValueError(f'c_dstr=\"{self.c_dstr}\" not implemented')\n",
    "        \n",
    "        if self.n_dstr == 'uball':   \n",
    "            rnd3 = self.batch_rng.normal((self.n_bch, n, self.d, 1))\n",
    "            normals = rnd3 / ((rnd3**2).sum(-2, keepdims=True)**0.5)\n",
    "        elif self.n_dstr == 'fixed':\n",
    "            normals = self.n_fixed.reshape(self.n_bch, 1, self.d, 1)\n",
    "            if self.lib == 'torch':\n",
    "                normals = normals.expand(self.n_bch, n, self.d, 1)\n",
    "            elif self.lib == 'numpy':\n",
    "                normals = np.broadcast_to(normals, \n",
    "                    (self.n_bch, n, self.d, 1)).copy()\n",
    "            else:\n",
    "                raise ValueError(f'lib={self.lib} not implemented')\n",
    "        else:\n",
    "            raise ValueError(f'n_dstr={self.n_dstr} not implemented')\n",
    "        \n",
    "        d = dict()\n",
    "        d['type'] = 'disk'\n",
    "        d['centers'] = centers\n",
    "        d['radii'] = radii\n",
    "        d['normals'] = normals\n",
    "        return d\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4997d8",
   "metadata": {},
   "source": [
    "### Sruface Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f6df2",
   "metadata": {
    "code_folding": [
     1,
     6,
     12,
     20
    ]
   },
   "outputs": [],
   "source": [
    "class SphereSampler:\n",
    "    def __init__(self, batch_rng):\n",
    "        self.tch_dtype = batch_rng.dtype\n",
    "        self.tch_device = batch_rng.device\n",
    "        self.batch_rng = batch_rng\n",
    "\n",
    "    def np_exlinspace(self, start, end, n):\n",
    "        assert n >= 1\n",
    "        a = np.linspace(start, end, n, endpoint=False)\n",
    "        b = a + 0.5 * (end - a[-1])\n",
    "        return b\n",
    "\n",
    "    def tch_exlinspace(self, start, end, n):\n",
    "        assert n >= 1\n",
    "        a = torch.linspace(start, end, n+1,\n",
    "                           device=self.tch_device,\n",
    "                           dtype=self.tch_dtype)[:-1]\n",
    "        b = a + 0.5 * (end - a[-1])\n",
    "        return b\n",
    "\n",
    "    def __call__(self, volumes, n, do_detspacing=True):\n",
    "        # volumes -> dictionary\n",
    "        assert volumes['type'] == 'disk'\n",
    "        centers = volumes['centers']\n",
    "        radii = volumes['radii']\n",
    "        normals = volumes['normals']\n",
    "        \n",
    "        n_bch, n_v, d = centers.shape # d is the dimensionality of the original space (usually 3)\n",
    "        d_n = normals.shape[-1] # The dimensionality of the normal space (usually 1)\n",
    "        d_k = d - d_n # d_k is the dimensionality of the disk itself (usually 2).\n",
    "        \n",
    "        use_np = not torch.is_tensor(centers)\n",
    "        assert centers.shape == (n_bch, n_v, d)\n",
    "        assert radii.shape == (n_bch, n_v)\n",
    "        assert normals.shape == (n_bch, n_v, d, d_n)\n",
    "        assert not (use_np) or (self.batch_rng.lib == 'numpy')\n",
    "        assert use_np or (self.batch_rng.device == centers.device)\n",
    "        assert use_np or (self.batch_rng.dtype == centers.dtype)\n",
    "        assert self.batch_rng.shape == (n_bch,)\n",
    "        exlinspace = self.np_exlinspace if use_np else self.tch_exlinspace\n",
    "        meshgrid = np.meshgrid if use_np else torch.meshgrid\n",
    "        sin = np.sin if use_np else torch.sin\n",
    "        cos = np.cos if use_np else torch.cos\n",
    "        matmul = np.matmul if use_np else torch.matmul\n",
    "\n",
    "        if do_detspacing and (d_k == 2):\n",
    "            theta = exlinspace(0.0, 2*np.pi, n)\n",
    "            assert theta.shape == (n,)\n",
    "            theta_2d = theta.reshape(n, 1)\n",
    "            x_tilde_2d_list = [cos(theta_2d), sin(theta_2d)]\n",
    "            if use_np:\n",
    "                x_tilde_2d = np.concatenate(x_tilde_2d_list, axis=1)\n",
    "            else:\n",
    "                x_tilde_2d = torch.cat(x_tilde_2d_list, dim=1)\n",
    "            assert x_tilde_2d.shape == (n, d_k)\n",
    "            x_tilde_4d = x_tilde_2d.reshape(1, 1, n, d_k)\n",
    "            assert x_tilde_4d.shape == (1, 1, n, d_k)\n",
    "            x_tilde = x_tilde_4d.expand(n_bch, 1, n, d_k)\n",
    "            assert x_tilde.shape == (n_bch, 1, n, d_k)\n",
    "        elif do_detspacing and (d_k == 3):\n",
    "            n_sqrt = int(np.sqrt(n))\n",
    "            assert n == n_sqrt * n_sqrt, 'Need n to be int-square for now!'\n",
    "            theta_1d = exlinspace(0.0, 2*np.pi, n_sqrt)\n",
    "            unit_unif = exlinspace(0.0, 1.0, n_sqrt)\n",
    "            if use_np:\n",
    "                phi_1d = np.arccos(1-2*unit_unif)\n",
    "            else:\n",
    "                phi_1d = torch.arccos(1-2*unit_unif)\n",
    "            theta_msh, phi_msh = meshgrid(theta_1d, phi_1d)\n",
    "            assert theta_msh.shape == (n_sqrt, n_sqrt)\n",
    "            assert phi_msh.shape == (n_sqrt, n_sqrt)\n",
    "            theta_2d, phi_2d = theta_msh.reshape(n, 1), phi_msh.reshape(n, 1)\n",
    "            assert theta_2d.shape == (n, 1)\n",
    "            assert phi_2d.shape == (n, 1)\n",
    "            x_tilde_lst = [sin(phi_2d) * cos(theta),\n",
    "                           sin(phi_2d) * sin(theta), cos(phi_2d)]\n",
    "            if use_np:\n",
    "                x_tilde_2d = np.concatenate(x_tilde_lst, axis=1)\n",
    "            else:\n",
    "                x_tilde_2d = torch.cat(x_tilde_lst, dim=1)\n",
    "            assert x_tilde_2d.shape == (n, d_k)\n",
    "            x_tilde_4d = x_tilde_2d.reshape(1, 1, n, d_k)\n",
    "            assert x_tilde_4d.shape == (1, 1, n, d_k)\n",
    "            x_tilde = x_tilde_4d.expand(n_bch, 1, n, d_k)\n",
    "            assert x_tilde.shape == (n_bch, 1, n, d_k)\n",
    "        elif (not do_detspacing) and (not use_np):\n",
    "            x_tilde_unnorm = self.batch_rng.normal((n_bch, n_v, n, d_k))\n",
    "            x_tilde_l2 = torch.sqrt(torch.square(x_tilde_unnorm).sum(dim=-1))\n",
    "            x_tilde = x_tilde_unnorm / x_tilde_l2.reshape(n_bch, n_v, n, 1)\n",
    "            assert x_tilde.shape == (n_bch, n_v, n, d_k)\n",
    "        else:\n",
    "            raise RuntimeError('Not implemented yet!')\n",
    "\n",
    "        if do_detspacing:\n",
    "            rot_mats = self.batch_rng.so_n((n_bch, n_v, d_k, d_k))\n",
    "            assert rot_mats.shape == (n_bch, n_v, d_k, d_k)\n",
    "\n",
    "        if do_detspacing:\n",
    "            x_tilde_rot = matmul(x_tilde, rot_mats)\n",
    "        else:\n",
    "            x_tilde_rot = x_tilde\n",
    "        assert x_tilde_rot.shape == (n_bch, n_v, n, d_k)\n",
    "        \n",
    "        # Computing a null basis for the plane normals and projecting the \n",
    "        # `d_k`-dimensional samples into the original space using the null basis.\n",
    "        U_n, _, _ = normals.reshape(n_bch, n_v, d, d_n).svd(some=False, compute_uv=True)\n",
    "        assert U_n.shape == (n_bch, n_v, d, d)\n",
    "        \n",
    "        U_tilde_n = U_n[..., d_n:].reshape(n_bch, n_v, 1, d, d_k)\n",
    "        assert U_tilde_n.shape == (n_bch, n_v, 1, d, d_k)\n",
    "        \n",
    "        xtr_ = x_tilde_rot.reshape(n_bch, n_v, n, d_k, 1)\n",
    "        x_tilde_rp = matmul(U_tilde_n, xtr_).reshape(n_bch, n_v, n, d)\n",
    "        assert x_tilde_rp.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        # Scaling the points to the desired radii and shifting them\n",
    "        points = x_tilde_rp * \\\n",
    "            radii.reshape(n_bch, n_v, 1, 1) + centers.reshape(n_bch, n_v, 1, d)\n",
    "        assert points.shape == (n_bch, n_v, n, d)\n",
    "\n",
    "        if use_np:\n",
    "            x_tilde_bc = np.broadcast_to(x_tilde, (n_bch, n_v, n, d_k))\n",
    "        else:\n",
    "            x_tilde_bc = x_tilde.expand(n_bch, n_v, n, d_k)\n",
    "\n",
    "        if do_detspacing:\n",
    "            rot_x_tilde = matmul(x_tilde_bc, rot_mats)\n",
    "        else:\n",
    "            rot_x_tilde = x_tilde_bc\n",
    "        assert rot_x_tilde.shape == (n_bch, n_v, n, d_k)\n",
    "        \n",
    "        rxt_ = rot_x_tilde.reshape(n_bch, n_v, n, d_k, 1)\n",
    "        rp_x_tilde = matmul(U_tilde_n, rxt_).reshape(n_bch, n_v, n, d)\n",
    "        assert rp_x_tilde.shape == (n_bch, n_v, n, d)\n",
    "        \n",
    "        if (d == 3) and (d_n == 1):\n",
    "            dsknrmls = normals.reshape(n_bch, n_v, 1, d)\n",
    "            tangents = torch.cross(dsknrmls, rp_x_tilde, dim=-1)\n",
    "            assert tangents.shape == (n_bch, n_v, n, d)\n",
    "        else:\n",
    "            tangents = None\n",
    "\n",
    "        cst = (2*(np.pi**(d_k/2))) / gamma(d_k/2)\n",
    "        csts = cst * (radii**(d_k-1))\n",
    "        assert csts.shape == (n_bch, n_v)\n",
    "\n",
    "        ret_dict = dict(points=points, normals=rp_x_tilde, tangents=tangents, areas=csts)\n",
    "        return ret_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f368db",
   "metadata": {},
   "source": [
    "### Visualization Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xg(xi_low, xi_high, n_gi, tch_device, tch_dtype):\n",
    "    dim = len(xi_low)\n",
    "    xi_1d_list = []\n",
    "    for i_dim, ng_dim in enumerate(n_gi):\n",
    "        xi_1d = torch.linspace(xi_low[i_dim], xi_high[i_dim], \n",
    "            ng_dim, dtype=tch_dtype, device=tch_device)\n",
    "        xi_1d_list.append(xi_1d)\n",
    "        \n",
    "    n_g = np.prod(n_gi)\n",
    "    x_g = torch.cartesian_prod(*xi_1d_list)\n",
    "    assert x_g.shape == (n_g, dim)\n",
    "    \n",
    "    return x_g.reshape(*n_gi, dim)\n",
    "\n",
    "def draw_heatmap(x_g, y_g, fig, axes, share_cnorm=True, \n",
    "    cmap='RdBu', field_abrv='A', components='xyz', \n",
    "    show_title='all', zstr_loc='top'):\n",
    "    \n",
    "    n_g1, n_g2, n_g3, dim = x_g.shape\n",
    "    x3_1d = x_g[0, 0, :, 2]\n",
    "    n_g = n_g1 * n_g2 * n_g3\n",
    "    \n",
    "    assert show_title in ('all', 'top', 'none')\n",
    "    assert zstr_loc in ('top', 'left')\n",
    "    \n",
    "    i_comps = ['xyz'.index(ss) for ss in components]\n",
    "    n_comps = len(components)\n",
    "    \n",
    "    assert dim == 3\n",
    "    assert y_g.shape == (n_g, dim)\n",
    "\n",
    "    y_g2 = y_g.reshape(n_g1, n_g2, n_g3, dim).detach().cpu().numpy()\n",
    "    assert y_g2.shape == (n_g1, n_g2, n_g3, dim)\n",
    "\n",
    "    x_g2 = x_g.reshape(n_g1, n_g2, n_g3, dim).detach().cpu().numpy()\n",
    "    assert x_g2.shape == (n_g1, n_g2, n_g3, dim)\n",
    "\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "    cnorm = mpl.colors.Normalize(vmin=y_g.min().item(), vmax=y_g.max().item())\n",
    "    cnorm = cnorm if share_cnorm else None\n",
    "    \n",
    "    axes = np.array(axes)\n",
    "    \n",
    "    n_q = int(axes.size // n_comps)\n",
    "    q = np.linspace(0, 1, n_q)\n",
    "    zi_list = np.quantile(np.arange(n_g3), q).round().astype(int).tolist()\n",
    "    \n",
    "    for ax_idx, ax in enumerate(axes.reshape(-1)):\n",
    "        di = i_comps[ax_idx %  n_comps]\n",
    "        \n",
    "        # zi = ax_idx // n_comps\n",
    "        zi = zi_list[ax_idx // n_comps]\n",
    "        \n",
    "        if ax_idx >= (n_g3 * n_comps):\n",
    "            break\n",
    "        \n",
    "        x_hmap = x_g2[:, :, zi, :]\n",
    "        assert x_hmap.shape == (n_g1, n_g2, 3)\n",
    "        assert (x_hmap[:, :, 2] == x_hmap[0, 0, 2]).all()\n",
    "        \n",
    "        y_hmap = y_g2[:, :, zi, di]\n",
    "        assert y_hmap.shape == (n_g1, n_g2)\n",
    "        \n",
    "        im = ax.pcolormesh(x_hmap[:, :, 0], x_hmap[:, :, 1], y_hmap, \n",
    "            shading='auto', norm=cnorm, cmap=cmap, linewidth=0, \n",
    "            rasterized=True)\n",
    "        \n",
    "        n_rows, n_cols = axes.shape\n",
    "        row, col = ax_idx // n_cols, ax_idx % n_cols\n",
    "        z_str = f' ($z$={x3_1d[zi].item():.2f})'\n",
    "        \n",
    "        x_label, y_label = '$x$', '$y$'\n",
    "        ax_title = ''\n",
    "        if field_abrv is not None:\n",
    "            ax_title += f'${field_abrv}_{\"xyz\"[di]}$'\n",
    "        if zstr_loc == 'top':\n",
    "            ax_title += z_str\n",
    "        elif zstr_loc == 'left':\n",
    "            y_label += z_str\n",
    "        else:\n",
    "            raise ValueError(f'zstr_loc={zstr_loc} undefined')\n",
    "        \n",
    "        if row == (n_rows - 1):\n",
    "            ax.set_xlabel(x_label)\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "        \n",
    "        if col == 0:\n",
    "            ax.set_ylabel(y_label)\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        print_title  = (show_title == 'all')\n",
    "        print_title += (show_title == 'top') and (row == 0)\n",
    "        print_title += (show_title == 'none') and False\n",
    "        if print_title:\n",
    "            ax.set_title(ax_title)\n",
    "    \n",
    "    if show_title in ('top', 'none'): \n",
    "        fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "def draw_quiver(x_g, y_g, fig, axes, share_cnorm=True, \n",
    "    cmap='RdBu', field_abrv='A', show_title='all', \n",
    "    zstr_loc='top', subsamp=1, **kwargs):\n",
    "    \n",
    "    n_g1, n_g2, n_g3, dim = x_g.shape\n",
    "    x3_1d = x_g[0, 0, :, 2]\n",
    "    n_g = n_g1 * n_g2 * n_g3\n",
    "    \n",
    "    assert dim == 3\n",
    "    assert y_g.shape == (n_g, dim)\n",
    "\n",
    "    y_g2 = y_g.reshape(n_g1, n_g2, n_g3, dim).detach().cpu().numpy()\n",
    "    assert y_g2.shape == (n_g1, n_g2, n_g3, dim)\n",
    "\n",
    "    x_g2 = x_g.reshape(n_g1, n_g2, n_g3, dim).detach().cpu().numpy()\n",
    "    assert x_g2.shape == (n_g1, n_g2, n_g3, dim)\n",
    "\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "    cnorm = mpl.colors.Normalize(vmin=y_g[:, 2].min().item(), vmax=y_g[:, 2].max().item())\n",
    "    cnorm = cnorm if share_cnorm else None\n",
    "    \n",
    "    axes = np.array(axes)\n",
    "    n_q = int(axes.size)\n",
    "    q = np.linspace(0, 1, n_q)\n",
    "    zi_list = np.quantile(np.arange(n_g3), q).round().astype(int).tolist()\n",
    "    \n",
    "    for ax_idx, ax in enumerate(axes.reshape(-1)):\n",
    "        # zi = ax_idx\n",
    "        zi = zi_list[ax_idx]\n",
    "        \n",
    "        x_hmap = x_g2[:, :, zi, :]\n",
    "        assert x_hmap.shape == (n_g1, n_g2, dim)\n",
    "        assert (x_hmap[:, :, 2] == x_hmap[0, 0, 2]).all()\n",
    "        \n",
    "        y_hmap = y_g2[:, :, zi, :]\n",
    "        assert y_hmap.shape == (n_g1, n_g2, dim)\n",
    "        \n",
    "        ss = subsamp\n",
    "        ax.quiver(x_hmap[::ss, ::ss, 0], x_hmap[::ss, ::ss, 1], \n",
    "            y_hmap[::ss, ::ss, 0], y_hmap[::ss, ::ss, 1], y_hmap[::ss, ::ss, 2], \n",
    "            norm=cnorm, cmap=cmap, **kwargs)\n",
    "        \n",
    "        n_rows, n_cols = axes.shape\n",
    "        row, col = ax_idx // n_cols, ax_idx % n_cols\n",
    "        z_str = f' ($z$={x3_1d[zi].item():.2f})'\n",
    "        \n",
    "        x_label, y_label = '$x$', '$y$'\n",
    "        ax_title = ''\n",
    "        if field_abrv is not None:\n",
    "            ax_title += f'${field_abrv}$'\n",
    "        if zstr_loc == 'top':\n",
    "            ax_title += z_str\n",
    "        elif zstr_loc == 'left':\n",
    "            y_label += z_str\n",
    "        else:\n",
    "            raise ValueError(f'zstr_loc={zstr_loc} undefined')\n",
    "        \n",
    "        if row == (n_rows - 1):\n",
    "            ax.set_xlabel(x_label)\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "        \n",
    "        if col == 0:\n",
    "            ax.set_ylabel(y_label)\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        print_title  = (show_title == 'all')\n",
    "        print_title += (show_title == 'top') and (row == 0)\n",
    "        print_title += (show_title == 'none') and False\n",
    "        if print_title:\n",
    "            ax.set_title(ax_title)\n",
    "    \n",
    "    if show_title in ('top', 'none'): \n",
    "        fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9c50f",
   "metadata": {},
   "source": [
    "### Mathematical Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curl(A, x, create_graph=True):\n",
    "    assert A.shape[-1] == 3\n",
    "    assert x.shape[-1] == 3\n",
    "    assert A.shape == x.shape\n",
    "    \n",
    "    Ax, Ay, Az = A[..., 0], A[..., 1], A[..., 2]\n",
    "    Ax_, Ay_, Az_ = Ax.sum(), Ay.sum(), Az.sum()\n",
    "\n",
    "    Az_xyz, = torch.autograd.grad(Az_, [x],\n",
    "        grad_outputs=None, retain_graph=True, create_graph=create_graph,\n",
    "        only_inputs=True, allow_unused=False)\n",
    "    Ay_xyz, = torch.autograd.grad(Ay_, [x],\n",
    "        grad_outputs=None, retain_graph=True, create_graph=create_graph,\n",
    "        only_inputs=True, allow_unused=False)\n",
    "    Ax_xyz, = torch.autograd.grad(Ax_, [x],\n",
    "        grad_outputs=None, retain_graph=True, create_graph=create_graph,\n",
    "        only_inputs=True, allow_unused=False)\n",
    "\n",
    "    Az_x, Az_y = Az_xyz[..., 0], Az_xyz[..., 1]\n",
    "    Ay_x, Ay_z = Ay_xyz[..., 0], Ay_xyz[..., 2]\n",
    "    Ax_y, Ax_z = Ax_xyz[..., 1], Ax_xyz[..., 2]\n",
    "\n",
    "    A_curl = torch.stack([Az_y - Ay_z, Ax_z - Az_x, Ay_x - Ax_y], dim=-1)\n",
    "    return A_curl\n",
    "\n",
    "\n",
    "# MCMC Utility Functions\n",
    "class MCMCWireEncoder:\n",
    "    def __init__(self, kind, source, sink, current):\n",
    "        assert isinstance(source, np.ndarray)\n",
    "        assert isinstance(sink, np.ndarray)\n",
    "        assert isinstance(current, np.ndarray)\n",
    "        source = source.copy()\n",
    "        sink = sink.copy()\n",
    "        current = current.copy()\n",
    "        n_seeds, wire_n, dim = source.shape\n",
    "        \n",
    "        if kind == 'srcsnkxy':\n",
    "            assert dim == 3\n",
    "            sdim = wire_n * 2\n",
    "            source = source.copy()\n",
    "            source[:, :, :2] = np.nan\n",
    "            sink[:, :, :2] = np.nan\n",
    "        else:\n",
    "            raise ValueError(f'kind={kind} not implmntd')\n",
    "        \n",
    "        self.kind = kind\n",
    "        self.n_seeds = n_seeds\n",
    "        self.wire_n = wire_n\n",
    "        self.dim = dim\n",
    "        self.sdim = sdim\n",
    "        self.source = source\n",
    "        self.sink = sink\n",
    "        self.current = current\n",
    "        \n",
    "    def __call__(self, mu):\n",
    "        kind, n_seeds = self.kind, self.n_seeds\n",
    "        wire_n, dim, sdim = self.wire_n, self.dim, self.sdim\n",
    "        source, sink, current = self.source, self.sink, self.current\n",
    "        \n",
    "        assert mu.shape == (n_seeds, sdim)\n",
    "        \n",
    "        if kind == 'srcsnkxy':\n",
    "            src_enc = source.copy()\n",
    "            src_enc[:, :, :2] = mu.reshape(n_seeds, wire_n, 2)\n",
    "            assert src_enc.shape == (n_seeds, wire_n, dim)\n",
    "            \n",
    "            snk_enc = sink.copy()\n",
    "            snk_enc[:, :, :2] = mu.reshape(n_seeds, wire_n, 2)\n",
    "            assert snk_enc.shape == (n_seeds, wire_n, dim)\n",
    "            \n",
    "            cur_enc = current.copy()\n",
    "            assert cur_enc.shape == (n_seeds, wire_n)\n",
    "        else:\n",
    "            raise ValueError(f'kind={kind} not defined')\n",
    "        \n",
    "        outdict = dict(source=src_enc, sink=snk_enc, current=cur_enc)\n",
    "        return outdict            \n",
    "\n",
    "\n",
    "def normal_logprob(x, loc, scale):\n",
    "    n_seeds, n_pnts, dim = loc.shape\n",
    "    assert scale.shape == (n_seeds, n_pnts)\n",
    "    \n",
    "    scale2 = scale.reshape(n_seeds, n_pnts, 1)\n",
    "    assert scale2.shape == (n_seeds, n_pnts, 1)\n",
    "    \n",
    "    e = (x - loc) / scale2\n",
    "    assert e.shape == (n_seeds, n_pnts, dim)\n",
    "    \n",
    "    hltp = np.log(2*np.pi).item() / 2\n",
    "    ll = -0.5 * e.square() - hltp - scale2.log()\n",
    "    assert ll.shape == (n_seeds, n_pnts, dim)\n",
    "    \n",
    "    llsum = ll.sum(dim=-1)\n",
    "    assert llsum.shape == (n_seeds, n_pnts)\n",
    "    \n",
    "    return llsum\n",
    "\n",
    "\n",
    "def replicate_top(param, top_idx):\n",
    "    n_seeds = param.shape[0]\n",
    "    n_grps, srch_reset_k = top_idx.shape\n",
    "    \n",
    "    n_cpg = n_seeds // n_grps\n",
    "    assert n_seeds == (n_grps * n_cpg)\n",
    "    \n",
    "    srch_reset_reps = n_cpg // srch_reset_k\n",
    "    assert n_cpg == (srch_reset_reps * srch_reset_k)\n",
    "    \n",
    "    pshape = param.shape[1:]\n",
    "    opshape = tuple([1 for _ in pshape])\n",
    "    \n",
    "    param2 = param.reshape(n_grps, n_cpg, *pshape)\n",
    "    assert param2.shape == (n_grps, n_cpg, *pshape)\n",
    "    \n",
    "    top_idx2 = top_idx.reshape(n_grps, srch_reset_k, *opshape)\n",
    "    assert top_idx2.shape == (n_grps, srch_reset_k, *opshape)\n",
    "    \n",
    "    param3 = torch.take_along_dim(param2, top_idx2, dim=1)\n",
    "    assert param3.shape == (n_grps, srch_reset_k, *pshape)\n",
    "    \n",
    "    param4 = param3.unsqueeze(1)\n",
    "    assert param4.shape == (n_grps, 1, srch_reset_k, *pshape)\n",
    "    \n",
    "    param5 = param4.expand(n_grps, srch_reset_reps, srch_reset_k, *pshape).clone()\n",
    "    assert param5.shape == (n_grps, srch_reset_reps, srch_reset_k, *pshape)\n",
    "    \n",
    "    param6 = param5.reshape(n_seeds, *pshape)\n",
    "    assert param6.shape == (n_seeds, *pshape)\n",
    "    \n",
    "    return param6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173df0d",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0843b",
   "metadata": {
    "code_folding": [
     0,
     57,
     66,
     142,
     225
    ]
   },
   "outputs": [],
   "source": [
    "def get_nn_sol(model, x, n_eval=None, get_field=True, \n",
    "    out_lib='numpy'):\n",
    "    \"\"\"\n",
    "    Gets a model and evaluates it minibatch-wise on the tensor x. \n",
    "    The minibatch size is capped at n_eval. The output will have the \n",
    "    predicted potentials and the vector fields at them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: (nn.module) the batched neural network.\n",
    "\n",
    "    x: (torch.tensor) the evaluation points. This array should be \n",
    "        >2-dimensional and have a shape of `(..., x_rows, x_cols)`.\n",
    "\n",
    "    n_eval: (int or None) the maximum mini-batch size. If None is \n",
    "        given, `x_rows` will be used as `n_eval`.\n",
    "        \n",
    "    out_lib: (str) determines the output tensor type. Should be either \n",
    "        'numpy' or 'torch'.\n",
    "    \n",
    "    Output Dictionary\n",
    "    ----------\n",
    "    v: (np.array or torch.tensor) the evaluated potentials \n",
    "        with a shape of `(*model.shape, x_rows)` where\n",
    "        model.shape is the batch dimensions of the model. \n",
    "\n",
    "    e: (np.array or torch.tensor) the evaluated vector fields \n",
    "        with a shape of `(*model.shape, x_rows, x_cols)` where\n",
    "        model.shape is the batch dimensions of the model.\n",
    "    \"\"\"\n",
    "    x_rows, x_cols = tuple(x.shape)[-2:]\n",
    "    x_bd_ = tuple(x.shape)[:-2]\n",
    "    x_bd = (1,) if len(x_bd_) == 0 else x_bd_\n",
    "    msg_ = f'Cannot have {x.shape} fed to {model.shape}'\n",
    "    assert len(x_bd) <= model.ndim, msg_\n",
    "    if len(x_bd) < model.ndim:\n",
    "        x_bd = tuple([1] * (model.ndim-len(x_b)) + list(x_bd))\n",
    "    assert all((a == b) or (a == 1) or (b == 1) \n",
    "               for a, b in zip(x_bd, model.shape)), msg_\n",
    "    n_eval = x_rows if n_eval is None else n_eval\n",
    "    if out_lib == 'numpy':\n",
    "        to_lib = lambda a: a.detach().cpu().numpy()\n",
    "        lib_cat = lambda al: np.concatenate(al, axis=1)\n",
    "        lpf = '_np'\n",
    "    elif out_lib == 'torch':\n",
    "        to_lib = lambda a: a\n",
    "        lib_cat = lambda al: torch.cat(al, dim=1)\n",
    "        lpf = ''\n",
    "    else:\n",
    "        raise ValueError(f'outlib={outlib} not defined.')\n",
    "\n",
    "    n_batches = int(np.ceil(x_rows / n_eval))\n",
    "    v_pred_list = []\n",
    "    e_pred_list = []\n",
    "    for i in range(n_batches):\n",
    "        x_i = x[..., (i*n_eval):((i+1)*n_eval), :]\n",
    "        xi_rows = x_i.shape[-2]\n",
    "        x_ii = x_i.reshape(*x_bd, xi_rows, x_cols)\n",
    "        x_iii = x_ii.expand(*model.shape, xi_rows, x_cols)\n",
    "        x_iiii = nn.Parameter(x_iii)\n",
    "        v_pred_i = model(x_iiii)\n",
    "        v_pred_ii = to_lib(v_pred_i.detach())\n",
    "        v_pred_list.append(v_pred_ii)\n",
    "        if get_field:\n",
    "            e_pred_i = curl(v_pred_i, x_iiii, create_graph=False)\n",
    "            e_pred_ii = to_lib(e_pred_i.squeeze(-1).detach())\n",
    "            e_pred_list.append(e_pred_ii)\n",
    "\n",
    "    v_pred = lib_cat(v_pred_list)\n",
    "    if get_field:\n",
    "        e_pred = lib_cat(e_pred_list)\n",
    "    else:\n",
    "        e_pred = None\n",
    "\n",
    "    outdict = {f'v{lpf}': v_pred, f'e{lpf}': e_pred}\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def get_prob_sol(problem, x, n_eval=None, get_field=True, \n",
    "    out_lib='numpy'):\n",
    "    \"\"\"\n",
    "    Gets a problem and evaluates the analytical solution to its \n",
    "    potentials and vector fields minibatch-wise on the tensor x. \n",
    "    The minibatch size is capped at n_eval. The output will have the \n",
    "    predicted potentials and the vector fields at them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problem: (object) the problem with both the `potential` and \n",
    "        `field` methods for analytical solution evaluation.\n",
    "\n",
    "    x: (torch.tensor) the evaluation points. This array should be \n",
    "        >2-dimensional and have a shape of `(..., x_rows, x_cols)`.\n",
    "\n",
    "    n_eval: (int or None) the maximum mini-batch size. If None is \n",
    "        given, `x_rows` will be used as `n_eval`.\n",
    "\n",
    "    Output Dictionary\n",
    "    ----------\n",
    "    v_np: (np.array) the evaluated potentials with a shape of\n",
    "        `(..., x_rows)`. \n",
    "\n",
    "    e_np: (np.array) the evaluated vector fields with a shape of\n",
    "        `(..., x_rows, x_cols)`.\n",
    "    \"\"\"\n",
    "\n",
    "    assert hasattr(problem, 'potential')\n",
    "    assert callable(problem.potential)\n",
    "    assert hasattr(problem, 'field')\n",
    "    assert callable(problem.field)\n",
    "\n",
    "    x_rows, x_cols = tuple(x.shape)[-2:]\n",
    "    x_bd_ = tuple(x.shape)[:-2]\n",
    "    x_bd = (1,) if len(x_bd_) == 0 else x_bd_\n",
    "    msg_ = f'Cannot have {x.shape} fed to {problem.shape}'\n",
    "    assert len(x_bd) <= problem.ndim, msg_\n",
    "    if len(x_bd) < problem.ndim:\n",
    "        x_bd = tuple([1] * (problem.ndim-len(x_b)) + list(x_bd))\n",
    "    assert all((a == b) or (a == 1) or (b == 1) \n",
    "               for a, b in zip(x_bd, problem.shape)), msg_\n",
    "    n_eval = x_rows if n_eval is None else n_eval\n",
    "    if out_lib == 'numpy':\n",
    "        to_lib = lambda a: a.detach().cpu().numpy()\n",
    "        lib_cat = lambda al: np.concatenate(al, axis=1)\n",
    "        lpf = '_np'\n",
    "    elif out_lib == 'torch':\n",
    "        to_lib = lambda a: a\n",
    "        lib_cat = lambda al: torch.cat(al, dim=1)\n",
    "        lpf = ''\n",
    "    else:\n",
    "        raise ValueError(f'outlib={outlib} not defined.')\n",
    "\n",
    "    n_batches = int(np.ceil(x_rows / n_eval))\n",
    "    v_list = []\n",
    "    e_list = []\n",
    "    for i in range(n_batches):\n",
    "        x_i = x[..., (i*n_eval):((i+1)*n_eval), :]\n",
    "        xi_rows = x_i.shape[-2]\n",
    "        x_ii = x_i.reshape(*x_bd, xi_rows, x_cols)\n",
    "        x_iii = x_ii.expand(*problem.shape, xi_rows, x_cols)\n",
    "        v_i = problem.potential(x_iii)\n",
    "        v_list.append(to_lib(v_i))\n",
    "        if get_field:\n",
    "            e_i = problem.field(x_iii)\n",
    "            e_list.append(to_lib(e_i))\n",
    "\n",
    "    v = lib_cat(v_list)\n",
    "    if get_field:\n",
    "        e = lib_cat(e_list)\n",
    "    else:\n",
    "        e = None\n",
    "    outdict = {f'v{lpf}': v, f'e{lpf}': e}\n",
    "    return outdict\n",
    "\n",
    "# useless!\n",
    "def make_grid(x_low, x_high, dim, n_gpd, lib):\n",
    "    \"\"\"\n",
    "    Creates a grid of points using the mesgrid functions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_low: (list) a list of length `dim` with floats \n",
    "        representing the lower limits of the grid.\n",
    "    \n",
    "    x_high: (list) a list of length `dim` with floats \n",
    "        representing the higher limits of the grid.\n",
    "    \n",
    "    dim: (int) the dimension of the grid space.\n",
    "    \n",
    "    n_gpd: (int) the number of points in each \n",
    "        grid dimension. This yields a total of \n",
    "        `n_gpd**dim` points in the total grid.\n",
    "        \n",
    "    lib: (str) either 'torch' or 'numpy'. This determines \n",
    "        the type of `x` output.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    x: (torch.tensor or np.array) a 2-d tensor or array \n",
    "        with the shape of `(n_gpd**dim, dim)`. \n",
    "    \n",
    "    xi_msh_np: (list of np.array) a list of length `dim` \n",
    "        with meshgrid tensors each with a shape of \n",
    "        `[n_gpd] * dim`.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert dim == 2, 'not implemented yet'\n",
    "    assert len(x_low) == dim\n",
    "    assert len(x_high) == dim\n",
    "    assert lib in ('torch', 'numpy')\n",
    "    library = torch if lib == 'torch' else np\n",
    "    tnper = lambda a: a.cpu().detach().numpy()\n",
    "    nper = tnper if lib == 'torch' else lambda a: a\n",
    "    \n",
    "    x1_low, x2_low = x_low\n",
    "    x1_high, x2_high = x_high\n",
    "    n_g_plt = n_gpd ** dim\n",
    "\n",
    "    x1_1d = library.linspace(x1_low, x1_high, n_gpd)\n",
    "    assert x1_1d.shape == (n_gpd,)\n",
    "\n",
    "    x2_1d = library.linspace(x2_low, x2_high, n_gpd)\n",
    "    assert x2_1d.shape == (n_gpd,)\n",
    "\n",
    "    x1_msh, x2_msh = library.meshgrid(x1_1d, x2_1d)\n",
    "    assert x1_msh.shape == (n_gpd, n_gpd)\n",
    "    assert x2_msh.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x1 = x1_msh.reshape(n_g_plt, 1)\n",
    "    assert x1.shape == (n_g_plt, 1)\n",
    "\n",
    "    x2 = x2_msh.reshape(n_g_plt, 1)\n",
    "    assert x2.shape == (n_g_plt, 1)\n",
    "\n",
    "    x1_1d_c = x1_1d.reshape(n_gpd, 1)\n",
    "    assert x1_1d_c.shape == (n_gpd, 1)\n",
    "\n",
    "    x2_1d_c = x2_1d.reshape(n_gpd, 1)\n",
    "    assert x2_1d_c.shape == (n_gpd, 1)\n",
    "\n",
    "    x1_msh_np = nper(x1_msh)\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x2_msh_np = nper(x2_msh)\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "\n",
    "    x = torch.cat([x1, x2], dim=1)\n",
    "    assert x.shape == (n_g_plt, dim)\n",
    "\n",
    "    x_np = nper(x)\n",
    "    assert x_np.shape == (n_g_plt, dim)\n",
    "    \n",
    "    xi_msh_np = [x1_msh_np, x2_msh_np]\n",
    "    outdict = dict(x=x, xi_msh_np=xi_msh_np)\n",
    "\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def plot_sol(x1_msh_np, x2_msh_np, sol_dict, fig=None, ax=None, cax=None):\n",
    "    n_gpd, dim = x1_msh_np.shape[0], x1_msh_np.ndim\n",
    "    assert dim == 2, f'dim={dim}, x1_msh_np.shape={x1_msh_np.shape}'\n",
    "    assert x1_msh_np.shape == (n_gpd, n_gpd)\n",
    "    assert x2_msh_np.shape == (n_gpd, n_gpd)\n",
    "    n_g = (n_gpd ** dim)\n",
    "   \n",
    "    if fig is None:\n",
    "        assert ax is None\n",
    "        assert cax is None\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(3.0, 2.5), dpi=72)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    else:\n",
    "        assert ax is not None\n",
    "   \n",
    "    e_percentile_cap = 90\n",
    "    if 'v_np' in sol_dict:\n",
    "        v_np = sol_dict['v_np']\n",
    "    else:\n",
    "        v_np = sol_dict['v'].detach().cpu().numpy()\n",
    "    assert v_np.shape[-1] == n_g\n",
    "    \n",
    "    v_msh_np = v_np.reshape(-1, n_gpd, n_gpd).mean(axis=0)\n",
    "    im = ax.pcolormesh(x1_msh_np, x2_msh_np, v_msh_np,\n",
    "                        shading='auto', cmap='RdBu')\n",
    "    if cax is not None:\n",
    "        fig.colorbar(im, cax=cax)\n",
    "\n",
    "    if 'e_np' in sol_dict:\n",
    "        e_msh_np = sol_dict['e_np']\n",
    "    else:\n",
    "        e_msh_np = sol_dict['e']\n",
    "        if e_msh_np is not None:\n",
    "            e_msh_np = e_msh_np.detach().cpu().numpy()\n",
    "    \n",
    "    if e_msh_np is not None:\n",
    "        assert e_msh_np.shape[-2:] == (n_g, dim)\n",
    "        e_msh_np = e_msh_np.reshape(-1, n_gpd,\n",
    "            n_gpd, dim).mean(axis=0)\n",
    "        if e_percentile_cap is not None:\n",
    "            e_size = np.sqrt((e_msh_np**2).sum(axis=-1))\n",
    "            e_size_cap = np.percentile(a=e_size, \n",
    "                q=e_percentile_cap, axis=None)\n",
    "            cap_coef = np.ones_like(e_size)\n",
    "            cap_coef[e_size > e_size_cap] = e_size_cap / \\\n",
    "                e_size[e_size > e_size_cap]\n",
    "            e_msh_capped = e_msh_np * \\\n",
    "                cap_coef.reshape(*e_msh_np.shape[:-1], 1)\n",
    "        else:\n",
    "            e_msh_capped = e_msh_np\n",
    "\n",
    "        ax.quiver(x1_msh_np, x2_msh_np,\n",
    "            e_msh_capped[:, :, 0], e_msh_capped[:, :, 1])\n",
    "    return fig, ax, cax\n",
    "\n",
    "\n",
    "def get_perfdict(e_pnts, e_mdlsol, e_prbsol):\n",
    "    \"\"\"\n",
    "    Computes the biased, bias-corrected, and slope-corrected error \n",
    "    metrics for the solutions of a Poisson problem.\n",
    "    \n",
    "    This function computes three types of MSE and MAE statistics:\n",
    "        \n",
    "        1. Plain: just take the model and ground truth solution\n",
    "            and subtract them to get the errors. No bias- or slope-correction \n",
    "            is applied to offset those degrees of freedom.\n",
    "            \n",
    "            shorthand: 'pln'\n",
    "            \n",
    "        2. Bias-corrected: subtracts the average value from both the model \n",
    "            and ground truth solutions, and then computes the errors.\n",
    "            \n",
    "            shorthand: 'bc'\n",
    "            \n",
    "        3. Slope-corrected: Since any linear function can be added to the\n",
    "            Poisson solutions without violating the poisson equation, this\n",
    "            function fits an ordinary least squares to both the model and\n",
    "            ground truth solutions, and then subtracts it from them. This\n",
    "            way, even the arbitrary-slope issue can be addressed.\n",
    "            \n",
    "            shorthand: 'slc'\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    e_pnts: (torch.tensor) The input points to the model and the ground truth.\n",
    "        This should have a shape of (n_seeds, n_evlpnts, dim).\n",
    "        \n",
    "    e_mdlsol: (torch.tensor) The model solution with a\n",
    "        (n_seeds, n_evlpnts) shape.\n",
    "    \n",
    "    e_prbsol: (torch.tensor) The ground truth solution with a\n",
    "        (n_seeds, n_evlpnts) shape.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    outdict: (dict) A mapping between the error keys and their numpy arrays.\n",
    "        The error keys are the cartesian product of ('pln', 'bc', 'slc') \n",
    "        and ('mse', 'mae').\n",
    "    \"\"\"\n",
    "    n_seeds, n_evlpnts, dim = e_pnts.shape\n",
    "    assert e_mdlsol.shape == (n_seeds, n_evlpnts, dim)\n",
    "    assert e_prbsol.shape == (n_seeds, n_evlpnts, dim)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # The plain non-processed error matrix\n",
    "        err_pln = e_mdlsol - e_prbsol\n",
    "        assert err_pln.shape == (n_seeds, n_evlpnts, dim)\n",
    "        \n",
    "        # The bias-corrected error matrix\n",
    "        e_mdlsol2 = e_mdlsol - e_mdlsol.mean(dim=1, keepdims=True)\n",
    "        assert e_mdlsol2.shape == (n_seeds, n_evlpnts, dim)\n",
    "        e_prbsol2 = e_prbsol - e_prbsol.mean(dim=1, keepdims=True)\n",
    "        assert e_prbsol2.shape == (n_seeds, n_evlpnts, dim)\n",
    "        err_bc = e_mdlsol2 - e_prbsol2\n",
    "        assert err_bc.shape == (n_seeds, n_evlpnts, dim)\n",
    "        \n",
    "        # The normalized error matrix\n",
    "        e_mdlsol3 = e_mdlsol2.reshape(n_seeds, n_evlpnts*dim)\n",
    "        assert e_mdlsol3.shape == (n_seeds, n_evlpnts*dim)\n",
    "        \n",
    "        e_prbsol3 = e_prbsol2.reshape(n_seeds, n_evlpnts*dim)\n",
    "        assert e_prbsol3.shape == (n_seeds, n_evlpnts*dim)\n",
    "        \n",
    "        e_mdlsol4 = e_mdlsol3 / e_mdlsol3.std(dim=1, keepdim=True)\n",
    "        assert e_mdlsol4.shape == (n_seeds, n_evlpnts*dim)\n",
    "        \n",
    "        e_prbsol4 = e_prbsol3 / e_prbsol3.std(dim=1, keepdim=True)\n",
    "        assert e_prbsol4.shape == (n_seeds, n_evlpnts*dim)\n",
    "        \n",
    "        err_scn = e_mdlsol4 - e_prbsol4\n",
    "        assert err_scn.shape == (n_seeds, n_evlpnts*dim)\n",
    "        \n",
    "        # Computing the mse and mae values\n",
    "        e_plnmse = err_pln.square().sum(dim=-1).mean(dim=-1)\n",
    "        assert e_plnmse.shape == (n_seeds,)\n",
    "        e_plnmae = err_pln.abs().sum(dim=-1).mean(dim=-1)\n",
    "        assert e_plnmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_bcmse = err_bc.square().sum(dim=-1).mean(dim=-1)\n",
    "        assert e_bcmse.shape == (n_seeds,)\n",
    "        e_bcmae = err_bc.abs().sum(dim=-1).mean(dim=-1)\n",
    "        assert e_bcmse.shape == (n_seeds,)\n",
    "        \n",
    "        e_scnmse = err_scn.square().mean(dim=-1)\n",
    "        assert e_scnmse.shape == (n_seeds,)\n",
    "        e_scnmae = err_scn.abs().mean(dim=-1)\n",
    "        assert e_scnmse.shape == (n_seeds,)\n",
    "    \n",
    "        outdict = {'pln/mse': e_plnmse.detach().cpu().numpy(),\n",
    "                   'pln/mae': e_plnmae.detach().cpu().numpy(),\n",
    "                   'bc/mse': e_bcmse.detach().cpu().numpy(),\n",
    "                   'bc/mae': e_bcmae.detach().cpu().numpy(),\n",
    "                   'scn/mse': e_scnmse.detach().cpu().numpy(),\n",
    "                   'scn/mae': e_scnmae.detach().cpu().numpy()}\n",
    "    \n",
    "    return outdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca080cf",
   "metadata": {},
   "source": [
    "## Optional Visualization Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052d0d8",
   "metadata": {},
   "source": [
    "### Visualizing the True Potential and Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf43c47",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "ex_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "ex_tchdevice = torch.device(ex_device)\n",
    "ex_tchdtype = torch.double\n",
    "\n",
    "n_bch = 1\n",
    "ex_rng = BatchRNG(shape=(n_bch,), lib='torch', device=ex_tchdevice, dtype=ex_tchdtype,\n",
    "               unif_cache_cols=10000, norm_cache_cols=10000)\n",
    "ex_rng.seed(np.broadcast_to(12345+np.arange(n_bch), ex_rng.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b83a2",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "dim = 3\n",
    "n_wires = 3\n",
    "ex_source = [[[-0.5, -0.5, -1], [0, 0, -1], [0.5, 0.5, -1]]]\n",
    "ex_sink =   [[[-0.5, -0.5,  1], [0, 0,  1], [0.5, 0.5,  1]]]\n",
    "ex_source = np.broadcast_to(ex_source, (n_bch, n_wires, dim)).copy()\n",
    "ex_sink = np.broadcast_to(ex_sink, (n_bch, n_wires, dim)).copy()\n",
    "current = np.ones((n_bch, n_wires))\n",
    "# ex_source, ex_sink = ex_source + 1e-3, ex_sink + 1e-3\n",
    "ex_problem = DeltaLineProblem(source=ex_source, sink=ex_sink, current=current,\n",
    "    tch_device=ex_tchdevice, tch_dtype=ex_tchdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a423106",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "ex_ng1, ex_ng2, ex_ng3 = (200, 200, 6)\n",
    "ex_xilow  = [-1., -1., 0.]\n",
    "ex_xihigh = [ 1.,  1., 1.]\n",
    "ex_ngi = (ex_ng1, ex_ng2, ex_ng3)\n",
    "\n",
    "ex_dim, ex_n_xg = len(ex_ngi), int(np.prod(ex_ngi))\n",
    "\n",
    "ex_x_g = create_xg(ex_xilow, ex_xihigh, ex_ngi, ex_tchdevice, ex_tchdtype)\n",
    "assert ex_x_g.shape == (ex_ng1, ex_ng2, ex_ng3, ex_dim)\n",
    "\n",
    "ex_x_gf = ex_x_g.reshape(1, ex_n_xg, ex_dim).expand(n_bch, ex_n_xg, ex_dim)\n",
    "assert ex_x_gf.shape == (n_bch, ex_n_xg, ex_dim)\n",
    "\n",
    "# 1. The magnetic potentials\n",
    "A_gf1 = ex_problem.potential(ex_x_gf)\n",
    "assert A_gf1.shape == (n_bch, ex_n_xg, ex_dim)\n",
    "\n",
    "A_gf2 = A_gf1.clip(A_gf1.nanquantile(0.001, dim=1, keepdim=True), \n",
    "                   A_gf1.nanquantile(0.999, dim=1, keepdim=True))\n",
    "assert A_gf2.shape == (n_bch, ex_n_xg, ex_dim)\n",
    "\n",
    "y_g1 = A_gf2.mean(dim=0)\n",
    "assert y_g1.shape == (ex_n_xg, ex_dim)\n",
    "\n",
    "# 2. The magnetic fields (1)\n",
    "B_gf1 = ex_problem.field(ex_x_gf)\n",
    "assert B_gf1.shape == (n_bch, ex_n_xg, ex_dim)\n",
    "\n",
    "B_gf2 = B_gf1.clip(B_gf1.nanquantile(0.01, dim=1, keepdim=True), \n",
    "                   B_gf1.nanquantile(0.99, dim=1, keepdim=True))\n",
    "assert B_gf2.shape == (n_bch, ex_n_xg, ex_dim)\n",
    "\n",
    "y_g2 = B_gf2.mean(dim=0)\n",
    "assert y_g2.shape == (ex_n_xg, ex_dim)\n",
    "\n",
    "# 3. The magnetic field (2)\n",
    "B_gf1n = B_gf1.norm(dim=-1, keepdim=True)\n",
    "assert B_gf1n.shape == (n_bch, ex_n_xg, 1)\n",
    "\n",
    "B_gf2n = B_gf1n.clip(B_gf1n.nanquantile(0.1, dim=1, keepdim=True), \n",
    "                     B_gf1n.nanquantile(0.9, dim=1, keepdim=True))\n",
    "assert B_gf2n.shape == (n_bch, ex_n_xg, 1)\n",
    "\n",
    "B_gf3 = B_gf1 * (B_gf2n / B_gf1n)\n",
    "assert B_gf3.shape == (n_bch, ex_n_xg, ex_dim)\n",
    "\n",
    "y_g3 = B_gf3.mean(dim=0)\n",
    "assert y_g3.shape == (ex_n_xg, ex_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b4686",
   "metadata": {},
   "source": [
    "### Checking the Analytical Curl Property "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4fdc38",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "ex_nx = 2500\n",
    "ex_x = ex_rng.uniform((n_bch, ex_nx, dim)) * 2 - 1.\n",
    "ex_x = torch.nn.Parameter(ex_x)\n",
    "assert ex_x.shape == (n_bch, ex_nx, dim)\n",
    "\n",
    "ex_A = ex_problem.potential(ex_x)\n",
    "assert ex_A.shape == (n_bch, ex_nx, dim)\n",
    "\n",
    "ex_B = ex_problem.field(ex_x)\n",
    "assert ex_B.shape == (n_bch, ex_nx, dim)\n",
    "\n",
    "ex_Acurl = curl(ex_A, ex_x)\n",
    "assert ex_Acurl.shape == (n_bch, ex_nx, dim)\n",
    "\n",
    "assert (ex_B - ex_Acurl).abs().max() < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334269a0",
   "metadata": {},
   "source": [
    "### Visualizing the Magnetic Potential and Field Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929e79d",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "n_rows, n_cols = 2, 3 \n",
    "ex1_fig, ex1_axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*2., n_rows*2.), \n",
    "    sharex=False, sharey=False)\n",
    "ex1_axes = np.array(ex1_axes).reshape(n_rows, n_cols)\n",
    "draw_heatmap(ex_x_g, y_g1, ex1_fig, ex1_axes, field_abrv='A', \n",
    "    components='xyz', show_title='top', zstr_loc='left')\n",
    "ex1_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd260e8",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "n_rows, n_cols = ex_ngi[-1]//3, 3\n",
    "ex2_fig, ex2_axes = plt.subplots(n_rows, n_cols, \n",
    "    figsize=(n_cols*2., n_rows*2.), sharex=False, sharey=False)\n",
    "ex2_axes = np.array(ex2_axes).reshape(n_rows, n_cols)\n",
    "draw_heatmap(ex_x_g, y_g1, ex2_fig, ex2_axes, field_abrv='A', \n",
    "    components='z', show_title='all')\n",
    "ex2_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa6749",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "n_rows, n_cols = ex_ngi[-1]//3, 3\n",
    "ex3_fig, ex3_axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*2., n_rows*2.), \n",
    "    sharex=False, sharey=False, dpi=100)\n",
    "ex3_axes = np.array(ex3_axes).reshape(n_rows, n_cols)\n",
    "draw_quiver(ex_x_g, y_g3, ex3_fig, ex3_axes, field_abrv='B', \n",
    "    show_title='all', cmap='RdBu', subsamp=10)\n",
    "ex3_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8825e6",
   "metadata": {},
   "source": [
    "### Visualizing Example Sampled Volumes and Surface Tangents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7147f68",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "n_bch = 5\n",
    "ex4_rng = BatchRNG(shape=(n_bch,), lib='torch', device=ex_tchdevice, dtype=ex_tchdtype,\n",
    "               unif_cache_cols=5000, norm_cache_cols=5000)\n",
    "ex4_rng.seed(np.broadcast_to(12345+np.arange(n_bch), ex4_rng.shape))\n",
    "\n",
    "dim = 3\n",
    "ex4_nwires = 3\n",
    "ex4_source = [[[-0.5, -0.5, -1], [0, 0, -1], [0.5, 0.5, -1]]]\n",
    "ex4_sink =   [[[-0.5, -0.5,  1], [0, 0,  1], [0.5, 0.5,  1]]]\n",
    "ex4_source = np.broadcast_to(ex4_source, (n_bch, ex4_nwires, dim)).copy()\n",
    "ex4_sink = np.broadcast_to(ex4_sink, (n_bch, ex4_nwires, dim)).copy()\n",
    "ex4_current = np.ones((n_bch, ex4_nwires))\n",
    "prob_ex4 = DeltaLineProblem(source=ex4_source, sink=ex4_sink, \n",
    "    current=ex4_current, tch_device=ex_tchdevice, tch_dtype=ex_tchdtype)\n",
    "\n",
    "volsampler_ex4 = DiskSampler(c_dstr='uniform', c_params=dict(\n",
    "                             low=np.broadcast_to([-1.0, -1.0, -1.0], (n_bch, 3)).copy(),\n",
    "                             high=np.broadcast_to([1.0,  1.0,  1.0], (n_bch, 3)).copy() ),\n",
    "                             r_dstr='uniform', r_params=dict(\n",
    "                             low=np.broadcast_to([0.1], (n_bch,)).copy(),\n",
    "                             high=np.broadcast_to([1.5], (n_bch,)).copy()),\n",
    "                             n_dstr='uball', n_params=dict(),\n",
    "                             batch_rng=ex4_rng)\n",
    "\n",
    "sphsampler_ex4 = SphereSampler(batch_rng=ex4_rng)\n",
    "\n",
    "ex4_nvol, ex4_nsrfpts, ex4_dim = 5, 100, 3\n",
    "ex4_vols = volsampler_ex4(n=ex4_nvol)\n",
    "ex4_cntrvols = ex4_vols['centers']\n",
    "assert ex4_cntrvols.shape == (n_bch, ex4_nvol, ex4_dim)\n",
    "ex4_nrmlvols = ex4_vols['normals'].squeeze(-1)\n",
    "assert ex4_nrmlvols.shape == (n_bch, ex4_nvol, ex4_dim)\n",
    "\n",
    "integs = prob_ex4.integrate_volumes(ex4_vols)\n",
    "\n",
    "ex4_srfsamps = sphsampler_ex4(ex4_vols, ex4_nsrfpts, do_detspacing=True)\n",
    "ex4_points = ex4_srfsamps['points']\n",
    "assert ex4_points.shape == (n_bch, ex4_nvol, ex4_nsrfpts, ex4_dim)\n",
    "ex4_srfnrmls = ex4_srfsamps['normals']\n",
    "assert ex4_srfnrmls.shape == (n_bch, ex4_nvol, ex4_nsrfpts, ex4_dim)\n",
    "ex4_srftans = ex4_srfsamps['tangents']\n",
    "assert ex4_srftans.shape == (n_bch, ex4_nvol, ex4_nsrfpts, ex4_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c750fd6",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "# Making sure that the disk normal vectors are prependicular \n",
    "# to the point vectors\n",
    "aa = (ex4_points - ex4_cntrvols.unsqueeze(-2))\n",
    "assert aa.shape == (n_bch, ex4_nvol, ex4_nsrfpts, ex4_dim)\n",
    "bb = ex4_nrmlvols.reshape(n_bch, ex4_nvol, 1, ex4_dim)\n",
    "\n",
    "assert ((aa * bb).sum(dim=-1).abs() < 1e-8).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7768b07",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "ex4_fig = plt.figure(figsize=(4, 4), dpi=100)\n",
    "ex4_ax = ex4_fig.add_subplot(projection='3d')\n",
    "ex4_ax.set_aspect('equal')\n",
    "ex4_ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "ex4_ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "ex4_ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "ex4_ax.set_xlim(-2., 2.)\n",
    "ex4_ax.set_ylim(-2., 2.)\n",
    "ex4_ax.set_zlim(-2., 2.)\n",
    "\n",
    "pts_plt = ex4_points.reshape(n_bch*ex4_nvol, ex4_nsrfpts, ex4_dim).detach().cpu().numpy()\n",
    "tngnts_plt = ex4_srftans.reshape(n_bch*ex4_nvol, ex4_nsrfpts, ex4_dim).detach().cpu().numpy()\n",
    "cntrs_plt = ex4_cntrvols.reshape(n_bch*ex4_nvol, ex4_dim).detach().cpu().numpy()\n",
    "dsknrmls_plt = ex4_nrmlvols.reshape(n_bch*ex4_nvol, ex4_dim).detach().cpu().numpy()\n",
    "for i_sph in [0, 2, 3]:\n",
    "    pts_iplt = pts_plt[i_sph]\n",
    "    tngs_iplt = tngnts_plt[i_sph] * 0.5\n",
    "    cntrs_iplt = cntrs_plt[i_sph]\n",
    "    dnrms_iplt = dsknrmls_plt[i_sph]\n",
    "    \n",
    "    ex4_ax.scatter(pts_iplt[:, 0], pts_iplt[:, 1], pts_iplt[:, 2], \n",
    "        marker='o', c='rbbgcmyk'[i_sph], s=1)\n",
    "    \n",
    "    ex4_ax.quiver(pts_iplt[::10, 0], pts_iplt[::10, 1], pts_iplt[::10, 2], \n",
    "        tngs_iplt[::10, 0], tngs_iplt[::10, 1], tngs_iplt[::10, 2], \n",
    "        color='black')\n",
    "    \n",
    "    ex4_ax.quiver(cntrs_iplt[0], cntrs_iplt[1], cntrs_iplt[2], \n",
    "        dnrms_iplt[0], dnrms_iplt[1], dnrms_iplt[2], \n",
    "        color='orange')\n",
    "\n",
    "ex4_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817399f6",
   "metadata": {},
   "source": [
    "## Utility Functions for Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d63e3",
   "metadata": {
    "code_folding": [
     8,
     60,
     85
    ]
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "########### Sanity Checking Utility Functions ###########\n",
    "#########################################################\n",
    "\n",
    "msg_bcast = '{} should be np broadcastable to {}={}. '\n",
    "msg_bcast += 'However, it has an inferred shape of {}.'\n",
    "\n",
    "\n",
    "def get_arr(name, trgshp_str, trns_opts):\n",
    "    \"\"\"\n",
    "    Gets a list of values, and checks if it is broadcastable to a \n",
    "    target shape. If the shape does not match, it will raise a proper\n",
    "    assertion error with a meaninful message. The output is a numpy \n",
    "    array that is guaranteed to be broadcastable to the target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: (str) name of the option / hyper-parameter.\n",
    "\n",
    "    trgshp_str: (str) the target shape elements representation. Must be a \n",
    "        valid python expression where the needed elements .\n",
    "\n",
    "    trns_opts: (dict) a dictionary containing the variables needed \n",
    "        for the string to list translation of val.\n",
    "\n",
    "    Key Variables\n",
    "    -------------\n",
    "    `val = trns_opts[name]`: (list or str) list of values read \n",
    "        from the config file. If a string is provided, python's \n",
    "        `eval` function will be used to translate it into a list.\n",
    "        \n",
    "    `trg_shape = eval_formula(trgshp_str, trns_opts)`: (tuple) \n",
    "        the target shape.\n",
    "    \n",
    "    Output\n",
    "    ----------\n",
    "    val_np: (np.array) the numpy array of val. \n",
    "    \"\"\"\n",
    "    msg_ =  f'\"{name}\" must be in trns_opts but it isnt: {trns_opts}'\n",
    "    assert name in trns_opts, msg_\n",
    "    val = trns_opts[name]\n",
    "    \n",
    "    if isinstance(val, str):\n",
    "        val_list = eval_formula(val, trns_opts)\n",
    "    else:\n",
    "        val_list = val\n",
    "    val_np = np.array(val_list)\n",
    "    src_shape = val_np.shape\n",
    "    trg_shape = eval_formula(trgshp_str, trns_opts)\n",
    "    msg_ = msg_bcast.format(name, trgshp_str, trg_shape, src_shape)\n",
    "\n",
    "    assert len(val_np.shape) == len(trg_shape), msg_\n",
    "\n",
    "    is_bcastble = all((x == y or x == 1 or y == 1) for x, y in\n",
    "                      zip(src_shape, trg_shape))\n",
    "    assert is_bcastble, msg_\n",
    "\n",
    "    return val_np\n",
    "\n",
    "\n",
    "def eval_formula(formula, variables):\n",
    "    \"\"\"\n",
    "    Gets a string formula and uses the `eval` function of python to  \n",
    "    translate it into a python variable. The necessary variables for \n",
    "    translation are provided through the `variables` argument.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    formula (str): a string that can be passed to `eval`.\n",
    "        Example: \"[np.sqrt(dim), 'a', None]\"\n",
    "\n",
    "    variables (dict): a dictionary of variables used in the formula.\n",
    "        Example: {\"dim\": 4}\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    pyobj (object): the translated formula into a python object\n",
    "        Example: [2.0, 'a', None]\n",
    "\n",
    "    \"\"\"\n",
    "    locals().update(variables)\n",
    "    pyobj = eval(formula)\n",
    "    return pyobj\n",
    "\n",
    "\n",
    "def chck_dstrargs(opt, cfgdict, dstr2args, opt2req, parnt_optdstr=None):\n",
    "    \"\"\"\n",
    "    Checks if the distribution arguments are provided correctly. Works \n",
    "    with hirarchical models through recursive applications. Proper error \n",
    "    messages are displayed if one of the checks fails.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    opt: (str) the option name.\n",
    "\n",
    "    cfgdict: (dict) the config dictionary.\n",
    "\n",
    "    dstr2args: (dict) a mapping between distribution and their \n",
    "        required arguments.\n",
    "        \n",
    "    opt2req: (dict) required arguments for an option itself, not \n",
    "        necessarily required by the option's distribution.\n",
    "    \"\"\"\n",
    "    opt_dstr = cfgdict.get(f'{opt}/dstr', 'fixed')\n",
    "\n",
    "    msg_ = f'Unknown {opt}_dstr: it should be one of {list(dstr2args.keys())}'\n",
    "    assert opt_dstr in dstr2args, msg_\n",
    "\n",
    "    opt2req = dict() if opt2req is None else opt2req\n",
    "    optreqs = opt2req.get(opt, tuple())\n",
    "    must_spec = list(dstr2args[opt_dstr]) + list(optreqs)\n",
    "    avid_spec = list(chain.from_iterable(\n",
    "        v for k, v in dstr2args.items() if k != opt_dstr))\n",
    "    avid_spec = [k for k in avid_spec if k not in must_spec]\n",
    "\n",
    "    if opt_dstr == 'fixed':\n",
    "        # To avoid infinite recursive calls, we should end this here.\n",
    "        msg_ = f'\"{opt}\" must be specified.'\n",
    "        if parnt_optdstr is not None:\n",
    "            parnt_opt, parnt_dstr = parnt_optdstr\n",
    "            msg_ += f'\"{parnt_opt}\" was specified as \"{parnt_dstr}\", and'\n",
    "        msg_ += f' \"{opt}\" was specified as \"{opt_dstr}\".'\n",
    "        if len(optreqs) > 0:\n",
    "            msg_ += f' Also, \"{opt}\" requires \"{optreqs}\" to be specified.'\n",
    "        opt_val = cfgdict.get(opt, None)\n",
    "        assert opt_val is not None, msg_\n",
    "    else:\n",
    "        for arg in must_spec:\n",
    "            opt_arg = f'{opt}{arg}'\n",
    "            chck_dstrargs(opt_arg, cfgdict, dstr2args, opt2req, (opt, opt_dstr))\n",
    "\n",
    "    for arg in avid_spec:\n",
    "        opt_arg = f'{opt}{arg}'\n",
    "        opt_arg_val = cfgdict.get(opt_arg, None)\n",
    "        msg_ = f'\"{opt_arg}\" should not be specified, since \"{opt}\" '\n",
    "        msg_ += f'appears to follow the \"{opt_dstr}\" distribution.'\n",
    "        assert opt_arg_val is None, msg_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de0932",
   "metadata": {},
   "source": [
    "## JSON Config Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450ad70",
   "metadata": {
    "code_folding": [
     1
    ],
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "json_cfgpath = f'../configs/00_scratch/08_maxwell.json'\n",
    "# ! rm -rf \"./18_maxwell/results/08_maxwell.h5\"\n",
    "# ! rm -rf \"./18_maxwell/storage/08_maxwell\"\n",
    "with open(json_cfgpath, 'r') as fp:\n",
    "    json_cfgdict = json.load(fp, object_pairs_hook=odict)\n",
    "json_cfgdict['io/config_id'] = '08_maxwell'\n",
    "json_cfgdict['io/results_dir'] = './18_maxwell/results'\n",
    "json_cfgdict['io/storage_dir'] = './18_maxwell/storage'\n",
    "json_cfgdict['io/tch/device'] = 'cuda:0'\n",
    "\n",
    "all_cfgdicts = preproc_cfgdict(json_cfgdict)\n",
    "cfg_dict_input = all_cfgdicts[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adaeaa29",
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "source": [
    "def main(cfg_dict_input):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd85e2b",
   "metadata": {},
   "source": [
    "## Retrieving Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c640bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    cfg_dict = cfg_dict_input.copy()\n",
    "\n",
    "    #########################################################\n",
    "    #################### Ignored Options ####################\n",
    "    #########################################################\n",
    "    cfgdesc = cfg_dict.pop('desc', None)\n",
    "    cfgdate = cfg_dict.pop('date', None)\n",
    "\n",
    "    #########################################################\n",
    "    ################### Mandatory Options ###################\n",
    "    #########################################################\n",
    "    prob_type = cfg_dict.pop('problem')\n",
    "    rng_seed_list = cfg_dict.pop('rng_seed/list')\n",
    "    dim = cfg_dict.pop('dim')\n",
    "    assert dim == 3\n",
    "\n",
    "    n_srf = cfg_dict.pop('vol/n')\n",
    "    n_srfpts_mdl = cfg_dict.pop('srfpts/n/mdl')\n",
    "    n_srfpts_trg = cfg_dict.pop('srfpts/n/trg')\n",
    "    do_detspacing = cfg_dict.pop('srfpts/detspc')\n",
    "    do_dblsampling = cfg_dict.pop('srfpts/dblsmpl')\n",
    "\n",
    "    do_bootstrap = cfg_dict.pop('trg/btstrp')\n",
    "    if do_bootstrap:\n",
    "        tau = cfg_dict.pop('trg/tau')\n",
    "        w_trgreg = cfg_dict.pop('trg/reg/w')\n",
    "    else:\n",
    "        w_trgreg = 0.0\n",
    "    w_trg = cfg_dict.pop('trg/w', None)\n",
    "\n",
    "    opt_type = cfg_dict.pop('opt/dstr')\n",
    "    n_epochs = cfg_dict.pop('opt/epoch')\n",
    "    lr = cfg_dict.pop('opt/lr')\n",
    "\n",
    "    #########################################################\n",
    "    ################## Neural Spec Options ##################\n",
    "    #########################################################\n",
    "    nn_dstr = cfg_dict.pop('nn/dstr')\n",
    "    if nn_dstr == 'mlp':\n",
    "        nn_width = cfg_dict.pop('nn/width')\n",
    "        nn_hidden = cfg_dict.pop('nn/hidden')\n",
    "        nn_act = cfg_dict.pop('nn/act')\n",
    "    else:\n",
    "        msg_ = f'nn/dstr=\"{nn_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    #########################################################\n",
    "    ############## Charge Distribution Options ##############\n",
    "    #########################################################\n",
    "    wire_dstr = cfg_dict.pop('wire/dstr')\n",
    "    wire_n = cfg_dict.pop('wire/n')\n",
    "\n",
    "    wire_w_dstr = cfg_dict.pop('wire/w/dstr', 'fixed')\n",
    "    if wire_w_dstr == 'fixed':\n",
    "        wire_w_ = cfg_dict.pop('wire/w')\n",
    "    elif wire_w_dstr == 'uniform':\n",
    "        wire_w_low_ = cfg_dict.pop('wire/w/low', None)\n",
    "        wire_w_high_ = cfg_dict.pop('wire/w/high', None)\n",
    "    elif wire_w_dstr == 'normal':\n",
    "        wire_w_loc_ = cfg_dict.pop('wire/w/loc', None)\n",
    "        wire_w_scale_ = cfg_dict.pop('wire/w/scale', None)\n",
    "    else:\n",
    "        msg_ = f'wire/w/dstr=\"{wire_w_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    wire_src_dstr = cfg_dict.pop('wire/src/dstr', 'fixed')\n",
    "    if wire_src_dstr == 'fixed':\n",
    "        wire_src_ = cfg_dict.pop('wire/src')\n",
    "    elif wire_src_dstr == 'uniform':\n",
    "        wire_src_low_ = cfg_dict.pop('wire/src/low')\n",
    "        wire_src_high_ = cfg_dict.pop('wire/src/high')\n",
    "    elif wire_src_dstr == 'normal':\n",
    "        wire_src_loc_ = cfg_dict.pop('wire/src/loc')\n",
    "        wire_src_scale_ = cfg_dict.pop('wire/src/scale')\n",
    "    elif wire_src_dstr == 'ball':\n",
    "        wire_src_c_ = cfg_dict.pop('wire/src/c')\n",
    "        wire_src_r_ = cfg_dict.pop('wire/src/r')\n",
    "    else:\n",
    "        msg_ = f'wire/src/dstr=\"{wire_src_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    wire_snk_dstr = cfg_dict.pop('wire/snk/dstr', 'fixed')\n",
    "    if wire_snk_dstr == 'fixed':\n",
    "        wire_snk_ = cfg_dict.pop('wire/snk')\n",
    "    elif wire_snk_dstr == 'uniform':\n",
    "        wire_snk_low_ = cfg_dict.pop('wire/snk/low')\n",
    "        wire_snk_high_ = cfg_dict.pop('wire/snk/high')\n",
    "    elif wire_snk_dstr == 'normal':\n",
    "        wire_snk_loc_ = cfg_dict.pop('wire/snk/loc')\n",
    "        wire_snk_scale_ = cfg_dict.pop('wire/snk/scale')\n",
    "    elif wire_snk_dstr == 'ball':\n",
    "        wire_snk_c_ = cfg_dict.pop('wire/snk/c')\n",
    "        wire_snk_r_ = cfg_dict.pop('wire/snk/r')\n",
    "    else:\n",
    "        msg_ = f'wire/snk/dstr=\"{wire_snk_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    #########################################################\n",
    "    ############ Problem Parameter Search Options ###########\n",
    "    #########################################################\n",
    "    srch_dstr = cfg_dict.pop('srch/dstr', None)\n",
    "    if srch_dstr in ('mcmc',):\n",
    "        srch_enc = cfg_dict.pop('srch/enc')\n",
    "        srch_prior_dstr = cfg_dict.pop('srch/prior/dstr')\n",
    "        assert srch_prior_dstr == 'normal'\n",
    "        srch_prior_loc_ = cfg_dict.pop('srch/prior/loc')\n",
    "        srch_prior_scale_ = cfg_dict.pop('srch/prior/scale')\n",
    "    elif srch_dstr is None:\n",
    "        pass\n",
    "    else:\n",
    "        msg_ = f'srch/dstr=\"{srch_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    # Shared parameters for all search methods\n",
    "    if srch_dstr is not None:\n",
    "        obs_x_dstr = cfg_dict.pop('srch/obs/x/dstr')\n",
    "        assert obs_x_dstr == 'uniform'\n",
    "        obs_x_low_ = cfg_dict.pop('srch/obs/x/low')\n",
    "        obs_x_high_ = cfg_dict.pop('srch/obs/x/high')\n",
    "        \n",
    "        obs_y_dstr = cfg_dict.pop('srch/obs/y/dstr')\n",
    "        obs_n = cfg_dict.pop('srch/obs/n')\n",
    "        \n",
    "        srch_metric_dstr = cfg_dict.pop('srch/metric/dstr')\n",
    "        srch_metric_coeff = cfg_dict.pop('srch/metric/coeff')\n",
    "        srch_metric_min = cfg_dict.pop('srch/metric/min', None)\n",
    "        srch_metric_max = cfg_dict.pop('srch/metric/max', None)\n",
    "        \n",
    "        srch_frq = cfg_dict.pop('srch/frq')\n",
    "        srch_inittrn = cfg_dict.pop('srch/inittrn')\n",
    "        srch_tmprtr = cfg_dict.pop('srch/tmprtr')\n",
    "        srch_sigma = cfg_dict.pop('srch/sigma')\n",
    "        n_grps = cfg_dict.pop('srch/grps')\n",
    "        srch_reset_n = cfg_dict.pop('srch/reset/n')\n",
    "        srch_reset_k = cfg_dict.pop('srch/reset/k')\n",
    "    \n",
    "    #########################################################\n",
    "    ############### Surface Sampling Options ################\n",
    "    #########################################################\n",
    "    vol_dstr = cfg_dict.pop('vol/dstr')\n",
    "\n",
    "    vol_c_dstr = cfg_dict.pop('vol/c/dstr', 'fixed')\n",
    "    if vol_c_dstr == 'fixed':\n",
    "        vol_c_ = cfg_dict.pop('vol/c')\n",
    "    elif vol_c_dstr == 'uniform':\n",
    "        vol_c_low_ = cfg_dict.pop('vol/c/low')\n",
    "        vol_c_high_ = cfg_dict.pop('vol/c/high')\n",
    "    elif vol_c_dstr == 'normal':\n",
    "        vol_c_loc_ = cfg_dict.pop('vol/c/loc')\n",
    "        vol_c_scale_ = cfg_dict.pop('vol/c/scale')\n",
    "    elif vol_c_dstr == 'ball':\n",
    "        vol_c_c_ = cfg_dict.pop('vol/c/c')\n",
    "        vol_c_r_ = cfg_dict.pop('vol/c/r')\n",
    "    else:\n",
    "        msg_ = f'vol/c/dstr=\"{vol_c_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    vol_r_dstr = cfg_dict.pop('vol/r/dstr', 'fixed')\n",
    "    if vol_r_dstr == 'fixed':\n",
    "        vol_r_ = cfg_dict.pop('vol/r')\n",
    "    elif vol_r_dstr in ('uniform', 'unifdpow'):\n",
    "        vol_r_low_ = cfg_dict.pop('vol/r/low')\n",
    "        vol_r_high_ = cfg_dict.pop('vol/r/high')\n",
    "    else:\n",
    "        msg_ = f'vol/r/dstr=\"{vol_r_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    vol_nv_dstr = cfg_dict.pop('vol/nv/dstr', 'fixed')\n",
    "    if vol_nv_dstr == 'fixed':\n",
    "        vol_nv_ = cfg_dict.pop('vol/nv')\n",
    "    elif vol_nv_dstr in ('uball',):\n",
    "        pass\n",
    "    else:\n",
    "        msg_ = f'vol/nv/dstr=\"{vol_nv_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    #########################################################\n",
    "    ############## Initial Condition Options ###############\n",
    "    #########################################################\n",
    "    ic_dstr = cfg_dict.pop('ic/dstr', None)\n",
    "    if ic_dstr in ('sphere', 'trnvol'):\n",
    "        w_ic = cfg_dict.pop('ic/w')\n",
    "        ic_bpp = cfg_dict.pop('ic/bpp')\n",
    "        ic_n = cfg_dict.pop('ic/n')\n",
    "        ic_frq = cfg_dict.pop('ic/frq')\n",
    "        ic_bs = cfg_dict.pop('ic/bs')\n",
    "        ic_needsampling = True\n",
    "    elif ic_dstr in ('trnsrf',):\n",
    "        w_ic = cfg_dict.pop('ic/w')\n",
    "        ic_bpp = cfg_dict.pop('ic/bpp')\n",
    "        ic_n = n_srf * n_srfpts_mdl\n",
    "        ic_frq = cfg_dict.pop('ic/frq')\n",
    "        ic_bs = ic_n\n",
    "        ic_needsampling = False\n",
    "    elif ic_dstr is None:\n",
    "        ic_needsampling = False\n",
    "        ic_frq = 1\n",
    "        w_ic, ic_bpp = 0, 'all'\n",
    "    else:\n",
    "        msg_ = f'ic/dstr={ic_dstr} not defined'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    if ic_dstr == 'sphere':\n",
    "        ic_c_dstr = cfg_dict.pop('ic/c/dstr', 'fixed')\n",
    "        if ic_c_dstr == 'fixed':\n",
    "            ic_c_ = cfg_dict.pop('ic/c')\n",
    "        elif ic_c_dstr == 'uniform':\n",
    "            ic_c_low_ = cfg_dict.pop('ic/c/low')\n",
    "            ic_c_high_ = cfg_dict.pop('ic/c/high')\n",
    "        elif ic_c_dstr == 'normal':\n",
    "            ic_c_loc_ = cfg_dict.pop('ic/c/loc')\n",
    "            ic_c_scale_ = cfg_dict.pop('ic/c/scale')\n",
    "        else:\n",
    "            msg_ = f'ic/c/dstr=\"{ic_c_dstr}\" not defined!'\n",
    "            raise ValueError(msg_)\n",
    "\n",
    "        ic_r_dstr = cfg_dict.pop('ic/r/dstr', 'fixed')\n",
    "        if ic_r_dstr == 'fixed':\n",
    "            ic_r_ = cfg_dict.pop('ic/r')\n",
    "        elif ic_r_dstr in ('uniform', 'unifdpow'):\n",
    "            ic_r_low_ = cfg_dict.pop('ic/r/low')\n",
    "            ic_r_high_ = cfg_dict.pop('ic/r/high')\n",
    "        else:\n",
    "            msg_ = f'ic/r/dstr=\"{ic_r_dstr}\" not defined!'\n",
    "            raise ValueError(msg_)\n",
    "    elif ic_dstr in ('trnsrf', 'trnvol', None):\n",
    "        pass\n",
    "    else:\n",
    "        msg_ = f'ic/dstr=\"{ic_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "\n",
    "    #########################################################\n",
    "    ########### Evaluation Point Sampling Options ###########\n",
    "    #########################################################\n",
    "    eid_list_dup = [opt.split('/')[1] for opt in cfg_dict\n",
    "                    if opt.startswith('eval/')]\n",
    "    eid_list = list(odict.fromkeys(eid_list_dup))\n",
    "    evalcfgs = odict()\n",
    "    for eid in eid_list:\n",
    "        evalcfgs[eid] = odict()\n",
    "        cfgopts = list(cfg_dict.keys())\n",
    "        for opt in cfgopts:\n",
    "            prfx = f'eval/{eid}/'\n",
    "            if opt.startswith(prfx):\n",
    "                optn = opt[len(prfx):]\n",
    "                optv = cfg_dict.pop(opt)\n",
    "                evalcfgs[eid][optn] = optv\n",
    "\n",
    "    #########################################################\n",
    "    ################# I/O Logistics Options #################\n",
    "    #########################################################\n",
    "    config_id = cfg_dict.pop('io/config_id')\n",
    "    results_dir = cfg_dict.pop('io/results_dir')\n",
    "    storage_dir = cfg_dict.pop('io/storage_dir', None)\n",
    "    io_avgfrq = cfg_dict.pop('io/avg/frq')\n",
    "    ioflsh_period = cfg_dict.pop('io/flush/frq')\n",
    "    chkpnt_period = cfg_dict.pop('io/ckpt/frq')\n",
    "    device_name = cfg_dict.pop('io/tch/device')\n",
    "    dtype_name = cfg_dict.pop('io/tch/dtype')\n",
    "    iomon_period = cfg_dict.pop('io/mon/frq')\n",
    "    io_cmprssnlvl = cfg_dict.pop('io/cmprssn_lvl')\n",
    "    eval_bs = cfg_dict.pop('io/eval/bs', None)\n",
    "\n",
    "    dtnow = datetime.datetime.now().isoformat(timespec='seconds')\n",
    "    hostname = socket.gethostname()\n",
    "    commit_hash = get_git_commit()\n",
    "    cfg_tree = '/'.join(config_id.split('/')[:-1])\n",
    "    cfg_name = config_id.split('/')[-1]\n",
    "    #########################################################\n",
    "    ##################### Sanity Checks #####################\n",
    "    #########################################################\n",
    "\n",
    "    # Making sure the specified option distributions are implemented.\n",
    "    fixed_opts = ['desc', 'date', 'rng_seed/list', 'problem',\n",
    "                  'dim', 'vol/n',  'srfpts/n/mdl', 'srfpts/n/trg', \n",
    "                  'srfpts/detspc', 'srfpts/dblsmpl','trg/btstrp', \n",
    "                  'trg/w', 'trg/tau', 'opt/lr', 'opt/epoch']\n",
    "\n",
    "    opt2availdstrs = {**{opt: ('fixed',) for opt in fixed_opts},\n",
    "        'wire': ('dmm',), 'wire/n': ('fixed',), 'wire/w': ('fixed',), \n",
    "        'wire/src': ('fixed', 'uniform', 'normal', 'ball'),\n",
    "        'wire/snk': ('fixed', 'uniform', 'normal', 'ball'),\n",
    "        'vol': ('disk',), 'vol/c': ('uniform', 'ball', 'normal'), \n",
    "        'vol/r': ('uniform', 'unifdpow'), 'vol/nv': ('fixed', 'uball'), \n",
    "        'ic': ('sphere', 'trnsrf', 'trnvol', 'fixed'), \n",
    "        'ic/c': ('fixed',), 'nn': ('mlp',), 'ic/r': ('fixed',), \n",
    "        'vol/c/low': ('fixed',), 'vol/c/high': ('fixed',),\n",
    "        'vol/c/loc': ('fixed',), 'vol/c/scale': ('fixed',),\n",
    "        'vol/c/c': ('fixed',),   'vol/c/r': ('fixed',),\n",
    "        'vol/r/low': ('fixed',), 'vol/r/high': ('fixed',),\n",
    "        **{f'eval/{eid}': ('uniform', 'grid', 'ball', 'trnvol')\n",
    "            for eid in eid_list}}\n",
    "\n",
    "    for opt, avail_dstrs in opt2availdstrs.items():\n",
    "        opt_dstr = cfg_dict_input.get(f'{opt}/dstr', 'fixed')\n",
    "        msg_  = f'\"{opt}\" cannot follow the \"{opt_dstr}\" distribution or type '\n",
    "        msg_ += f' since it is not implemented or available at least yet. The '\n",
    "        msg_ += f'only available options for \"{opt}\" are {avail_dstrs}.'\n",
    "        assert opt_dstr in avail_dstrs, msg_\n",
    "\n",
    "    # Making sure no other options are left unused.\n",
    "    if len(cfg_dict) > 0:\n",
    "        msg_ = f'The following settings were left unused:\\n'\n",
    "        for key, val in cfg_dict.items():\n",
    "            msg_ += f'  {key}: {val}'\n",
    "        raise RuntimeError(msg_)\n",
    "\n",
    "    # Making sure that all \"*_dstr\" options are valid\n",
    "    dstr2args = {'uniform':  ('/low', '/high'),\n",
    "                 'unifdpow': ('/low', '/high'),\n",
    "                 'normal':   ('/loc', '/scale'),\n",
    "                 'dmm':      ('/n', '/w', '/src', '/snk'),\n",
    "                 'disk':     ('/c', '/r', '/nv'),\n",
    "                 'ball':     ('/c', '/r'),\n",
    "                 'sphere':   ('/c', '/r'),\n",
    "                 'fixed':    ('',),\n",
    "                 'uball':    (),\n",
    "                 'trnvol':   (),\n",
    "                 'trnsrf':   ()}\n",
    "\n",
    "    key2req = {'vol': ('/n',)}\n",
    "    if ic_dstr is not None:\n",
    "        key2req['ic'] = (*key2req['ic'], '/w')\n",
    "    if ic_needsampling: \n",
    "        key2req['ic'] = (*key2req['ic'], '/n', '/frq')\n",
    "\n",
    "    for key in ['wire', 'vol']:\n",
    "        if key in key2req:\n",
    "            chck_dstrargs(key, cfg_dict_input, dstr2args, key2req)\n",
    "\n",
    "    edstr2args = {'uniform': ('/low', '/high', '/n', '/frq'),\n",
    "                  'grid':    ('/low', '/high', '/n', '/frq'),\n",
    "                  'ball':    ('/c', '/r', '/n', '/frq'),\n",
    "                  'fixed':   ('', '/n', '/frq'),\n",
    "                  'trnvol':  ('/n', '/frq')}\n",
    "    for eid in eid_list:\n",
    "        chck_dstrargs(f'eval/{eid}', cfg_dict_input, \n",
    "            edstr2args, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3d99d",
   "metadata": {},
   "source": [
    "## Problem Objects Construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbb4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Derived options and assertions\n",
    "    n_points = n_srfpts_mdl + n_srfpts_trg\n",
    "\n",
    "    assert not (do_dblsampling) or (n_srfpts_trg > 1)\n",
    "    if w_trg is None:\n",
    "        w_trg = n_srfpts_trg / n_points\n",
    "    assert not (n_srfpts_mdl == 0) or (w_trg == 1.0)\n",
    "    n_rsdls = 2 if do_dblsampling else 1\n",
    "\n",
    "    if eval_bs is None:\n",
    "        eval_bs = max(n_srfpts_mdl, n_srfpts_trg) * n_srf\n",
    "        \n",
    "    #########################################################\n",
    "    ########### I/O-Related Options and Operations ##########\n",
    "    #########################################################\n",
    "\n",
    "    name2dtype = dict(float64=torch.double,\n",
    "                      float32=torch.float32,\n",
    "                      float16=torch.float16)\n",
    "    tch_device = torch.device(device_name)\n",
    "    tch_dtype = name2dtype[dtype_name]\n",
    "\n",
    "    tch_dvcmdl = device_name\n",
    "    if device_name.startswith('cuda'):\n",
    "        tch_dvcmdl = torch.cuda.get_device_name(tch_device)\n",
    "\n",
    "    # Reserving 15.596 GB of memory for later usage\n",
    "    # t_gpumem = torch.cuda.get_device_properties(tch_device).total_memory\n",
    "    # tdt_elsize = torch.tensor(1).to(tch_device, tch_dtype).element_size()\n",
    "    # nuslss = int((0.90 * t_gpumem) / tdt_elsize)\n",
    "    # useless_tensor = torch.empty((nuslss,), device=tch_device, dtype=tch_dtype)\n",
    "    # del useless_tensor\n",
    "    \n",
    "    msg_ = f'\"io/mon/frq\" % \"io/avg/frq\" != 0'\n",
    "    assert iomon_period % io_avgfrq == 0, msg_\n",
    "    msg_ = f'\"io/ckpt/frq\" % \"io/avg/frq\" != 0'\n",
    "    assert chkpnt_period % io_avgfrq == 0, msg_\n",
    "    \n",
    "    do_logtb = storage_dir is not None\n",
    "    do_profile = False\n",
    "    do_tchsave = storage_dir is not None\n",
    "    \n",
    "    assert not(do_logtb) or (storage_dir is not None)\n",
    "    assert not(do_profile) or (storage_dir is not None)\n",
    "    assert not(do_tchsave) or (storage_dir is not None)\n",
    "\n",
    "    #########################################################\n",
    "    ########### Constructing the Batch RNG Object ###########\n",
    "    #########################################################\n",
    "    n_seeds = len(rng_seed_list)\n",
    "    rng_seeds = np.array(rng_seed_list)\n",
    "    rng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "                   device=tch_device, dtype=tch_dtype,\n",
    "                   unif_cache_cols=1_000_000,\n",
    "                   norm_cache_cols=5_000_000)\n",
    "    rng.seed(np.broadcast_to(rng_seeds, rng.shape))\n",
    "\n",
    "    #########################################################\n",
    "    ########## Defining the Maxwell Problem Object ##########\n",
    "    #########################################################\n",
    "    assert prob_type == 'maxwell'\n",
    "\n",
    "    msg_ = f'wire_dstr = {wire_dstr} is not available/implemented.'\n",
    "    assert wire_dstr in ('dmm',), msg_\n",
    "\n",
    "    trns_opts = dict(dim=dim, wire_n=wire_n, sqrt=np.sqrt)\n",
    "\n",
    "    # The wire currents\n",
    "    if wire_w_dstr == 'fixed':\n",
    "        wire_w_0 = get_arr('wire_w', '(wire_n,)', \n",
    "            {**trns_opts, 'wire_w': wire_w_})\n",
    "        wire_w = np.broadcast_to(wire_w_0[None, ...],\n",
    "                                 (n_seeds, wire_n)).copy()\n",
    "        assert wire_w.shape == (n_seeds, wire_n)\n",
    "    else:\n",
    "        raise ValueError(f'wire_w_dstr={wire_w_dstr} '\n",
    "                         'not implemented.')\n",
    "\n",
    "    # The Maxwell wire current source points\n",
    "    if wire_src_dstr == 'fixed':\n",
    "        wire_src_0 = get_arr('wire_src', '(wire_n, dim)', \n",
    "            {**trns_opts, 'wire_src': wire_src_})\n",
    "        wire_src = np.broadcast_to(wire_src_0[None, ...],\n",
    "                                  (n_seeds, wire_n, dim)).copy()\n",
    "        assert wire_src.shape == (n_seeds, wire_n, dim)\n",
    "    elif wire_src_dstr == 'uniform':\n",
    "        wire_src_low_0 =  get_arr('wire_src_low', '(wire_n, dim)', \n",
    "            {**trns_opts, 'wire_src_low': wire_src_low_})\n",
    "        wire_src_low = np.broadcast_to(wire_src_low_0[None, ...],\n",
    "            (n_seeds, wire_n, dim)).copy()\n",
    "        assert wire_src_low.shape == (n_seeds, wire_n, dim)\n",
    "        \n",
    "        wire_src_high_0 =  get_arr('wire_src_high', '(wire_n, dim)', \n",
    "            {**trns_opts, 'wire_src_high': wire_src_high_})\n",
    "        wire_src_high = np.broadcast_to(wire_src_high_0[None, ...],\n",
    "            (n_seeds, wire_n, dim)).copy()\n",
    "        assert wire_src_high.shape == (n_seeds, wire_n, dim)\n",
    "        \n",
    "        rnds = rng.uniform((n_seeds, wire_n, dim)).detach().cpu().numpy()\n",
    "        wire_src = wire_src_low + rnds * (wire_src_high - wire_src_low)\n",
    "        assert wire_src.shape == (n_seeds, wire_n, dim)\n",
    "    elif wire_src_dstr == 'normal':\n",
    "        wire_src_loc_0 =  get_arr('wire_src_loc', '(wire_n, dim)', \n",
    "            {**trns_opts, 'wire_src_loc': wire_src_loc_})\n",
    "        wire_src_loc = np.broadcast_to(wire_src_loc_0[None, ...],\n",
    "            (n_seeds, wire_n, dim)).copy()\n",
    "        assert wire_src_loc.shape == (n_seeds, wire_n, dim)\n",
    "        \n",
    "        wire_src_scale_0 =  get_arr('wire_src_scale', '(wire_n,)', \n",
    "            {**trns_opts, 'wire_src_scale': wire_src_scale_})\n",
    "        wire_src_scale = np.broadcast_to(wire_src_scale_0[None, ...],\n",
    "            (n_seeds, wire_n)).copy()\n",
    "        assert wire_src_scale.shape == (n_seeds, wire_n)\n",
    "        \n",
    "        rnds = rng.normal((n_seeds, wire_n, dim)).detach().cpu().numpy()\n",
    "        wire_src = wire_src_loc + rnds * wire_src_scale.reshape(n_seeds, wire_n, 1)\n",
    "        assert wire_src.shape == (n_seeds, wire_n, dim)\n",
    "    elif wire_src_dstr == 'ball':\n",
    "        wire_src_c_0 =  get_arr('wire_src_c', '(wire_n, dim)', \n",
    "            {**trns_opts, 'wire_src_c': wire_src_c_})\n",
    "        wire_src_c = np.broadcast_to(wire_src_c_0[None, ...],\n",
    "            (n_seeds, wire_n, dim)).copy()\n",
    "        assert wire_src_c.shape == (n_seeds, wire_n, dim)\n",
    "        \n",
    "        wire_src_r_0 =  get_arr('wire_src_r', '(wire_n,)', \n",
    "            {**trns_opts, 'wire_src_r': wire_src_r_})\n",
    "        wire_src_r = np.broadcast_to(wire_src_r_0[None, ...],\n",
    "            (n_seeds, wire_n)).copy()\n",
    "        assert wire_src_r.shape == (n_seeds, wire_n)\n",
    "        \n",
    "        rnds1 = rng.normal((n_seeds, wire_n, dim)).detach().cpu().numpy()\n",
    "        rnds1_tilde = rnds1 / np.sqrt((rnds1**2).sum(axis=-1, keepdims=True))\n",
    "        assert rnds1_tilde.shape == (n_seeds, wire_n, dim)\n",
    "        \n",
    "        rnds2 = rng.uniform((n_seeds, wire_n)).detach().cpu().numpy()\n",
    "        rnds2_tilde = rnds2 ** (1.0 / dim)\n",
    "        assert rnds2_tilde.shape == (n_seeds, wire_n)\n",
    "        \n",
    "        rnds3_tilde = (wire_src_r * rnds2_tilde).reshape(n_seeds, wire_n, 1)\n",
    "        assert rnds3_tilde.shape == (n_seeds, wire_n, 1)\n",
    "        \n",
    "        wire_src = wire_src_c + rnds1_tilde * rnds3_tilde\n",
    "        assert wire_src.shape == (n_seeds, wire_n, dim)\n",
    "    else:\n",
    "        raise ValueError(f'wire_src_dstr={wire_src_dstr} '\n",
    "                         'not implemented.')\n",
    "\n",
    "    # The Maxwell wire current sink points\n",
    "    # Note: The following is a copy of the above current source definition\n",
    "    #       with the `_src` prefixes being replaced with `_snk`  \n",
    "    if wire_snk_dstr == 'fixed':\n",
    "        wire_snk_0 = get_arr('wire_snk', '(wire_n, dim)', \n",
    "            {**trns_opts, 'wire_snk': wire_snk_})\n",
    "        wire_snk = np.broadcast_to(wire_snk_0[None, ...],\n",
    "                                  (n_seeds, wire_n, dim)).copy()\n",
    "        assert wire_snk.shape == (n_seeds, wire_n, dim)\n",
    "    elif wire_snk_dstr == 'uniform':\n",
    "        wire_snk_low_0 =  get_arr('wire_snk_low', '(wire_n, dim)', \n",
    "            {**trns_opts, 'wire_snk_low': wire_snk_low_})\n",
    "        wire_snk_low = np.broadcast_to(wire_snk_low_0[None, ...],\n",
    "            (n_seeds, wire_n, dim)).copy()\n",
    "        assert wire_snk_low.shape == (n_seeds, wire_n, dim)\n",
    "        \n",
    "        wire_snk_high_0 =  get_arr('wire_snk_high', '(wire_n, dim)', \n",
    "            {**trns_opts, 'wire_snk_high': wire_snk_high_})\n",
    "        wire_snk_high = np.broadcast_to(wire_snk_high_0[None, ...],\n",
    "            (n_seeds, wire_n, dim)).copy()\n",
    "        assert wire_snk_high.shape == (n_seeds, wire_n, dim)\n",
    "        \n",
    "        rnds = rng.uniform((n_seeds, wire_n, dim)).detach().cpu().numpy()\n",
    "        wire_snk = wire_snk_low + rnds * (wire_snk_high - wire_snk_low)\n",
    "        assert wire_snk.shape == (n_seeds, wire_n, dim)\n",
    "    elif wire_snk_dstr == 'normal':\n",
    "        wire_snk_loc_0 =  get_arr('wire_snk_loc', '(wire_n, dim)', \n",
    "            {**trns_opts, 'wire_snk_loc': wire_snk_loc_})\n",
    "        wire_snk_loc = np.broadcast_to(wire_snk_loc_0[None, ...],\n",
    "            (n_seeds, wire_n, dim)).copy()\n",
    "        assert wire_snk_loc.shape == (n_seeds, wire_n, dim)\n",
    "        \n",
    "        wire_snk_scale_0 =  get_arr('wire_snk_scale', '(wire_n,)', \n",
    "            {**trns_opts, 'wire_snk_scale': wire_snk_scale_})\n",
    "        wire_snk_scale = np.broadcast_to(wire_snk_scale_0[None, ...],\n",
    "            (n_seeds, wire_n)).copy()\n",
    "        assert wire_snk_scale.shape == (n_seeds, wire_n)\n",
    "        \n",
    "        rnds = rng.normal((n_seeds, wire_n, dim)).detach().cpu().numpy()\n",
    "        wire_snk = wire_snk_loc + rnds * wire_snk_scale.reshape(n_seeds, wire_n, 1)\n",
    "        assert wire_snk.shape == (n_seeds, wire_n, dim)\n",
    "    elif wire_snk_dstr == 'ball':\n",
    "        wire_snk_c_0 =  get_arr('wire_snk_c', '(wire_n, dim)', \n",
    "            {**trns_opts, 'wire_snk_c': wire_snk_c_})\n",
    "        wire_snk_c = np.broadcast_to(wire_snk_c_0[None, ...],\n",
    "            (n_seeds, wire_n, dim)).copy()\n",
    "        assert wire_snk_c.shape == (n_seeds, wire_n, dim)\n",
    "        \n",
    "        wire_snk_r_0 =  get_arr('wire_snk_r', '(wire_n,)', \n",
    "            {**trns_opts, 'wire_snk_r': wire_snk_r_})\n",
    "        wire_snk_r = np.broadcast_to(wire_snk_r_0[None, ...],\n",
    "            (n_seeds, wire_n)).copy()\n",
    "        assert wire_snk_r.shape == (n_seeds, wire_n)\n",
    "        \n",
    "        rnds4 = rng.normal((n_seeds, wire_n, dim)).detach().cpu().numpy()\n",
    "        rnds4_tilde = rnds4 / np.sqrt((rnds4**2).sum(axis=-1, keepdims=True))\n",
    "        assert rnds4_tilde.shape == (n_seeds, wire_n, dim)\n",
    "        \n",
    "        rnds5 = rng.uniform((n_seeds, wire_n)).detach().cpu().numpy()\n",
    "        rnds5_tilde = rnds5 ** (1.0 / dim)\n",
    "        assert rnds5_tilde.shape == (n_seeds, wire_n)\n",
    "        \n",
    "        rnds6_tilde = (wire_snk_r * rnds5_tilde).reshape(n_seeds, wire_n, 1)\n",
    "        assert rnds6_tilde.shape == (n_seeds, wire_n, 1)\n",
    "        \n",
    "        wire_snk = wire_snk_c + rnds1_tilde * rnds6_tilde\n",
    "        assert wire_snk.shape == (n_seeds, wire_n, dim)\n",
    "    else:\n",
    "        raise ValueError(f'wire_src_dstr={wire_src_dstr} '\n",
    "                         'not implemented.')\n",
    "        \n",
    "\n",
    "    # Defining the problem object\n",
    "    problem = DeltaLineProblem(source=wire_src, sink=wire_snk, current=wire_w,\n",
    "        tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "    \n",
    "    #########################################################\n",
    "    ######### Defining the Latent Parameter Search ##########\n",
    "    #########################################################\n",
    "    if srch_dstr in ('mcmc',):\n",
    "        true_problem = problem\n",
    "        \n",
    "        mcmc_enc = MCMCWireEncoder(kind=srch_enc, source=wire_src, sink=wire_snk, current=wire_w)\n",
    "        assert srch_prior_dstr == 'normal'\n",
    "        \n",
    "        # The search dimension\n",
    "        sdim = mcmc_enc.sdim\n",
    "        trns_opts['sdim'] = sdim\n",
    "        \n",
    "        srch_prior_loc_0 =  get_arr('srch_prior_loc', '(sdim,)', \n",
    "            {**trns_opts, 'srch_prior_loc': srch_prior_loc_})\n",
    "        srch_prior_loc = np.broadcast_to(srch_prior_loc_0[None, ...],\n",
    "            (n_seeds, sdim)).copy()\n",
    "        assert srch_prior_loc.shape == (n_seeds, sdim)\n",
    "        \n",
    "        srch_prior_scale_0 =  get_arr('srch_prior_scale', '(sdim,)', \n",
    "            {**trns_opts, 'srch_prior_scale': srch_prior_scale_})\n",
    "        srch_prior_scale = np.broadcast_to(srch_prior_scale_0[None, ...],\n",
    "            (n_seeds, sdim)).copy()\n",
    "        assert srch_prior_scale.shape == (n_seeds, sdim)\n",
    "        \n",
    "        srch_prior_loc_tch = torch.from_numpy(srch_prior_loc)\n",
    "        srch_prior_loc_tch = srch_prior_loc_tch.to(device=tch_device, dtype=tch_dtype)\n",
    "        assert srch_prior_loc_tch.shape == (n_seeds, sdim)\n",
    "        \n",
    "        srch_prior_scale_tch = torch.from_numpy(srch_prior_scale)\n",
    "        srch_prior_scale_tch = srch_prior_scale_tch.to(device=tch_device, dtype=tch_dtype)\n",
    "        assert srch_prior_scale_tch.shape == (n_seeds, sdim)\n",
    "        \n",
    "        rnds = rng.normal((n_seeds, sdim))\n",
    "        old_srch_mu = srch_prior_loc_tch + rnds * srch_prior_scale_tch\n",
    "        assert old_srch_mu.shape == (n_seeds, sdim)\n",
    "        \n",
    "        old_srch_mu_np = old_srch_mu.detach().cpu().numpy()\n",
    "        assert old_srch_mu_np.shape == (n_seeds, sdim)\n",
    "        \n",
    "        old_srch_params = mcmc_enc(old_srch_mu_np)\n",
    "        \n",
    "        # Re-creating the problem instance with randomized initial parameters for search\n",
    "        problem = DeltaLineProblem(source=old_srch_params['source'], \n",
    "            sink=old_srch_params['sink'], current=old_srch_params['current'],\n",
    "            tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "\n",
    "        old_logpri, old_loglike, old_logpost = None, None, None\n",
    "        \n",
    "        # Total number of MCMC search draws\n",
    "        n_srchdraws = (n_epochs - srch_inittrn) // srch_frq\n",
    "    elif srch_dstr is None:\n",
    "        pass\n",
    "    else:\n",
    "        msg_ = f'srch/dstr=\"{srch_dstr}\" not defined!'\n",
    "        raise ValueError(msg_)\n",
    "    \n",
    "    if srch_dstr is not None:\n",
    "        n_cpg = n_seeds // n_grps\n",
    "        assert n_seeds == (n_grps * n_cpg)\n",
    "        \n",
    "        assert n_cpg % srch_reset_k == 0\n",
    "        \n",
    "        assert obs_x_dstr == 'uniform'\n",
    "        \n",
    "        obs_x_low_0 =  get_arr('obs_x_low', '(obs_n, dim)', \n",
    "            {**trns_opts, 'obs_x_low': obs_x_low_})\n",
    "        obs_x_low = np.broadcast_to(obs_x_low_0[None, ...],\n",
    "            (n_seeds, obs_n, dim)).copy()\n",
    "        assert obs_x_low.shape == (n_seeds, obs_n, dim)\n",
    "        \n",
    "        obs_x_high_0 =  get_arr('obs_x_high', '(obs_n, dim)', \n",
    "            {**trns_opts, 'obs_x_high': obs_x_high_})\n",
    "        obs_x_high = np.broadcast_to(obs_x_high_0[None, ...],\n",
    "            (n_seeds, obs_n, dim)).copy()\n",
    "        assert obs_x_high.shape == (n_seeds, obs_n, dim)\n",
    "        \n",
    "        rnds = rng.uniform((n_seeds, obs_n, dim))\n",
    "        \n",
    "        # Creating the x observations, and making sure that each \n",
    "        # group gets the same set of observations.\n",
    "        obs_x_low_tch = torch.from_numpy(obs_x_low).to(device=tch_device, dtype=tch_dtype)\n",
    "        obs_x_high_tch = torch.from_numpy(obs_x_high).to(device=tch_device, dtype=tch_dtype)\n",
    "        \n",
    "        obs_x1 = obs_x_low_tch + rnds * (obs_x_high_tch - obs_x_low_tch)\n",
    "        assert obs_x1.shape == (n_seeds, obs_n, dim)\n",
    "        obs_x2 = obs_x1.reshape(n_grps, n_cpg, obs_n, dim)\n",
    "        assert obs_x2.shape == (n_grps, n_cpg, obs_n, dim)\n",
    "        obs_x3 = obs_x2[:, :1, ...]\n",
    "        assert obs_x3.shape == (n_grps, 1, obs_n, dim)\n",
    "        obs_x4 = obs_x3.expand(n_grps, n_cpg, obs_n, dim)\n",
    "        assert obs_x4.shape == (n_grps, n_cpg, obs_n, dim)\n",
    "        obs_x = obs_x4.reshape(n_seeds, obs_n, dim)\n",
    "        assert obs_x.shape == (n_seeds, obs_n, dim)\n",
    "        \n",
    "        if obs_y_dstr == 'potential':\n",
    "            obs_ydim = dim\n",
    "            obs_y = true_problem.potential(obs_x)\n",
    "            assert obs_y.shape == (n_seeds, obs_n, obs_ydim)\n",
    "            obs_y = obs_y - obs_y.mean(dim=-2, keepdim=True)\n",
    "            assert obs_y.shape == (n_seeds, obs_n, obs_ydim)\n",
    "        elif obs_y_dstr == 'field':\n",
    "            obs_ydim = dim\n",
    "            obs_y = true_problem.field(obs_x)\n",
    "            assert obs_y.shape == (n_seeds, obs_n, obs_ydim)\n",
    "        else:\n",
    "            raise ValueError(f'obs_y_dstr={obs_y_dstr} not implemented.')\n",
    "\n",
    "    #########################################################\n",
    "    ####### Defining the Initial Condition Parameters #######\n",
    "    #########################################################\n",
    "    msg_ = f'ic/dstr = {ic_dstr} is not available/implemented.'\n",
    "    assert ic_dstr in ('sphere', 'trnsrf', None), msg_\n",
    "    msg_ = '\"ic/bpp\" must be either \"bias\" or \"all\".'\n",
    "    assert ic_bpp in ('bias', 'all', None), msg_\n",
    "\n",
    "    if ic_dstr == 'sphere':\n",
    "        if ic_c_dstr == 'fixed':\n",
    "            ic_c_0_np = get_arr('ic_c', '(dim,)', \n",
    "                {**trns_opts, 'ic_c': ic_c_})\n",
    "            ic_c_np = np.broadcast_to(ic_c_0_np[None, ...], \n",
    "                (n_seeds, dim)).copy()\n",
    "            assert ic_c_np.shape == (n_seeds, dim)\n",
    "        else:\n",
    "            raise ValueError(f'ic_c_dstr={ic_c_dstr} '\n",
    "                            'not implemented.')\n",
    "\n",
    "        if ic_r_dstr == 'fixed':\n",
    "            ic_r_0_np = get_arr('ic_r', '()', \n",
    "                {**trns_opts, 'ic_r': ic_r_})\n",
    "            ic_r_np = np.broadcast_to(ic_r_0_np[None, ...], \n",
    "                (n_seeds,)).copy()\n",
    "            assert ic_r_np.shape == (n_seeds,)\n",
    "        else:\n",
    "            raise ValueError(f'ic_r_dstr={ic_r_dstr} '\n",
    "                            'not implemented.')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ic_c = torch.from_numpy(ic_c_np).to(device=tch_device, \n",
    "                dtype=tch_dtype).reshape(n_seeds, 1, dim).expand(n_seeds, ic_n, dim)\n",
    "            assert ic_c.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "            ic_r = torch.from_numpy(ic_r_np).to(device=tch_device, \n",
    "                dtype=tch_dtype).reshape(n_seeds, 1, 1).expand(n_seeds, ic_n, 1)\n",
    "            assert ic_r.shape == (n_seeds, ic_n, 1)\n",
    "    elif ic_dstr in ('trnsrf', 'trnvol', None):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'ic_dstr={ic_dstr} not implemented')\n",
    "\n",
    "    #########################################################\n",
    "    ########## Defining the Volume Sampling Object ##########\n",
    "    #########################################################\n",
    "    msg_ = f'vol/dstr = {vol_dstr} is not available/implemented.'\n",
    "    assert vol_dstr in ('disk',), msg_\n",
    "\n",
    "    if vol_c_dstr == 'uniform':\n",
    "        vol_c_low_0 = get_arr('vol_c_low', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_low': vol_c_low_})\n",
    "        vol_c_low = np.broadcast_to(vol_c_low_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_low.shape == (n_seeds, dim)\n",
    "\n",
    "        vol_c_high_0 = get_arr('vol_c_high', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_high': vol_c_high_})\n",
    "        vol_c_high = np.broadcast_to(vol_c_high_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_high.shape == (n_seeds, dim)\n",
    "        \n",
    "        vol_c_params = dict(low=vol_c_low, high=vol_c_high)\n",
    "    elif vol_c_dstr == 'normal':\n",
    "        vol_c_loc_0 = get_arr('vol_c_loc', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_loc': vol_c_loc_})\n",
    "        vol_c_loc = np.broadcast_to(vol_c_loc_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_loc.shape == (n_seeds, dim)\n",
    "\n",
    "        vol_c_scale_0 = get_arr('vol_c_scale', '()', \n",
    "            {**trns_opts, 'vol_c_scale': vol_c_scale_})\n",
    "        vol_c_scale = np.broadcast_to(vol_c_scale_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_c_scale.shape == (n_seeds,)\n",
    "        \n",
    "        vol_c_params = dict(loc=vol_c_loc, scale=vol_c_scale)\n",
    "    elif vol_c_dstr == 'ball':\n",
    "        vol_c_c_0 = get_arr('vol_c_c', '(dim,)', \n",
    "            {**trns_opts, 'vol_c_c': vol_c_c_})\n",
    "        vol_c_c = np.broadcast_to(vol_c_c_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_c_c.shape == (n_seeds, dim)\n",
    "\n",
    "        vol_c_r_0 = get_arr('vol_c_r', '()', \n",
    "            {**trns_opts, 'vol_c_r': vol_c_r_})\n",
    "        vol_c_r = np.broadcast_to(vol_c_r_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_c_r.shape == (n_seeds,)\n",
    "        \n",
    "        vol_c_params = dict(c=vol_c_c, r=vol_c_r)\n",
    "    else:\n",
    "        raise ValueError(f'vol_c_dstr={vol_c_dstr} not implemented.')\n",
    "\n",
    "    if vol_r_dstr in ('uniform', 'unifdpow'):\n",
    "        vol_r_low_0 = get_arr('vol_r_low', '()', \n",
    "            {**trns_opts, 'vol_r_low': vol_r_low_, \n",
    "             'vol_r_high': vol_r_high_})\n",
    "        vol_r_low = np.broadcast_to(vol_r_low_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_r_low.shape == (n_seeds,)\n",
    "\n",
    "        vol_r_high_0 = get_arr('vol_r_high', '()', \n",
    "            {**trns_opts, 'vol_r_low': vol_r_low_, \n",
    "             'vol_r_high': vol_r_high_})\n",
    "        vol_r_high = np.broadcast_to(vol_r_high_0[None, ...],\n",
    "            (n_seeds,)).copy()\n",
    "        assert vol_r_high.shape == (n_seeds,)\n",
    "        \n",
    "        vol_r_params = dict(low=vol_r_low, high=vol_r_high)\n",
    "    else:\n",
    "        raise ValueError(f'vol_r_dstr={vol_r_dstr} not implemented.')\n",
    "    \n",
    "    if vol_nv_dstr == 'uball':\n",
    "        vol_nv_params = dict()\n",
    "    elif vol_nv_dstr == 'fixed':\n",
    "        vol_nv_0 = get_arr('vol_nv', '(dim,)', \n",
    "            {**trns_opts, 'vol_nv': vol_nv_})\n",
    "        vol_nv = np.broadcast_to(vol_nv_0[None, ...],\n",
    "            (n_seeds, dim)).copy()\n",
    "        assert vol_nv.shape == (n_seeds, dim)\n",
    "        \n",
    "        vol_nv_params = dict(value=vol_nv)\n",
    "    else:\n",
    "        raise ValueError(f'vol_nv_dstr={vol_nv_dstr} not implemented.')\n",
    "        \n",
    "\n",
    "    volsampler = DiskSampler(c_dstr=vol_c_dstr, c_params=vol_c_params,\n",
    "                             r_dstr=vol_r_dstr, r_params=vol_r_params,\n",
    "                             n_dstr=vol_nv_dstr, n_params=vol_nv_params,\n",
    "                             batch_rng=rng)\n",
    "\n",
    "    srfsampler = SphereSampler(batch_rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80da6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    #### Evaluation Param Tensorization and Sanitization ####\n",
    "    #########################################################\n",
    "\n",
    "    # The following evaluates the 'eval/*' options and creates \n",
    "    # array parameters for evaluation.\n",
    "    # The input is mainly the `evalcfgs` dictionary, which holds \n",
    "    # some keys and list/string values. The output will be the \n",
    "    # `evalprms` dictionary which has the same keys but with \n",
    "    # np.array values.\n",
    "    # --------------\n",
    "    # Example input: \n",
    "    #   evalcfgs = {'ur': {'dstr': 'uniform', \n",
    "    #       'low': [0], \n",
    "    #       'high': '[sqrt(dim)]'}}\n",
    "    # --------------\n",
    "    # Example output\n",
    "    #   evalprms = {'ur': {'dstr': 'uniform',\n",
    "    #       'low' : torch.tensor([0]).expand(n_seeds, dim)\n",
    "    #       'high': torch.tensor(np.sqrt(dim)).expand(n_seeds, dim))}}\n",
    "\n",
    "    dstr2shapes = {'uniform': {'low':  '(dim,)', \n",
    "                               'high': '(dim,)'},\n",
    "                   'grid':    {'low':  '(dim,)', \n",
    "                               'high': '(dim,)'},\n",
    "                   'ball':    {'c':    '(dim,)', \n",
    "                               'r':    '()'    },\n",
    "                   'trnvol': {}}\n",
    "\n",
    "    assert all('dstr' in eopts for eopts in evalcfgs.values())\n",
    "    assert all('frq'  in eopts for eopts in evalcfgs.values())\n",
    "    assert all('n'    in eopts for eopts in evalcfgs.values())\n",
    "    evalprms = odict()\n",
    "    for eid, eopts_ in evalcfgs.items():\n",
    "        eopts = eopts_.copy()\n",
    "        eparam = odict()\n",
    "        for eopt in ('dstr', 'n', 'frq'):\n",
    "            eparam[eopt] = eopts.pop(eopt)\n",
    "\n",
    "        edstr = eparam['dstr']\n",
    "        msg_  = f'Unknown eval \"{eid}\" dstr -> \"{edstr}\". '\n",
    "        msg_ += f'dstr should be one of {dstr2shapes.keys()}.'\n",
    "        assert edstr in dstr2shapes, msg_\n",
    "\n",
    "        estore_dflt = (edstr == 'grid')\n",
    "        eparam['store'] = eopts.pop('store', estore_dflt)\n",
    "\n",
    "        opts2shape = dstr2shapes[edstr]\n",
    "        for eopt, eoptshpstr in opts2shape.items():\n",
    "            # Example: edstr = 'uniform'\n",
    "            #          eopt = 'low'\n",
    "            #          eoptshpstr = '(dim,)'\n",
    "            #          eoptshp = (dim,)\n",
    "            #          eoptval = \"[sqrt(dim)]\"\n",
    "            #          eopt_pnp0 = np.array([sqrt(dim)]*dim)\n",
    "            #          eopt_pnp0.shape = (dim,)\n",
    "            #          eopt_pnp = eopt_pnp0.expand(n_seeds, dim)\n",
    "            #          eopt_pnp.shape = (n_seeds, dim)\n",
    "            #          eopt_p = torch.from_numpy(eopt_pa0)\n",
    "            #          eopt_p.shape = (n_seeds, dim)\n",
    "            msg_  = f'\"eval/{eid}/{eopt}\" must be fixed and determined.'\n",
    "            msg_ += f' Hierarchical support not available yet.'\n",
    "            assert eopt in eopts, msg_\n",
    "\n",
    "            eoptval = eopts.pop(eopt)\n",
    "            etrns = {eopt: eoptval, 'dim': dim, \n",
    "                     'sqrt': np.sqrt}\n",
    "\n",
    "            eopt_pnp0 = get_arr(eopt, eoptshpstr, etrns)        \n",
    "            eoptshp = eval_formula(eoptshpstr, {'dim': dim})\n",
    "            eopt_pnp = np.broadcast_to(eopt_pnp0[None, ...],\n",
    "                                       (n_seeds, *eoptshp)).copy()\n",
    "            assert eopt_pnp.shape == (n_seeds, *eoptshp)\n",
    "            eopt_pc = torch.from_numpy(eopt_pnp)\n",
    "            eopt_p = eopt_pc.to(device=tch_device, dtype=tch_dtype)\n",
    "            assert eopt_p.shape == (n_seeds, *eoptshp)\n",
    "            eparam[eopt] = eopt_p\n",
    "\n",
    "        assert len(eopts) == 0, f'unused eval items left: {eopts}'\n",
    "        evalprms[eid] = eparam\n",
    "\n",
    "    #########################################################\n",
    "    ############# Evaluation Parameter Creation #############\n",
    "    #########################################################\n",
    "    for eid, eopts in evalprms.items():\n",
    "        edstr = eopts['dstr']\n",
    "        n_evlpnts = eopts['n']\n",
    "        if edstr == 'uniform':\n",
    "            e_low_ = eopts['low']\n",
    "            assert e_low_.shape == (n_seeds, dim)\n",
    "            e_low = e_low_.unsqueeze(dim=-2)\n",
    "            assert e_low.shape == (n_seeds, 1, dim)\n",
    "\n",
    "            e_high_ = eopts['high']\n",
    "            assert e_high_.shape == (n_seeds, dim)\n",
    "            e_high = e_high_.unsqueeze(dim=-2)\n",
    "            assert e_high.shape == (n_seeds, 1, dim)\n",
    "\n",
    "            e_slope = e_high - e_low\n",
    "            assert e_slope.shape == (n_seeds, 1, dim)\n",
    "\n",
    "            eopts['bias'] = e_low\n",
    "            eopts['slope'] = e_slope\n",
    "        elif edstr == 'ball':\n",
    "            e_c_ = eopts['c']\n",
    "            assert e_c_.shape == (n_seeds, dim)\n",
    "            e_c = e_c_.unsqueeze(dim=-2).expand(n_seeds, n_evlpnts, dim)\n",
    "            assert e_c.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "            e_r_ = eopts['r']\n",
    "            assert e_r_.shape == (n_seeds,)\n",
    "\n",
    "            e_r = e_r_.reshape(n_seeds, 1, 1).expand(n_seeds, n_evlpnts, 1)\n",
    "            assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "            eopts['c_xpnd'] = e_c\n",
    "            eopts['r_xpnd'] = e_r\n",
    "        elif edstr == 'trnvol':\n",
    "            pass\n",
    "        elif edstr == 'grid':\n",
    "            n_g_ = eopts['n']\n",
    "            if isinstance(n_g_, int):\n",
    "                n_gpd, n_g = [int(np.ceil(n_g_**(1./dim)))] * dim, n_g_\n",
    "                assert n_g == int(np.prod(n_gpd))\n",
    "            elif isinstance(n_g_, (list, tuple)):\n",
    "                n_gpd = n_g_\n",
    "                n_g = int(np.prod(n_gpd))\n",
    "            else:\n",
    "                raise ValueError(f'eopts[\\'n\\']={n_g} is not defined')\n",
    "\n",
    "            elowt = eopts['low']\n",
    "            assert elowt.shape == (n_seeds, dim)\n",
    "\n",
    "            ehight = eopts['high']\n",
    "            assert ehight.shape == (n_seeds, dim)\n",
    "\n",
    "            assert (elowt[:1] == elowt).all()\n",
    "            assert (ehight[:1] == ehight).all()\n",
    "\n",
    "            elow = elowt.cpu().detach().numpy()[0].tolist()\n",
    "            ehigh = ehight.cpu().detach().numpy()[0].tolist()\n",
    "\n",
    "            e_pntsplt = create_xg(elow, ehigh, n_gpd, tch_device, tch_dtype)\n",
    "            assert e_pntsplt.shape == (*n_gpd, dim)\n",
    "\n",
    "            e_pnts = e_pntsplt.reshape(1, n_g, dim).expand(n_seeds, n_g, dim)\n",
    "            assert e_pnts.shape == (n_seeds, n_g, dim)\n",
    "\n",
    "            eopts['pnts'] = e_pnts.to(tch_device, tch_dtype)\n",
    "            eopts['pnts_plt'] = e_pntsplt.to(tch_device, tch_dtype)\n",
    "            eopts['n_gpd'] = n_gpd\n",
    "            eopts['n'] = n_g\n",
    "        else:\n",
    "            raise ValueError(f'\"{edstr}\" not defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78fbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    #### Collecting the Config Columns in the Dataframe #####\n",
    "    #########################################################\n",
    "    # Identifying the hyper-parameter from etc config columns\n",
    "    hppats = ['problem', 'dim', 'vol/n', 'srfpts/n/mdl', \n",
    "        'srfpts/n/trg',  'srfpts/detspc', 'srfpts/dblsmpl',\n",
    "        'trg/w', 'trg/btstrp', 'trg/tau', 'trg/reg/w', 'opt/lr', \n",
    "        'opt/dstr',  'nn/*', 'wire/*', 'ic/*', 'vol/*', 'eval/*', 'srch/*']\n",
    "    etcpats = ['desc', 'date', 'opt/epoch', 'rng_seed/list', 'io/*']\n",
    "\n",
    "    hpopts = [x for pat in hppats for x in \n",
    "               fnmatch.filter(cfg_dict_input.keys(), pat)]\n",
    "    etcopts = [x for pat in etcpats for x in \n",
    "               fnmatch.filter(cfg_dict_input.keys(), pat)]\n",
    "\n",
    "    err_list = []\n",
    "    for opt in cfg_dict_input:\n",
    "        if (opt in hpopts) and (opt in etcopts):\n",
    "            msg_ = f'\"{opt}\" should both be treated as hp and etc!'\n",
    "            err_list.append(msg_)\n",
    "        if (opt not in hpopts) and (opt not in etcopts):\n",
    "            msg_ = f'\"{opt}\" is neither hp nor etc!'\n",
    "            err_list.append(msg_)\n",
    "    if len(err_list) > 0:\n",
    "        raise RuntimeError(('\\n'+80*'*'+'\\n').join(err_list))\n",
    "\n",
    "    # Converting the list and tuples to strings\n",
    "    hp_dict_ = odict()\n",
    "    etc_dict_ = odict()\n",
    "    for opt, val in cfg_dict_input.items():\n",
    "        val = cfg_dict_input[opt]\n",
    "        if isinstance(val, (int, float, str, bool, type(None))):\n",
    "            srlval = val\n",
    "        elif isinstance(val, (list, tuple)):\n",
    "            srlval = repr(val)\n",
    "        else: \n",
    "            msg_  = f'Not sure how to log \"{opt}\" with '\n",
    "            msg_ += f'a value type of \"{type(val)}\"'\n",
    "            raise RuntimeError(msg_)\n",
    "\n",
    "        if opt in hpopts:\n",
    "            hp_dict_[opt] = srlval\n",
    "        elif opt in etcopts:\n",
    "            etc_dict_[opt] = srlval\n",
    "        else:\n",
    "            raise RuntimeError(f'Not sure how to log \"{opt}\"')\n",
    "\n",
    "    # Few exceptions for the etc directory\n",
    "    etc_dict_['hostname'] = hostname\n",
    "    etc_dict_['commit'] = commit_hash\n",
    "    etc_dict_['date/cfg'] = etc_dict_.pop('date')\n",
    "    etc_dict_['date/run'] = dtnow\n",
    "    etc_dict_['io/dvc_mdl'] = tch_dvcmdl\n",
    "    etc_dict_.pop('io/results_dir')\n",
    "    etc_dict_.pop('io/storage_dir')\n",
    "\n",
    "    # Repeating the values by n_seeds\n",
    "    hp_dict = odict()\n",
    "    for opt, val in hp_dict_.items():\n",
    "        hp_dict[opt] = [val] * n_seeds\n",
    "    etc_dict = odict()\n",
    "    for opt, val in etc_dict_.items():\n",
    "        etc_dict[opt] = [val] * n_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0001e67",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if results_dir is not None:\n",
    "        pathlib.Path(os.sep.join([results_dir, cfg_tree])\n",
    "                     ).mkdir(parents=True, exist_ok=True)\n",
    "    if storage_dir is not None:\n",
    "        cfgstrgpnt_dir = os.sep.join([storage_dir, cfg_tree, cfg_name])\n",
    "        pathlib.Path(cfgstrgpnt_dir).mkdir(parents=True, exist_ok=True)\n",
    "        strgidx = sum(isdir(f'{cfgstrgpnt_dir}/{x}') for x in os.listdir(cfgstrgpnt_dir))\n",
    "        dtnow_ = dtnow[2:].replace('-', '').replace(':', '').replace('.', '')\n",
    "        cfgstrg_dir = f'{cfgstrgpnt_dir}/{strgidx:02d}_{dtnow_}'\n",
    "        pathlib.Path(cfgstrg_dir).mkdir(parents=True, exist_ok=True)\n",
    "    if do_logtb:\n",
    "        if 'tbwriter' in locals():\n",
    "            tbwriter.close()\n",
    "        tbwriter = SummaryWriter(cfgstrg_dir)\n",
    "    if do_profile:\n",
    "        profiler = Profiler()\n",
    "        profiler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initializing the model\n",
    "    model = bffnn(dim, nn_width, nn_hidden, nn_act, (n_seeds,), rng, out_width=dim)\n",
    "    if do_bootstrap:\n",
    "        target = bffnn(dim, nn_width, nn_hidden, nn_act, (n_seeds,), rng, out_width=dim)\n",
    "        target.load_state_dict(model.state_dict())\n",
    "    else:\n",
    "        target = model\n",
    "\n",
    "    # Set the optimizer\n",
    "    if opt_type == 'adam':\n",
    "        opt = torch.optim.Adam(model.parameters(), lr)\n",
    "    elif opt_type == 'sgd':\n",
    "        opt = torch.optim.SGD(model.parameters(), lr)\n",
    "    else:\n",
    "        raise NotImplementedError(f'opt/dstr=\"{opt_type}\" not implmntd')\n",
    "\n",
    "    # Evaluation tools\n",
    "    erng = rng\n",
    "    last_perfdict = dict()\n",
    "    ema = EMA(gamma=0.999, gamma_sq=0.998)\n",
    "    trn_sttime = time.time()\n",
    "\n",
    "    # Data writer construction\n",
    "    hdfpth = None\n",
    "    if results_dir is not None:\n",
    "        hdfpth = f'{results_dir}/{cfg_tree}/{cfg_name}.h5'\n",
    "    avg_history = odict()\n",
    "    dwriter = DataWriter(flush_period=ioflsh_period*n_seeds, \n",
    "                         compression_level=io_cmprssnlvl)\n",
    "\n",
    "    if storage_dir is not None:\n",
    "        with plt.style.context('default') as aa, plt.ioff() as bb:\n",
    "            ax_rows, ax_cols = 7, 3\n",
    "            figax_list = [plt.subplots(ax_rows, ax_cols, \n",
    "                figsize=(2.5*ax_cols, 2.5*ax_rows), dpi=100) for _ in range(3)]\n",
    "            (fig_mdl, ax_mdl), (fig_trg, ax_trg), (fig_gt, ax_gt) = figax_list\n",
    "            cax_mdl, cax_trg, cax_gt = None, None, None\n",
    "            # cax_list = [[make_axes_locatable(ax).append_axes('right', size='5%', pad=0.05) \n",
    "            #              for ax in np.array(axes).ravel().tolist()]\n",
    "            #             for axes in (ax_mdl, ax_trg, ax_gt)]\n",
    "            # cax_mdl, cax_trg, cax_gt = cax_list\n",
    "        stat_history = defaultdict(list)\n",
    "        model_history = odict()\n",
    "        target_history = odict()\n",
    "        \n",
    "        if (srch_dstr is not None) and (dim == 2):\n",
    "            with plt.style.context('default'):\n",
    "                fig_srch, ax_srch = plt.subplots(1, 1, figsize=(3.2, 2.5), dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67d989",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    for epoch in range(n_epochs+1):\n",
    "        if srch_dstr is not None:\n",
    "            update_problem = (epoch >= srch_inittrn) and ((epoch - srch_inittrn) % srch_frq == 0)\n",
    "            if update_problem:\n",
    "                if srch_dstr in ('mcmc',):\n",
    "                    prpsd_srch_mu = old_srch_mu + srch_sigma * rng.normal((n_seeds, sdim))\n",
    "                    assert prpsd_srch_mu.shape == (n_seeds, sdim)\n",
    "                    \n",
    "                    srch_mu_tch = prpsd_srch_mu\n",
    "                    srch_mu = srch_mu_tch.detach().cpu().numpy()\n",
    "                    assert srch_mu.shape == (n_seeds, sdim)\n",
    "                    \n",
    "                    prblmparams = mcmc_enc(srch_mu_tch)\n",
    "                    problem = DeltaLineProblem(source=prblmparams['source'], \n",
    "                        sink=prblmparams['sink'], current=prblmparams['current'],\n",
    "                        tch_device=tch_device, tch_dtype=tch_dtype)\n",
    "                else:\n",
    "                    raise ValueError(f'srch_dstr={srch_dstr} not implemented')\n",
    "                \n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Sampling the volumes\n",
    "        volsamps = volsampler(n=n_srf)\n",
    "\n",
    "        # Sampling the points from the srferes\n",
    "        srfsamps = srfsampler(volsamps, n_points, do_detspacing=do_detspacing)\n",
    "        points = nn.Parameter(srfsamps['points'])\n",
    "        surfacenorms = srfsamps['tangents']\n",
    "        areas = srfsamps['areas']\n",
    "        assert points.shape == (n_seeds, n_srf, n_points, dim)\n",
    "        assert surfacenorms.shape == (n_seeds, n_srf, n_points, dim)\n",
    "        assert areas.shape == (n_seeds, n_srf,)\n",
    "\n",
    "        points_mdl = points[:, :, :n_srfpts_mdl, :]\n",
    "        assert points_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "        points_trg = points[:, :, n_srfpts_mdl:, :]\n",
    "        assert points_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "        surfacenorms_mdl = surfacenorms[:, :, :n_srfpts_mdl, :]\n",
    "        assert surfacenorms_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "        surfacenorms_trg = surfacenorms[:, :, n_srfpts_mdl:, :]\n",
    "        assert surfacenorms_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "        # Making surface integral predictions using the reference model\n",
    "        u_mdl = model(points_mdl)\n",
    "        assert u_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "        nabla_x_u_mdl = curl(u_mdl, points_mdl, create_graph=True)\n",
    "        assert nabla_x_u_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl, dim)\n",
    "        \n",
    "        normprods_mdl = (nabla_x_u_mdl * surfacenorms_mdl).sum(dim=-1)\n",
    "        assert normprods_mdl.shape == (n_seeds, n_srf, n_srfpts_mdl)\n",
    "        if n_srfpts_mdl > 0:\n",
    "            mean_normprods_mdl = normprods_mdl.mean(dim=-1, keepdim=True)\n",
    "            assert mean_normprods_mdl.shape == (n_seeds, n_srf, 1)\n",
    "        else:\n",
    "            mean_normprods_mdl = 0.0\n",
    "\n",
    "        # Making surface integral predictions using the target model\n",
    "        u_trg = target(points_trg)\n",
    "        assert u_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "        nabla_x_u_trg = curl(u_trg, points_trg, create_graph=not(do_bootstrap))\n",
    "        assert nabla_x_u_trg.shape == (n_seeds, n_srf, n_srfpts_trg, dim)\n",
    "\n",
    "        normprods_trg = (nabla_x_u_trg * surfacenorms_trg).sum(dim=-1)\n",
    "        assert normprods_trg.shape == (n_seeds, n_srf, n_srfpts_trg)\n",
    "        if do_dblsampling:\n",
    "            assert n_rsdls == 2\n",
    "\n",
    "            mean_normprods_trg1 = normprods_trg[..., 0::2].mean(\n",
    "                dim=-1, keepdim=True)\n",
    "            assert mean_normprods_trg1.shape == (n_seeds, n_srf, 1)\n",
    "\n",
    "            mean_normprods_trg2 = normprods_trg[..., 1::2].mean(\n",
    "                dim=-1, keepdim=True)\n",
    "            assert mean_normprods_trg2.shape == (n_seeds, n_srf, 1)\n",
    "\n",
    "            mean_normprods_trg = torch.cat(\n",
    "                [mean_normprods_trg1, mean_normprods_trg2], dim=-1)\n",
    "            assert mean_normprods_trg.shape == (n_seeds, n_srf, n_rsdls)\n",
    "        else:\n",
    "            assert n_rsdls == 1\n",
    "\n",
    "            mean_normprods_trg = normprods_trg.mean(dim=-1, keepdim=True)\n",
    "            assert mean_normprods_trg.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Linearly combining the reference and target predictions\n",
    "        mean_normprods = (       w_trg  * mean_normprods_trg +\n",
    "                          (1.0 - w_trg) * mean_normprods_mdl)\n",
    "        assert mean_normprods.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Considering the surface areas\n",
    "        pred_surfintegs = mean_normprods * areas.reshape(n_seeds, n_srf, 1)\n",
    "        assert pred_surfintegs.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Getting the reference volume integrals\n",
    "        ref_volintegs = problem.integrate_volumes(volsamps)\n",
    "        assert ref_volintegs.shape == (n_seeds, n_srf)\n",
    "\n",
    "        # Getting the residual terms\n",
    "        resterms = pred_surfintegs - ref_volintegs.reshape(n_seeds, n_srf, 1)\n",
    "        assert resterms.shape == (n_seeds, n_srf, n_rsdls)\n",
    "\n",
    "        # Multiplying the residual terms\n",
    "        if do_dblsampling:\n",
    "            resterms_prod = resterms.prod(dim=-1)\n",
    "            assert resterms_prod.shape == (n_seeds, n_srf)\n",
    "        else:\n",
    "            resterms_prod = torch.square(resterms).squeeze(-1)\n",
    "            assert resterms_prod.shape == (n_seeds, n_srf)\n",
    "\n",
    "        # Computing the main loss\n",
    "        loss_main = resterms_prod.mean(-1)\n",
    "        assert loss_main.shape == (n_seeds,)\n",
    "\n",
    "        if do_bootstrap:\n",
    "            with torch.no_grad():\n",
    "                u_mdl_prime = target(points_mdl)\n",
    "            loss_trgreg = torch.square(u_mdl - u_mdl_prime).mean([-3, -2, -1])\n",
    "            assert loss_trgreg.shape == (n_seeds,)\n",
    "        else:\n",
    "            loss_trgreg = torch.zeros(n_seeds, device=tch_device, dtype=tch_dtype)\n",
    "            assert loss_trgreg.shape == (n_seeds,)\n",
    "\n",
    "        # The initial condition loss\n",
    "        renew_ic = (epoch == 0) if (ic_frq == 0) else (epoch % ic_frq == 0)\n",
    "        if ic_needsampling and renew_ic and (ic_dstr is not None):\n",
    "            with torch.no_grad():\n",
    "                if ic_dstr == 'sphere':\n",
    "                    ic_normsamps =rng.normal((n_seeds, ic_n, dim))\n",
    "                    assert ic_normsamps.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_ptstilde = ic_normsamps / ic_normsamps.norm()\n",
    "                    assert ic_ptstilde.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    assert ic_c.shape == (n_seeds, ic_n, dim)\n",
    "                    assert ic_r.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    ic_allpnts = ic_c + ic_ptstilde * ic_r\n",
    "                    assert ic_allpnts.shape == (n_seeds, ic_n, dim)\n",
    "                elif ic_dstr == 'trnvol':\n",
    "                    icvols = volsampler(n=ic_n)\n",
    "                    assert icvols['type'] == 'ball'\n",
    "\n",
    "                    ic_c = icvols['centers']\n",
    "                    assert ic_c.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_r_ = icvols['radii']\n",
    "                    assert ic_r_.shape == (n_seeds, ic_n)\n",
    "\n",
    "                    ic_r = ic_r_.unsqueeze(dim=-1)\n",
    "                    assert ic_r.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    untrd = rng.uniform((n_seeds, ic_n, 1))\n",
    "                    assert untrd.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    untr = untrd.pow(1.0 / dim)\n",
    "                    assert untr.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    ic_pntrs = untr * ic_r\n",
    "                    assert ic_pntrs.shape == (n_seeds, ic_n, 1)\n",
    "\n",
    "                    ic_theta = rng.normal((n_seeds, ic_n, dim))\n",
    "                    assert ic_theta.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_thtilde = ic_theta / ic_theta.norm(dim=-1, keepdim=True)\n",
    "                    assert ic_thtilde.shape == (n_seeds, ic_n, dim)\n",
    "\n",
    "                    ic_allpnts = ic_c + ic_thtilde * ic_pntrs\n",
    "                    assert ic_allpnts.shape == (n_seeds, ic_n, dim)\n",
    "                else:\n",
    "                    raise ValueError(f'ic/dstr={ic_dstr} not defined')\n",
    "\n",
    "                ic_allgtvs = get_prob_sol(problem, ic_allpnts, eval_bs, \n",
    "                    get_field=False, out_lib='torch')['v']\n",
    "                assert ic_allgtvs.shape == (n_seeds, ic_n)\n",
    "\n",
    "        if ic_needsampling:\n",
    "            ic_idxs = ((np.arange(ic_bs) + epoch * ic_bs) % ic_n).tolist()\n",
    "\n",
    "            ic_pnts = ic_allpnts[:, ic_idxs, :]\n",
    "            assert ic_pnts.shape == (n_seeds, ic_bs, dim)\n",
    "\n",
    "            ic_vpreds = model(ic_pnts).squeeze(dim=-1)\n",
    "            assert ic_vpreds.shape == (n_seeds, ic_bs)\n",
    "\n",
    "            ic_gtvs = ic_allgtvs[:, ic_idxs]\n",
    "            assert ic_gtvs.shape == (n_seeds, ic_bs)\n",
    "        elif (ic_dstr is not None):\n",
    "            ic_pnts = points_mdl.reshape(n_seeds, n_srf * n_srfpts_mdl, dim)\n",
    "            assert ic_pnts.shape == (n_seeds, ic_bs, dim)\n",
    "\n",
    "            ic_vpreds = u_mdl.reshape(n_seeds, n_srf * n_srfpts_mdl)\n",
    "            assert ic_vpreds.shape == (n_seeds, ic_bs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ic_gtvs = get_prob_sol(problem, ic_pnts, eval_bs, \n",
    "                    get_field=False, out_lib='torch')['v']\n",
    "            assert ic_gtvs.shape == (n_seeds, ic_bs)\n",
    "\n",
    "        if ic_bpp == 'bias':\n",
    "            mdl_bias = model.layer_last[1].squeeze(dim=-1)\n",
    "            assert mdl_bias.shape == (n_seeds, 1)\n",
    "\n",
    "            ic_vpreds = ic_vpreds.detach() - mdl_bias.detach() + mdl_bias\n",
    "            assert ic_vpreds.shape == (n_seeds, ic_bs)\n",
    "        elif ic_bpp == 'all':\n",
    "            pass\n",
    "        else:\n",
    "            raise RuntimeError(f'ic/bpp={ic_bpp} not defined')\n",
    "\n",
    "        if ic_dstr is not None:\n",
    "            loss_ic = torch.square(ic_vpreds - ic_gtvs).mean(dim=-1)\n",
    "            assert loss_ic.shape == (n_seeds,)\n",
    "        else:\n",
    "            loss_ic = torch.zeros(n_seeds, dtype=tch_dtype, device=tch_device)\n",
    "            assert loss_ic.shape == (n_seeds,)\n",
    "\n",
    "        # The total loss\n",
    "        loss = loss_main + w_trgreg * loss_trgreg + w_ic * loss_ic\n",
    "        assert loss.shape == (n_seeds,)\n",
    "\n",
    "        loss_sum = loss.sum()\n",
    "        loss_sum.backward()\n",
    "\n",
    "        # We will not update in the first epoch so that we will \n",
    "        # record the initialization statistics as well. Instead, \n",
    "        # we will update an extra epoch at the end.\n",
    "        if (epoch > 0):\n",
    "            opt.step()\n",
    "\n",
    "        # Updating the target network\n",
    "        if do_bootstrap and (epoch > 0):\n",
    "            model_sd = model.state_dict()\n",
    "            target_sd = target.state_dict()\n",
    "            newtrg_sd = dict()\n",
    "            with torch.no_grad():\n",
    "                for key, param in model_sd.items():\n",
    "                    param_trg = target_sd[key]\n",
    "                    newtrg_sd[key] = tau * param_trg + (1-tau) * param\n",
    "            target.load_state_dict(newtrg_sd)\n",
    "\n",
    "        # computing the normal product variances\n",
    "        with torch.no_grad(): \n",
    "            normprods = torch.cat([normprods_mdl, normprods_trg], dim=-1)\n",
    "            npvm = (normprods.var(dim=-1)*areas.square()).mean(-1)\n",
    "\n",
    "        # evaluating the performance of the model and target    \n",
    "        perf_dict = dict()\n",
    "        eval_strg = dict()\n",
    "        for eid, eopts in evalprms.items():\n",
    "            edstr = eopts['dstr']\n",
    "            n_evlpnts = eopts['n']\n",
    "            e_frq = eopts['frq']\n",
    "            e_store = eopts['store']\n",
    "\n",
    "            if (epoch % e_frq) > 0:\n",
    "                assert eid in last_perfdict\n",
    "                perf_dict[eid] = last_perfdict[eid]\n",
    "                continue\n",
    "\n",
    "            # Sampling the evaluation points\n",
    "            with torch.no_grad():\n",
    "                if edstr == 'uniform':\n",
    "                    e_bias = eopts['bias']\n",
    "                    assert e_bias.shape == (n_seeds, 1, dim)\n",
    "\n",
    "                    e_slope = eopts['slope']\n",
    "                    assert e_slope.shape == (n_seeds, 1, dim)\n",
    "\n",
    "                    e_unfpnts = erng.uniform((n_seeds, n_evlpnts, dim))\n",
    "                    assert e_unfpnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                    e_pnts = e_bias + e_unfpnts * e_slope\n",
    "                    assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "                elif edstr in ('ball', 'trnvol'):\n",
    "                    if edstr == 'ball':\n",
    "                        e_c = eopts['c_xpnd']\n",
    "                        assert e_c.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                        e_r = eopts['r_xpnd']\n",
    "                        assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "                    elif edstr == 'trnvol':\n",
    "                        evols = volsampler(n=n_evlpnts)\n",
    "                        assert evols['type'] == 'disk'\n",
    "                        assert dim == 3\n",
    "\n",
    "                        e_c = evols['centers']\n",
    "                        assert e_c.shape == (n_seeds, n_evlpnts, dim)\n",
    "                        \n",
    "                        e_n_ = evols['normals'].squeeze(-1)\n",
    "                        assert e_n_.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                        e_r_ = evols['radii']\n",
    "                        assert e_r_.shape == (n_seeds, n_evlpnts)\n",
    "\n",
    "                        e_r = e_r_.unsqueeze(dim=-1)\n",
    "                        assert e_r.shape == (n_seeds, n_evlpnts, 1)\n",
    "                    else:\n",
    "                        raise RuntimeError(f'case not defined')\n",
    "\n",
    "                    untrd = erng.uniform((n_seeds, n_evlpnts, 1))\n",
    "                    assert untrd.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                    untr = untrd.pow(1.0 / dim)\n",
    "                    assert untr.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                    e_pntrs = untr * e_r\n",
    "                    assert e_pntrs.shape == (n_seeds, n_evlpnts, 1)\n",
    "\n",
    "                    etheta = erng.normal((n_seeds, n_evlpnts, dim))\n",
    "                    assert etheta.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                    if edstr == 'trnvol':\n",
    "                        etheta = etheta -  (etheta * e_n_).sum(dim=-1, keepdim=True)\n",
    "                        assert etheta.shape == (n_seeds, n_evlpnts, dim)\n",
    "                    \n",
    "                    ethtilde = etheta / etheta.norm(dim=-1, keepdim=True)\n",
    "                    assert ethtilde.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "                    e_pnts = e_c + ethtilde * e_pntrs\n",
    "                    assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "                elif edstr in ('grid'):\n",
    "                    e_pnts = eopts['pnts']\n",
    "                    assert e_pnts.shape == (n_seeds, n_evlpnts, dim)\n",
    "                else:\n",
    "                    raise RuntimeError(f'eval dstr \"{edstr}\" not implmntd')\n",
    "\n",
    "            # Computing the model, target and ground truth solutions\n",
    "            prob_sol = get_prob_sol(problem, e_pnts, n_eval=eval_bs, \n",
    "                get_field=False, out_lib='torch')\n",
    "\n",
    "            e_prbsol = prob_sol['v']\n",
    "            assert e_prbsol.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "            # Computing the model solution\n",
    "            with torch.no_grad():\n",
    "                mdl_sol = get_nn_sol(model, e_pnts, n_eval=eval_bs,\n",
    "                    get_field=False, out_lib='torch')\n",
    "\n",
    "            e_mdlsol = mdl_sol['v']\n",
    "            assert e_mdlsol.shape == (n_seeds, n_evlpnts, dim)\n",
    "            \n",
    "            # Computing the target solution\n",
    "            if do_bootstrap:\n",
    "                with torch.no_grad():\n",
    "                    trg_sol = get_nn_sol(target, e_pnts, n_eval=eval_bs, \n",
    "                        get_field=False, out_lib='torch')\n",
    "\n",
    "                e_trgsol = trg_sol['v']\n",
    "                assert e_trgsol.shape == (n_seeds, n_evlpnts, dim)\n",
    "\n",
    "            eperfs = dict()\n",
    "            eperfs['mdl'] = get_perfdict(e_pnts, e_mdlsol, e_prbsol)\n",
    "            if do_bootstrap:\n",
    "                eperfs['trg'] = get_perfdict(e_pnts, e_trgsol, e_prbsol)\n",
    "            eperfs = deep2hie(eperfs, dictcls=dict)\n",
    "            # Example: eperfs = {'mdl/pln/mse': ...,\n",
    "            #                    'mdl/pln/mae': ...,\n",
    "            #                    'mdl/bc/mse': ...,\n",
    "            #                    'mdl/bc/mae': ...,\n",
    "            #                    'mdl/slc/mse': ...,\n",
    "            #                    'mdl/slc/mae': ...,\n",
    "            #                    'trg/pln/mse': ...,\n",
    "            #                    'trg/pln/mae': ...,\n",
    "            #                    'trg/bc/mse': ...,\n",
    "            #                    'trg/bc/mae': ...,\n",
    "            #                    'trg/slc/mse': ...,\n",
    "            #                    'trg/slc/mae': ...,\n",
    "            #                   }\n",
    "            perf_dict[eid] = eperfs\n",
    "            last_perfdict[eid] = eperfs\n",
    "            \n",
    "            if do_logtb:\n",
    "                for kk, vv in eperfs.items():\n",
    "                    tbwriter.add_scalar(f'perf/{eid}/{kk}', vv.mean(), epoch)\n",
    "            \n",
    "            # Storing the evaluation results\n",
    "            if e_store:\n",
    "                e_strg = dict()\n",
    "                e_strg['sol/mdl'] = e_mdlsol\n",
    "                if do_bootstrap:\n",
    "                    e_strg['sol/trg'] = e_trgsol\n",
    "                e_strg['sol/gt'] = e_prbsol\n",
    "                if edstr != 'grid':\n",
    "                    e_strg['pnts'] = e_pnts\n",
    "                e_strg = {kk: vv.detach().cpu().numpy().astype(np.float16) \n",
    "                          for kk, vv in e_strg.items()}\n",
    "                eval_strg[eid] = e_strg\n",
    "\n",
    "                if do_logtb and (edstr == 'grid') and (dim == 3):\n",
    "                    soltd_list = [('mdl', mdl_sol, fig_mdl, ax_mdl, cax_mdl)]\n",
    "                    if do_bootstrap:\n",
    "                        soltd_list += [('trg', trg_sol, fig_trg, ax_trg, cax_trg)]\n",
    "                    soltd_list += [('gt', prob_sol, fig_gt, ax_gt, cax_gt)]\n",
    "                    for sol_t, sol_dict, fig, ax, cax in soltd_list:\n",
    "                        e_ngpd = eopts['n_gpd']\n",
    "                        with torch.no_grad():\n",
    "                            y_plt1 = sol_dict['v']\n",
    "                            e_ng = int(np.prod(e_ngpd))\n",
    "                            assert y_plt1.shape == (n_seeds, e_ng, dim)\n",
    "                            y_plt2 = y_plt1 - y_plt1.mean(dim=-2, keepdim=True)\n",
    "                            assert y_plt2.shape == (n_seeds, e_ng, dim)\n",
    "                            y_plt3 = y_plt2.mean(dim=0)\n",
    "                            assert y_plt3.shape == (e_ng, dim)\n",
    "                        draw_heatmap(eopts['pnts_plt'], y_plt3, fig, ax,\n",
    "                            field_abrv='A', components='xyz', zstr_loc='left', \n",
    "                            show_title='all')\n",
    "                        fig.set_tight_layout(True)\n",
    "                        tbwriter.add_figure(f'viz/{eid}/{sol_t}', fig, epoch)\n",
    "                    tbwriter.flush()\n",
    "        \n",
    "        # Applying search on the latent problem parameters\n",
    "        update_search = False\n",
    "        if srch_dstr is not None:  \n",
    "            update_search = ((epoch - srch_inittrn) % srch_frq == (srch_frq - 1))\n",
    "            update_search = update_search and (epoch >= srch_inittrn)\n",
    "        \n",
    "        if update_search:\n",
    "            assert srch_dstr == 'mcmc'\n",
    "        \n",
    "            need_field = (obs_y_dstr == 'field')\n",
    "            sol_key = {'potential': 'v', 'field': 'e'}[obs_y_dstr]\n",
    "            \n",
    "            # Computing the problem solution\n",
    "            obs_probsol = get_prob_sol(problem, obs_x, n_eval=eval_bs, \n",
    "                get_field=need_field, out_lib='torch')[sol_key]\n",
    "            if (obs_y_dstr == 'potential'):\n",
    "                obs_probsol = obs_probsol.unsqueeze(-1)\n",
    "                obs_probsol = obs_probsol - obs_probsol.mean(dim=-2, keepdim=True)\n",
    "            assert obs_probsol.shape == (n_seeds, obs_n, obs_ydim)\n",
    "\n",
    "            # Computing the model solution\n",
    "            obs_mdlsol = get_nn_sol(model, obs_x, n_eval=eval_bs,\n",
    "                get_field=need_field, out_lib='torch')[sol_key]\n",
    "            if (obs_y_dstr == 'potential'):\n",
    "                obs_mdlsol = obs_mdlsol.unsqueeze(-1)\n",
    "                obs_mdlsol = obs_mdlsol - obs_mdlsol.mean(dim=-2, keepdim=True)\n",
    "            assert obs_mdlsol.shape == (n_seeds, obs_n, obs_ydim)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                obs_mdlerr = obs_mdlsol - obs_y\n",
    "                assert obs_mdlerr.shape == (n_seeds, obs_n, obs_ydim)\n",
    "                \n",
    "                assert srch_metric_dstr == 'mse'\n",
    "                \n",
    "                obs_loss = obs_mdlerr.square().sum(dim=-1)\n",
    "                assert obs_loss.shape == (n_seeds, obs_n)\n",
    "                    \n",
    "                if (srch_metric_min is not None) or (srch_metric_max is not None):\n",
    "                    obs_lossclp = torch.clip(obs_loss, srch_metric_min, srch_metric_max)\n",
    "                    assert obs_lossclp.shape == (n_seeds, obs_n)\n",
    "                else:\n",
    "                    obs_lossclp = obs_loss\n",
    "                    assert obs_lossclp.shape == (n_seeds, obs_n)\n",
    "                \n",
    "                prpsd_loglike = obs_lossclp.mean(dim=-1) * (-srch_metric_coeff)\n",
    "                assert prpsd_loglike.shape == (n_seeds,)\n",
    "                \n",
    "                prpsd_logpri_ = normal_logprob(prpsd_srch_mu, srch_prior_loc_tch, srch_prior_scale_tch)\n",
    "                assert prpsd_logpri_.shape == (n_seeds, srch_n)\n",
    "                \n",
    "                prpsd_logpri = prpsd_logpri_.sum(dim=-1)\n",
    "                assert prpsd_logpri.shape == (n_seeds,)\n",
    "                \n",
    "                prpsd_logpost = prpsd_logpri + prpsd_loglike\n",
    "                assert prpsd_logpost.shape == (n_seeds,)\n",
    "                \n",
    "                if old_logpri is None:\n",
    "                    old_logpri = prpsd_logpri\n",
    "                    assert old_logpri.shape == (n_seeds,)\n",
    "                \n",
    "                    old_loglike = prpsd_loglike\n",
    "                    assert old_loglike.shape == (n_seeds,)\n",
    "                    \n",
    "                    old_logpost = prpsd_logpost\n",
    "                    assert old_logpost.shape == (n_seeds,)\n",
    "                    \n",
    "                    best_loglike = old_loglike\n",
    "                    assert best_loglike.shape == (n_seeds,)\n",
    "\n",
    "                delta_logpost = prpsd_logpost - old_logpost\n",
    "                assert delta_logpost.shape == (n_seeds,)\n",
    "                \n",
    "                acceptance = (srch_tmprtr * delta_logpost) > rng.uniform((n_seeds, 1)).log().squeeze(-1)\n",
    "                assert acceptance.shape == (n_seeds,)\n",
    "                \n",
    "                # Updating the monte carlo chains and log-probabilities\n",
    "                old_srch_mu = torch.where(acceptance.reshape(n_seeds, 1, 1), prpsd_srch_mu, old_srch_mu)\n",
    "                assert old_srch_mu.shape == (n_seeds, sdim)\n",
    "                \n",
    "                old_loglike = torch.where(acceptance, prpsd_loglike, old_loglike)\n",
    "                assert old_loglike.shape == (n_seeds,)\n",
    "                \n",
    "                old_logpri = torch.where(acceptance, prpsd_logpri, old_logpri)\n",
    "                assert old_logpri.shape == (n_seeds,)\n",
    "                \n",
    "                old_logpost =  torch.where(acceptance, prpsd_logpost, old_logpost)\n",
    "                assert old_logpost.shape == (n_seeds,)\n",
    "                \n",
    "                best_loglike = torch.where(old_loglike > best_loglike, old_loglike, best_loglike)\n",
    "                assert best_loglike.shape == (n_seeds,)\n",
    "                \n",
    "                # Applying the reset curriculum\n",
    "                i_srchdrwas = (epoch - srch_inittrn) // srch_frq\n",
    "                \n",
    "                if (i_srchdrwas % (n_srchdraws // srch_reset_n) == 0):\n",
    "                    old_srch_mu_ = old_srch_mu.reshape(n_grps, n_cpg, sdim)\n",
    "                    assert old_srch_mu_.shape == (n_grps, n_cpg, sdim)\n",
    "                    \n",
    "                    old_logpost_ = old_logpost.reshape(n_grps, n_cpg)\n",
    "                    assert old_logpost_.shape == (n_grps, n_cpg)\n",
    "                    \n",
    "                    top_idx = torch.topk(old_logpost_, srch_reset_k, dim=1, largest=True, sorted=True).indices\n",
    "                    assert top_idx.shape == (n_grps, srch_reset_k)\n",
    "                    \n",
    "                    top_chains = torch.take_along_dim(old_srch_mu_, top_idx.unsqueeze(-1).unsqueeze(-1), dim=-3)\n",
    "                    assert top_chains.shape == (n_grps, srch_reset_k, sdim)\n",
    "                    \n",
    "                    # resetting the mu\n",
    "                    srch_reset_reps = n_cpg // srch_reset_k\n",
    "                    old_srch_mu = top_chains.reshape(n_grps, 1, srch_reset_k, sdim)\n",
    "                    old_srch_mu = old_srch_mu.expand(n_grps, srch_reset_reps, srch_reset_k, sdim)\n",
    "                    old_srch_mu = old_srch_mu.reshape(n_seeds, sdim)\n",
    "                    assert old_srch_mu.shape == (n_seeds, sdim)\n",
    "                    \n",
    "                    # resetting the models\n",
    "                    mdlrepsd = {name: replicate_top(param, top_idx) \n",
    "                                for name, param in model.state_dict().items()}\n",
    "                    model.load_state_dict(mdlrepsd)\n",
    "\n",
    "                    if do_bootstrap:\n",
    "                        trgrepsd = {name: replicate_top(param, top_idx) \n",
    "                            for name, param in target.state_dict().items()}\n",
    "                        target.load_state_dict(trgrepsd)\n",
    "                    \n",
    "                    # resetting the optimizer\n",
    "                    if opt_type == 'sgd':\n",
    "                        pass\n",
    "                    elif opt_type == 'adam':\n",
    "                        optrepsd = deepcopy(opt.state_dict())\n",
    "                        for kk, vv in optrepsd['state'].items():\n",
    "                            vv['exp_avg'] = replicate_top(vv['exp_avg'], top_idx)\n",
    "                            vv['exp_avg_sq'] = replicate_top(vv['exp_avg_sq'], top_idx)\n",
    "                        opt.load_state_dict(optrepsd)\n",
    "                    else:\n",
    "                        raise ValueError(f'opt_type={opt_type} not defined')\n",
    "                \n",
    "                if do_logtb:\n",
    "                    blm = best_loglike.reshape(n_grps, n_cpg)\n",
    "                    tbwriter.add_scalar('search/acceptance', acceptance.float().mean(), epoch)\n",
    "                    for q in np.linspace(0, 1, 5):\n",
    "                        tbwriter.add_scalar(f'search/best_loglike/q{int(100*q):02d}', \n",
    "                                            blm.quantile(q, dim=-1).median(), epoch)\n",
    "                    tbwriter.add_scalar('search/loglike', old_loglike.mean(), epoch)\n",
    "                    tbwriter.add_scalar('search/logpri', old_logpri.mean(), epoch)\n",
    "                    tbwriter.add_scalar('search/logpost', old_logpost.mean(), epoch)\n",
    "\n",
    "                    if (srch_enc_dstr == 'srcsnkxy'):\n",
    "                        # Finding the best group for scatter plotting\n",
    "                        i_grp = old_loglike.reshape(n_grps, n_cpg).max(dim=-1).values.argmax()\n",
    "                        x_scatt = old_srch_mu.reshape(n_grps, n_cpg, wire_n, dim-1)[i_grp]\n",
    "                        assert x_scatt.shape == (n_cpg, wire_n, dim-1)\n",
    "                        x_scatt = x_scatt.reshape(n_cpg * wire_n, dim-1)\n",
    "                        assert x_scatt.shape == (n_cpg * wire_n, dim-1)\n",
    "                        x_scatt = x_scatt.detach().cpu().numpy()\n",
    "                        assert x_scatt.shape == (n_cpg * wire_n, dim-1)\n",
    "\n",
    "                        ax_srch.clear()\n",
    "                        ax_srch.scatter(x_scatt[:, 0], x_scatt[:, 1], s=1)\n",
    "                        \n",
    "                        fig_srch.set_tight_layout(True)\n",
    "                        tbwriter.add_figure(f'viz/srch/mu', fig_srch, epoch)\n",
    "                    \n",
    "                    tbwriter.flush()\n",
    "  \n",
    "        # monitoring the resource utilization \n",
    "        if epoch % iomon_period == 0:\n",
    "            s_rsrc = resource.getrusage(resource.RUSAGE_SELF)\n",
    "            c_rsrc = resource.getrusage(resource.RUSAGE_CHILDREN)\n",
    "            \n",
    "            psmem = psutil.virtual_memory()\n",
    "            pscpu = psutil.cpu_times()\n",
    "            pscpuload = psutil.getloadavg()\n",
    "            mon_dict = {'cpu/mem/tot': [psmem.total] * n_seeds, \n",
    "                'cpu/mem/avail': [psmem.available] * n_seeds, \n",
    "                'cpu/mem/used': [psmem.used] * n_seeds,\n",
    "                'cpu/mem/free': [psmem.free] * n_seeds,\n",
    "                'cpu/time/user/ps': [pscpu.user] * n_seeds,\n",
    "                'cpu/time/sys/ps': [pscpu.system] * n_seeds,\n",
    "                'cpu/time/idle/ps': [pscpu.idle] * n_seeds,\n",
    "                'cpu/load/1m': [pscpuload[0]] * n_seeds,\n",
    "                'cpu/load/5m': [pscpuload[1]] * n_seeds,\n",
    "                'cpu/load/15m': [pscpuload[2]] * n_seeds,\n",
    "                'cpu/time/train': [time.time()   - trn_sttime] * n_seeds,\n",
    "                'cpu/time/sys/py':   [s_rsrc.ru_stime  + c_rsrc.ru_stime] * n_seeds,\n",
    "                'cpu/time/user/py':  [s_rsrc.ru_utime  + c_rsrc.ru_utime] * n_seeds,\n",
    "                'n_seeds': [n_seeds] * n_seeds}\n",
    "            if 'cuda' in device_name:\n",
    "                t_gpumem = torch.cuda.get_device_properties(tch_device).total_memory\n",
    "                r_gpumem = torch.cuda.memory_reserved(tch_device)\n",
    "                a_gpumem = torch.cuda.memory_allocated(tch_device)\n",
    "                f_gpumem = r_gpumem - a_gpumem\n",
    "                mon_dict.update({'gpu/mem/tot':   [t_gpumem] * n_seeds,\n",
    "                                 'gpu/mem/res':   [r_gpumem] * n_seeds,\n",
    "                                 'gpu/mem/alloc': [a_gpumem] * n_seeds,\n",
    "                                 'gpu/mem/free':  [f_gpumem] * n_seeds})\n",
    "\n",
    "        # pushing the results to the data writer\n",
    "        psld = deep2hie({'perf': perf_dict}, odict)\n",
    "        slst = [('loss/total',  loss.tolist()),\n",
    "                ('loss/main',   loss_main.tolist()),\n",
    "                ('loss/trgreg', loss_trgreg.tolist()),\n",
    "                ('loss/ic',     loss_ic.tolist()),\n",
    "                ('npvm',        npvm.tolist()),\n",
    "                *list(psld.items())]\n",
    "        stat_dict = odict(slst)\n",
    "        for stat_name, stat_vals in stat_dict.items():\n",
    "            avg_history.setdefault(stat_name, [])\n",
    "            avg_history[stat_name].append(stat_vals)\n",
    "\n",
    "        dtups = []\n",
    "        if epoch % io_avgfrq == 0:\n",
    "            avg_statlst  = [('epoch',       [epoch] * n_seeds),\n",
    "                            ('rng_seed',    rng_seeds.tolist())]\n",
    "            avg_statlst += [(name, np.stack(svl, axis=0).mean(axis=0).tolist())\n",
    "                             for name, svl in avg_history.items()]\n",
    "            avg_statdict = odict(avg_statlst)\n",
    "\n",
    "            dtups += [('hp',    hp_dict,      'pd.cat'),\n",
    "                      ('stat',  avg_statdict, 'pd.qnt'),\n",
    "                      ('etc',   etc_dict,     'pd.cat')]\n",
    "            avg_history = odict()\n",
    "            \n",
    "        for eid, e_strg in eval_strg.items():\n",
    "            msg_ =  f'eval/{eid} requires storage, thus \"eval/{eid}/frq\" '\n",
    "            msg_ += f'% \"io/avg/frq\" == 0 should hold.'\n",
    "            assert epoch % io_avgfrq == 0, msg_\n",
    "            dtups += [(f'var/eval/{eid}', e_strg, 'np.arr')]\n",
    "        \n",
    "        if epoch % iomon_period == 0:\n",
    "            assert epoch % io_avgfrq == 0\n",
    "            dtups += [('mon', mon_dict, 'pd.qnt')]\n",
    "\n",
    "        if epoch % chkpnt_period == 0:\n",
    "            assert epoch % io_avgfrq == 0\n",
    "            mdl_sdnp = {k: v.detach().cpu().numpy() \n",
    "                for k, v in model.state_dict().items()}\n",
    "            dtups += [('mdl',   mdl_sdnp, 'np.arr')]\n",
    "            if do_bootstrap:\n",
    "                trg_sdnp = {k: v.detach().cpu().numpy() \n",
    "                    for k, v in target.state_dict().items()}\n",
    "                dtups += [('trg',   trg_sdnp, 'np.arr')]\n",
    "                \n",
    "        if update_search:\n",
    "            srch_strg = {'prpsd/mu': srch_mu_tch, 'prpsd/loglike': prpsd_loglike, \n",
    "                         'prpsd/logpri': prpsd_logpri, 'prpsd/logpost': prpsd_logpost,\n",
    "                         'old/mu': old_srch_mu, 'old/loglike': old_loglike,\n",
    "                         'old/logpri': old_logpri, 'old/logpost': old_logpost,\n",
    "                         'acceptance': acceptance}\n",
    "            srch_strg = {k: v.detach().cpu().numpy() for k,v in srch_strg.items()}\n",
    "            dtups += [(f'var/srch', srch_strg, 'np.arr')]\n",
    "\n",
    "        dwriter.add(data_tups=dtups, file_path=hdfpth)\n",
    "\n",
    "        # Computing the loss moving averages\n",
    "        loss_ema_mean, loss_ema_std_mean = ema('loss', loss)\n",
    "        npvm_ema_mean, npvm_ema_std_mean = ema('npvm', npvm)\n",
    "        if (epoch % 1000 == 0) and (results_dir is not None):\n",
    "            print_str = f'Epoch {epoch}, EMA loss = {loss_ema_mean:.4f}'\n",
    "            print_str += f' +/- {2*loss_ema_std_mean:.4f}'\n",
    "            print_str += f', EMA Field-Norm Product Variance = {npvm_ema_mean:.4f}'\n",
    "            print_str += f' +/- {2*npvm_ema_std_mean:.4f} ({time.time()-trn_sttime:0.1f} s)'\n",
    "            print(print_str, flush=True)\n",
    "\n",
    "        if do_logtb:\n",
    "            import logging\n",
    "            logging.getLogger(\"tensorboardX.x2num\").setLevel(logging.CRITICAL)  \n",
    "            tbwriter.add_scalar('loss/total', loss.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/main', loss_main.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/trgreg', loss_trgreg.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/ic', loss_ic.mean(), epoch)\n",
    "            tbwriter.add_scalar('loss/npvm', npvm.mean(), epoch)\n",
    "\n",
    "        if do_tchsave and (epoch % chkpnt_period == 0):\n",
    "            model_history[epoch] = deepcopy({k: v.cpu() for k, v\n",
    "                in model.state_dict().items()})\n",
    "            target_history[epoch] = deepcopy({k: v.cpu() for k, v\n",
    "                in target.state_dict().items()})\n",
    "            \n",
    "\n",
    "    if results_dir is not None:\n",
    "        print(f'Training finished in {time.time() - trn_sttime:.1f} seconds.')\n",
    "    dwriter.close()\n",
    "    if do_logtb:\n",
    "        tbwriter.flush()\n",
    "    \n",
    "    outdict = dict()\n",
    "    tchmemusage = profmem()\n",
    "    assert str(tch_device) in tchmemusage\n",
    "    if 'cuda' in device_name:\n",
    "        tch_dvcmem = torch.cuda.get_device_properties(tch_device).total_memory\n",
    "    else:\n",
    "        tch_dvcmem = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
    "    outdict['dvc/mem/alloc'] = tchmemusage[str(tch_device)]\n",
    "    outdict['dvc/mem/total'] = tch_dvcmem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig = None\n",
    "    has_grid = any(eopts.get('dstr', '') == 'grid' \n",
    "                    for eid, eopts in evalprms.items())\n",
    "    if (storage_dir is not None) and has_grid:\n",
    "        eopts = list(eopts for eid, eopts in evalprms.items()\n",
    "                        if eopts.get('dstr', '') == 'grid')[0]\n",
    "        e_pnts = eopts['pnts']\n",
    "        \n",
    "        n_sols = 2 + do_bootstrap\n",
    "        n_rows, n_cols = 7, 3\n",
    "        fig, axes_all = plt.subplots(n_rows, n_cols*n_sols, \n",
    "            figsize=(n_cols*n_sols*2.5, n_rows*2.0), dpi=36)\n",
    "        axes_all = np.array(axes_all).reshape(n_rows, n_cols*n_sols)\n",
    "        figaxes_list = [(fig, axes_all[:, i*3:i*3+3]) for i in range(n_sols)]\n",
    "        \n",
    "        # Computing the model, target and ground truth solutions\n",
    "        prob_sol = get_prob_sol(problem, e_pnts, n_eval=eval_bs, \n",
    "            get_field=False, out_lib='torch')\n",
    "        with torch.no_grad():\n",
    "            mdl_sol = get_nn_sol(model, e_pnts, n_eval=eval_bs, \n",
    "                get_field=False, out_lib='torch') \n",
    "            if do_bootstrap:\n",
    "                trg_sol = get_nn_sol(target, e_pnts, n_eval=eval_bs, \n",
    "                    get_field=False, out_lib='torch')\n",
    "                \n",
    "        soltd_list = [('gt', prob_sol, figaxes_list[0], 'Ground Truth'),\n",
    "                    ('mdl', mdl_sol, figaxes_list[1], 'Prediction')]\n",
    "        if do_bootstrap:\n",
    "            soltd_list += [('trg', trg_sol, figaxes_list[2], 'Target')]\n",
    "            \n",
    "        for sol_t, sol_dict, (fig, ax), ax_ttl in soltd_list:\n",
    "            draw_heatmap(eopts['pnts_plt'], sol_dict['v'].mean(dim=0), fig, ax, \n",
    "                field_abrv=f'{ax_ttl} A', components='xyz', show_title='top', zstr_loc='left')\n",
    "            fig.set_tight_layout(True)\n",
    "        \n",
    "    fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3962d99",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    if do_tchsave:\n",
    "        torch.save(model_history, f'{cfgstrg_dir}/ckpt_mdl.pt')\n",
    "        if do_bootstrap:\n",
    "            torch.save(target_history, f'{cfgstrg_dir}/ckpt_trg.pt')\n",
    "    if storage_dir is not None:\n",
    "        shutil.copy2(hdfpth, f'{cfgstrg_dir}/progress.h5')\n",
    "        if fig is not None:\n",
    "            fig.savefig(f'{cfgstrg_dir}/finalpred.pdf', dpi=144, bbox_inches=\"tight\")   \n",
    "    if do_profile:\n",
    "        profiler.stop()\n",
    "        html = profiler.output_html()\n",
    "        htmlpath = f'{cfgstrg_dir}/profiler.html'\n",
    "        with open(htmlpath, 'w') as fp:\n",
    "            fp.write(html.encode('ascii', errors='ignore').decode('ascii'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f02ef41",
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "source": [
    "    return outdict\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    use_argparse = True\n",
    "    if use_argparse:\n",
    "        import argparse\n",
    "        my_parser = argparse.ArgumentParser()\n",
    "        my_parser.add_argument('-c', '--configid', action='store', type=str, required=True)\n",
    "        my_parser.add_argument('-d', '--device',   action='store', type=str, required=True)\n",
    "        my_parser.add_argument('-s', '--nodesize', action='store', type=int, default=1)\n",
    "        my_parser.add_argument('-r', '--noderank', action='store', type=int, default=0)\n",
    "        my_parser.add_argument('-i', '--rsmindex', action='store', type=str, default=\"0.0\")\n",
    "        my_parser.add_argument('--dry-run', action='store_true')\n",
    "        args = my_parser.parse_args()\n",
    "        args_configid = args.configid\n",
    "        args_device_name = args.device\n",
    "        args_nodesize = args.nodesize\n",
    "        args_noderank = args.noderank\n",
    "        arsg_rsmindex = args.rsmindex\n",
    "        args_dryrun = args.dry_run\n",
    "    else:\n",
    "        args_configid = 'lvl1/lvl2/poiss2d'\n",
    "        args_device_name = 'cuda:0'\n",
    "        args_nodesize = 1\n",
    "        args_noderank = 0\n",
    "        arsg_rsmindex = 0\n",
    "        args_dryrun = True\n",
    "\n",
    "    assert args_noderank < args_nodesize\n",
    "    cfgidsplit = args_configid.split('/')\n",
    "    # Example: args_configid == 'lvl1/lvl2/poiss2d'\n",
    "    config_id = cfgidsplit[-1]\n",
    "    # Example: config_id == 'poiss2d'\n",
    "    config_tree = '/'.join(cfgidsplit[:-1])\n",
    "    # Example: config_tree == 'lvl1/lvl2'\n",
    "\n",
    "    import os\n",
    "    os.makedirs(configs_dir, exist_ok=True)\n",
    "    # Example: configs_dir == '.../code_bspinn/config'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    # Example: results_dir == '.../code_bspinn/result'\n",
    "    # os.makedirs(storage_dir, exist_ok=True)\n",
    "    # Example: storage_dir == '.../code_bspinn/storage'\n",
    "    \n",
    "    if args_dryrun:\n",
    "        print('>> Running in dry-run mode', flush=True)\n",
    "\n",
    "    cfg_path = f'{configs_dir}/{config_tree}/{config_id}.json'\n",
    "    print(f'>> Reading configuration from {cfg_path}', flush=True)\n",
    "    with open(cfg_path) as fp:\n",
    "        json_cfgdict = json.load(fp, object_pairs_hook=odict)\n",
    "    \n",
    "    if args_dryrun:\n",
    "        import tempfile\n",
    "        temp_resdir = tempfile.TemporaryDirectory()\n",
    "        temp_strdir = tempfile.TemporaryDirectory()\n",
    "        print(f'>> [dry-run] Temporary results dir placed at {temp_resdir.name}')\n",
    "        print(f'>> [dry-run] Temporary storage dir placed at {temp_strdir.name}')\n",
    "        results_dir = temp_resdir.name\n",
    "        storage_dir = temp_strdir.name\n",
    "        \n",
    "        dr_maxfrq = 10\n",
    "        dr_opts = {'opt/epoch': dr_maxfrq, 'io/cmprssn_lvl': 0}\n",
    "        for opt in dr_opts:\n",
    "            assert opt in json_cfgdict\n",
    "            json_cfgdict[opt] = dr_opts[opt]\n",
    "        for opt in fnmatch.filter(json_cfgdict.keys(), '*/frq'):\n",
    "            if json_cfgdict[opt] > dr_maxfrq:\n",
    "                json_cfgdict[opt] = dr_opts[opt] = dr_maxfrq\n",
    "        print(f'>> [dry-run] The following options were made overriden:', flush=True)\n",
    "        for opt, val in dr_opts.items():\n",
    "            print(f'>>           {opt}: {val}', flush=True)\n",
    "            \n",
    "        \n",
    "    nodepstfx = '' if args_nodesize == 1 else f'_{args_noderank:02d}'\n",
    "    # Example: nodepstfx in ('', '_01')\n",
    "    json_cfgdict['io/config_id'] = f'{config_id}{nodepstfx}'\n",
    "    # Example: ans in ('poiss2d', 'poiss2d_01')\n",
    "    json_cfgdict['io/results_dir'] = f'{results_dir}/{config_tree}'\n",
    "    # Example: ans == '.../code_bspinn/result/lvl1/lv2/poiss2d'\n",
    "    json_cfgdict['io/storage_dir'] = None # f'{storage_dir}/{config_tree}'\n",
    "    # Example: ans == '.../code_bspinn/storage/lvl1/lv2/poiss2d'\n",
    "    json_cfgdict['io/tch/device'] = args_device_name\n",
    "    # Example: args_device_name == 'cuda:0'\n",
    "\n",
    "    # Pre-processing and applying the looping processes\n",
    "    all_cfgdicts = preproc_cfgdict(json_cfgdict)\n",
    "    \n",
    "    # Selecting this node's config dict subset\n",
    "    node_cfgdicts = [cfg for i, cfg in enumerate(all_cfgdicts) \n",
    "                     if (i % args_nodesize == args_noderank)]\n",
    "    n_nodecfgs = len(node_cfgdicts)\n",
    "    \n",
    "    # Going over the config dicts one-by-one\n",
    "    rsmidx, rsmprt = tuple(int(x) for x in arsg_rsmindex.split('.'))\n",
    "    for cfgidx, config_dict in enumerate(node_cfgdicts):\n",
    "        if cfgidx < rsmidx:\n",
    "            continue\n",
    "        # Getting a single seed run to estimate the memory usage\n",
    "        tempcfg = config_dict.copy()\n",
    "        tempcfg['io/results_dir'] = None\n",
    "        tempcfg['io/storage_dir'] = None\n",
    "        tempcfg['rng_seed/list'] = [0]\n",
    "        tempcfg['opt/epoch'] = 0\n",
    "        tod = main(tempcfg)\n",
    "        allocmem, totmem = tod['dvc/mem/alloc'], tod['dvc/mem/total']\n",
    "        nsd_max = int(0.5 * totmem / allocmem)\n",
    "        \n",
    "        # Computing how many parts we must split the original config into\n",
    "        cfg_seeds = config_dict['rng_seed/list']\n",
    "        nprts = int(np.ceil(len(cfg_seeds) / nsd_max))\n",
    "        print(f'>> Config index {cfgidx} takes {allocmem/1e6:.1f} ' + \n",
    "              f'MB/seed (out of {totmem/1e9:.1f} GB)', flush=True)\n",
    "        print(f'>> Config index {cfgidx} must be ' + \n",
    "              f'devided into {nprts} parts.', flush=True)\n",
    "        \n",
    "        # Looping over each part of the config\n",
    "        for iprt in range(nprts):\n",
    "            if (cfgidx == rsmidx) and (iprt < rsmprt):\n",
    "                continue\n",
    "            print(f'>>> Started Working on config index {cfgidx}.{iprt}' + \n",
    "                  f' (out of {nprts} parts and {n_nodecfgs} configs).', flush=True)\n",
    "            iprtcfgseeds = cfg_seeds[(iprt*nsd_max):((iprt+1)*nsd_max)]\n",
    "            iprtcfgdict = config_dict.copy()\n",
    "            iprtcfgdict['rng_seed/list'] = iprtcfgseeds\n",
    "            main(iprtcfgdict)\n",
    "            print('-' * 40, flush=True)\n",
    "        print(f'>> Finished working on config index {cfgidx} ' + \n",
    "              f'(out of {n_nodecfgs} configs).', flush=True)\n",
    "        print('='*80, flush=True)\n",
    "        \n",
    "    if args_dryrun:\n",
    "        print(f'>> [dry-run] Cleaning up {temp_resdir.name}')\n",
    "        temp_resdir.cleanup()\n",
    "        print(f'>> [dry-run] Cleaning up {temp_strdir.name}')\n",
    "        temp_strdir.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
